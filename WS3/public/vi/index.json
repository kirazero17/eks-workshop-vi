[
{
	"uri": "/vi/5-spot-instances/5.1-instance-type-diversification/",
	"title": "Đa dạng hóa loại Instance",
	"tags": [],
	"description": "",
	"content": "Đa dạng hóa loại Instance Amazon EC2 Spot Instances cung cấp khả năng tính toán dư thừa có sẵn trong Đám mây AWS với mức giảm giá đáng kể so với giá On-Demand. EC2 có thể ngắt Spot Instances với thông báo trước hai phút khi EC2 cần lại khả năng tính toán. Bạn có thể sử dụng Spot Instances cho các ứng dụng có tính chịu lỗi và linh hoạt khác nhau. Một số ví dụ là phân tích dữ liệu, các khối công việc được container hóa, tính toán hiệu suất cao (HPC), máy chủ web không trạng thái, kết xuất, CI/CD, và các khối công việc thử nghiệm và phát triển khác.\nMột trong những phương pháp tốt nhất để áp dụng thành công Spot Instances là thực hiện đa dạng hóa Spot Instance như một phần của cấu hình của bạn. Đa dạng hóa Spot Instance giúp tiếp nhận khả năng từ nhiều group Spot Instance khác nhau, cả cho việc mở rộng và thay thế Spot Instances có thể nhận thông báo chấm dứt Spot Instance. Một group Spot Instance là một tập hợp các EC2 instances không sử dụng với cùng một loại Instance, hệ điều hành và AZ (ví dụ: m5.large trên Red Hat Enterprise Linux ở us-east-1a).\nCluster Autoscaler với Đa dạng hóa Spot Instance Cluster Autoscaler là một công cụ tự động điều chỉnh kích thước của cluster Kubernetes khi có các pod không chạy trong cluster do tài nguyên không đủ (Mở rộng) hoặc có các nút trong cluster đã được sử dụng không đủ trong một khoảng thời gian (Thu nhỏ).\nKhi sử dụng Spot Instances với Cluster Autoscaler có một số điều cần xem xét. Một điều quan trọng cần xem xét là, mỗi Nhóm Tăng tự động nên được tạo từ các loại instance cung cấp khả năng gần như bằng nhau. Cluster Autoscaler sẽ cố gắng xác định các tài nguyên CPU, bộ nhớ và GPU được cung cấp bởi một Nhóm Tăng tự động dựa trên ghi đè đầu tiên được cung cấp trong Chính sách Các Loại Instances Kết hợp của một Nhóm Tăng tự động. Nếu có bất kỳ ghi đè nào được tìm thấy, chỉ loại instance đầu tiên được tìm thấy sẽ được sử dụng. Xem Sử dụng Chính Sách Các Loại Instances Kết hợp và Spot Instances để biết chi tiết.\nKhi áp dụng các phương pháp đa dạng hóa Spot vào EKS và các cluster K8s trong khi sử dụng Cluster Autoscaler để tự động mở rộng khả năng, chúng ta phải thực hiện đa dạng hóa theo cách tuân thủ các chế độ hoạt động được mong đợi của Cluster Autoscaler.\nChúng ta có thể đa dạng hóa các group Spot Instance bằng hai chiến lược:\nBằng cách tạo ra nhiều node groups, mỗi group có kích thước khác nhau. Ví dụ, một node groups có kích thước 4 vCPU và 16GB RAM, và một node groups khác có 8 vCPU và 32GB RAM. Bằng cách thực hiện đa dạng hóa instance trong các node groups, bằng cách chọn một sự kết hợp của các loại instance và các gia đình khác nhau từ các group Spot Instance khác nhau mà đáp ứng các tiêu chí cùng về vCPU và bộ nhớ. Trong bài thực hành này, chúng ta sẽ giả sử rằng các node groups của chúng ta nên được cung cấp với các loại instance có 2 vCPU và 4GiB bộ nhớ.\nChúng ta sẽ sử dụng amazon-ec2-instance-selector để giúp chọn các loại instance và gia đình phù hợp có đủ số lượng vCPU và bộ nhớ.\nCó hơn 350 loại instance khác nhau có sẵn trên EC2 có thể làm cho quá trình chọn loại instance phù hợp khó khăn amazon-ec2-instance-selector giúp bạn chọn các loại instance tương thích cho ứng dụng của bạn chạy trên. Giao diện dòng lệnh có thể được truyền các tiêu chí tài nguyên như vcpus, bộ nhớ, hiệu suất mạng, và nhiều hơn nữa và sau đó trả về các loại instance phù hợp có sẵn.\nCông cụ CLI đã được cài đặt sẵn trong IDE của bạn:\n$ ec2-instance-selector --version Bây giờ bạn đã cài đặt ec2-instance-selector, bạn có thể chạy ec2-instance-selector --help để hiểu cách bạn có thể sử dụng nó để chọn các loại instance phù hợp với yêu cầu công việc của bạn. Vì mục đích của bài thực hành này, chúng ta cần trước tiên lấy một group các loại instance đáp ứng mục tiêu của chúng tôi là 2 vCPU và 4 GB RAM.\nChạy lệnh sau để lấy danh sách các loại instance.\n$ ec2-instance-selector --vcpus 2 --memory 4 --gpus 0 --current-generation \\ -a x86_64 --deny-list \u0026#39;t.*\u0026#39; --output table-wide Instance Type VCPUs Mem (GiB) Hypervisor Current Gen Hibernation Support CPU Arch Network Performance ------------- ----- --------- ---------- ----------- ------------------- -------- ------------------- c5.large 2 4 nitro true true x86_64 Up to 10 Gigabit c5a.large 2 4 nitro true false x86_64 Up to 10 Gigabit c5ad.large 2 4 nitro true false x86_64 Up to 10 Gigabit c5d.large 2 4 nitro true true x86_64 Up to 10 Gigabit c6a.large 2 4 nitro true false x86_64 Up to 12.5 Gigabit c6i.large 2 4 nitro true true x86_64 Up to 12.5 Gigabit c6id.large 2 4 nitro true true x86_64 Up to 12.5 Gigabit c6in.large 2 4 nitro true false x86_64 Up to 25 Gigabit c7a.large 2 4 nitro true false x86_64 Up to 12.5 Gigabit c7i.large 2 4 nitro true false x86_64 Up to 12.5 Gigabit Chúng ta sẽ sử dụng các loại instance này khi chúng ta định nghĩa node groups của mình trong phần tiếp theo.\nBên trong, ec2-instance-selector đang gọi đến DescribeInstanceTypes cho khu vực cụ thể và lọc các loại instance dựa trên các tiêu chí được chọn trong dòng lệnh, trong trường hợp của chúng ta, chúng ta đã lọc các loại instance đáp ứng các tiêu chí sau:\nCác loại instance không có GPU Kiến trúc x86_64 (không có các loại instance ARM như A1 hoặc các loại instance m6g chẳng hạn) Các loại instance có 2 vCPU và 4 GB RAM Các loại instance thế hệ hiện tại (từ thế hệ thứ 4 trở đi) Các loại instance không đáp ứng biểu thức chính quy t.* để loại bỏ các loại instance burstable Khối công việc của bạn có thể có các ràng buộc khác mà bạn nên xem xét khi chọn loại instance. Ví dụ. Các loại instance t2 và t3 là loại burstable và có thể không phù hợp cho các khối công việc có hạn chế về CPU yêu cầu độ chính xác của CPU. Các loại instance như m5a là loại AMD Instances, nếu khối công việc của bạn nhạy cảm với sự khác biệt số liệu (ví dụ: tính toán rủi ro tài chính, mô phỏng công nghiệp) việc kết hợp các loại instance này có thể không phù hợp.\n"
},
{
	"uri": "/vi/",
	"title": "Kubernetes trên AWS",
	"tags": [],
	"description": "",
	"content": "Kubernetes trên AWS Kubernetes là một nền tảng mã nguồn mở, linh hoạt, có khả năng mở rộng, phục vụ việc quản lý các ứng dụng được đóng gói và các dịch vụ liên quan, giúp việc cấu hình và tự động hóa quá trình triển khai ứng dụng trở nên thuận tiện hơn. Được biết đến như một hệ sinh thái lớn và phát triển nhanh chóng, Kubernetes cung cấp sự hỗ trợ rộng rãi qua các dịch vụ và công cụ đa dạng.\nTên Kubernetes bắt nguồn từ tiếng Hy Lạp, nghĩa là người lái tàu hoặc hoa tiêu. Kubernetes được Google công bố mã nguồn vào năm 2014, dựa trên gần một thập kỷ kinh nghiệm quản lý workload lớn trong thực tế của Google, kết hợp với các ý tưởng và best practices từ cộng đồng.\nQuay ngược thời gian Hãy xem xét tại sao Kubernetes lại quan trọng thông qua việc nhìn lại quá khứ.\nThời kỳ triển khai truyền thống: Ban đầu, các ứng dụng được chạy trực tiếp trên máy chủ vật lý, khiến việc phân bổ tài nguyên gặp khó khăn do không có cơ chế xác định ranh giới tài nguyên cho từng ứng dụng. Cách tiếp cận này dẫn đến nguy cơ một ứng dụng có thể sử dụng quá nhiều tài nguyên, ảnh hưởng đến hoạt động của các ứng dụng khác. Giải pháp là chạy mỗi ứng dụng trên một máy chủ vật lý riêng biệt, nhưng điều này lại không hiệu quả về mặt chi phí và tài nguyên.\nThời kỳ triển khai ảo hóa: Ảo hóa được giới thiệu như một giải pháp cho phép chạy nhiều Máy ảo (VM) trên cùng một máy chủ vật lý, giúp cô lập ứng dụng và tăng cường bảo mật. Ảo hóa cũng giúp cải thiện hiệu quả sử dụng tài nguyên và khả năng mở rộng.\nThời kỳ triển khai Container: Container giống như VM nhưng nhẹ hơn và chia sẻ Hệ điều hành (HĐH) với nhau. Container mang lại nhiều lợi ích như tạo mới và triển khai ứng dụng nhanh chóng, phát triển và triển khai liên tục, phân biệt rõ ràng giữa quá trình phát triển và vận hành, cung cấp tính nhất quán qua các môi trường, khả năng di chuyển giữa các cloud và HĐH, và quản lý ứng dụng tập trung.\nTại sao bạn cần Kubernetes và nó có thể làm gì? Container là phương tiện hiệu quả để đóng gói và chạy ứng dụng của bạn. Trong môi trường sản xuất, cần có cơ chế quản lý các container một cách hiệu quả, đảm bảo không có downtime. Kubernetes giúp quản lý các hệ thống phân tán mạnh mẽ, tự động hóa việc nhân rộng, cung cấp các mẫu triển khai và nhiều hơn nữa.\nKubernetes mang lại:\nPhát hiện dịch vụ và cân bằng tải Điều phối bộ nhớ Tự động rollouts và rollbacks Đóng gói tự động Tự phục hồi Quản lý cấu hình và bảo mật Những gì Kubernetes không phải là Kubernetes không phải là một hệ thống PaaS truyền thống, toàn diện. Nó hoạt động ở tầng container, cung cấp tính năng giống như PaaS như triển khai, nhân rộng, cân bằng tải, nhưng là một giải pháp linh hoạt và có thể mở rộng, không giới hạn loại ứng dụng được hỗ trợ, không triển khai mã nguồn hoặc build ứng dụng, không cung cấp dịch vụ ứng dụng cấp cao như middleware, databases, không bắt buộc sử dụng các giải pháp ghi nhật ký, giám sát hoặc cảnh báo, và không cung cấp hoặc áp dụng bất kỳ cấu hình toàn diện, bảo trì, quản lý hoặc hệ thống tự phục hồi. Kubernetes loại bỏ nhu cầu về điều phối truyền thống, thay vào đó là kiểm soát liên tục từ trạng thái hiện tại sang trạng thái mong muốn.\n"
},
{
	"uri": "/vi/6-fargate/6.1-enabling-fargate/",
	"title": "Bật Fargate",
	"tags": [],
	"description": "",
	"content": "Cấu hình Fargate Profile cho EKS Trước khi bạn lên lịch chạy các Pods trên Fargate trong cluster của mình, bạn cần định nghĩa ít nhất một Fargate profile để xác định các Pods sử dụng Fargate khi được khởi chạy.\nLà một quản trị viên, bạn có thể sử dụng Fargate profile để khai báo các Pods nào chạy trên Fargate. Bạn có thể thực hiện điều này thông qua các bộ chọn (selectors) trong profile. Bạn có thể thêm tối đa năm bộ chọn cho mỗi profile. Mỗi bộ chọn bao gồm một namespace và các nhãn (labels) tùy chọn. Bạn cần phải định nghĩa một namespace cho mỗi bộ chọn. Trường nhãn bao gồm nhiều cặp khóa-giá trị tùy chọn. Các Pods khớp với một bộ chọn sẽ được lên lịch chạy trên Fargate. Pods được khớp sử dụng namespace và các nhãn được chỉ định trong bộ chọn. Nếu một bộ chọn namespace được định nghĩa mà không có nhãn, Amazon EKS sẽ cố gắng lên lịch chạy tất cả các Pods trong namespace đó lên Fargate sử dụng profile. Nếu một Pod được lên lịch chạy khớp với bất kỳ bộ chọn nào trong Fargate profile, thì Pod đó sẽ được lên lịch chạy trên Fargate.\nNếu một Pod khớp với nhiều Fargate profiles, bạn có thể chỉ định profile nào mà Pod sử dụng bằng cách thêm nhãn Kubernetes sau đây vào đặc tả Pod: eks.amazonaws.com/fargate-profile: my-fargate-profile. Pod phải khớp với một bộ chọn trong profile đó để được lên lịch chạy trên Fargate. Các quy tắc ưu tiên/anti-ưu tiên của Kubernetes không áp dụng và không cần thiết với các Pod Fargate của Amazon EKS.\nBây giờ, hãy bắt đầu bằng cách thêm một Fargate profile vào cluster EKS của chúng ta. Đây là cấu hình eksctl mà chúng ta sẽ sử dụng:\nmanifests/modules/fundamentals/fargate/profile/fargate.yaml Cấu hình này tạo ra một Fargate profile có tên là checkout-profile với các đặc điểm sau:\nMục tiêu là các Pods trong namespace checkout có nhãn fargate: yes Đặt pod trong các private subnet của VPC Áp dụng một IAM role cho cơ sở hạ tầng Fargate để nó có thể kéo các images từ ECR, ghi log vào CloudWatch, và như thế. Lệnh sau tạo profile, mất vài phút:\n$ cat ~/environment/eks-workshop/modules/fundamentals/fargate/profile/fargate.yaml \\ | envsubst \\ | eksctl create fargateprofile -f - Bây giờ chúng ta có thể kiểm tra Fargate profile:\n$ aws eks describe-fargate-profile \\ --cluster-name $EKS_CLUSTER_NAME \\ --fargate-profile-name checkout-profile { \u0026#34;fargateProfile\u0026#34;: { \u0026#34;fargateProfileName\u0026#34;: \u0026#34;checkout-profile\u0026#34;, \u0026#34;fargateProfileArn\u0026#34;: \u0026#34;arn:aws:eks:us-west-2:1234567890:fargateprofile/eks-workshop/checkout-profile/92c4e2e3-50cd-773c-1c32-52e4d44 cd0ca\u0026#34;, \u0026#34;clusterName\u0026#34;: \u0026#34;eks-workshop\u0026#34;, \u0026#34;createdAt\u0026#34;: \u0026#34;2023-08-05T12:57:58.022000+00:00\u0026#34;, \u0026#34;podExecutionRoleArn\u0026#34;: \u0026#34;arn:aws:iam::1234567890:role/eks-workshop-fargate\u0026#34;, \u0026#34;subnets\u0026#34;: [ \u0026#34;subnet-01c3614cdd385a93c\u0026#34;, \u0026#34;subnet-0e392224ce426565a\u0026#34;, \u0026#34;subnet-07f8a6fda62ec83df\u0026#34; ], \u0026#34;selectors\u0026#34;: [ { \u0026#34;namespace\u0026#34;: \u0026#34;checkout\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;fargate\u0026#34;: \u0026#34;yes\u0026#34; } } ], \u0026#34;status\u0026#34;: \u0026#34;ACTIVE\u0026#34;, \u0026#34;tags\u0026#34;: {} } } "
},
{
	"uri": "/vi/1-introduce/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Kubernetes là một nền tảng nguồn mở, khả chuyển, có thể mở rộng để quản lý các ứng dụng được đóng gói và các service, giúp thuận lợi trong việc cấu hình và tự động hoá việc triển khai ứng dụng. Kubernetes là một hệ sinh thái lớn và phát triển nhanh chóng. Các dịch vụ, sự hỗ trợ và công cụ có sẵn rộng rãi.\nTên gọi Kubernetes có nguồn gốc từ tiếng Hy Lạp, có ý nghĩa là người lái tàu hoặc hoa tiêu. Google mở mã nguồn Kubernetes từ năm 2014. Kubernetes xây dựng dựa trên một thập kỷ rưỡi kinh nghiệm mà Google có được với việc vận hành một khối lượng lớn workload trong thực tế, kết hợp với các ý tưởng và thực tiễn tốt nhất từ cộng đồng.\nQuay ngược thời gian Chúng ta hãy xem tại sao Kubernetes rất hữu ích bằng cách quay ngược thời gian.\nThời đại triển khai theo cách truyền thống: Ban đầu, các ứng dụng được chạy trên các máy chủ vật lý. Không có cách nào để xác định ranh giới tài nguyên cho các ứng dụng trong máy chủ vật lý và điều này gây ra sự cố phân bổ tài nguyên. Ví dụ, nếu nhiều ứng dụng cùng chạy trên một máy chủ vật lý, có thể có những trường hợp một ứng dụng sẽ chiếm phần lớn tài nguyên hơn và kết quả là các ứng dụng khác sẽ hoạt động kém đi. Một giải pháp cho điều này sẽ là chạy từng ứng dụng trên một máy chủ vật lý khác nhau. Nhưng giải pháp này không tối ưu vì tài nguyên không được sử dụng đúng mức và rất tốn kém cho các tổ chức để có thể duy trì nhiều máy chủ vật lý như vậy.\nThời đại triển khai ảo hóa: Như một giải pháp, ảo hóa đã được giới thiệu. Nó cho phép bạn chạy nhiều Máy ảo (VM) trên CPU của một máy chủ vật lý. Ảo hóa cho phép các ứng dụng được cô lập giữa các VM và cung cấp mức độ bảo mật vì thông tin của một ứng dụng không thể được truy cập tự do bởi một ứng dụng khác.\nẢo hóa cho phép sử dụng tốt hơn các tài nguyên trong một máy chủ vật lý và cho phép khả năng mở rộng tốt hơn vì một ứng dụng có thể được thêm hoặc cập nhật dễ dàng, giảm chi phí phần cứng và hơn thế nữa. Với ảo hóa, bạn có thể có một tập hợp các tài ng\nuyên vật lý dưới dạng một cụm các máy ảo sẵn dùng.\nMỗi VM là một máy tính chạy tất cả các thành phần, bao gồm cả hệ điều hành riêng của nó, bên trên phần cứng được ảo hóa.\nThời đại triển khai Container: Các container tương tự như VM, nhưng chúng có tính cô lập để chia sẻ Hệ điều hành (HĐH) giữa các ứng dụng. Do đó, container được coi là nhẹ (lightweight). Tương tự như VM, một container có hệ thống tệp (filesystem), CPU, bộ nhớ, process space, v.v. Khi chúng được tách rời khỏi cơ sở hạ tầng bên dưới, chúng có thể khả chuyển (portable) trên cloud hoặc các bản phân phối Hệ điều hành.\nCác container đã trở nên phổ biến vì chúng có thêm nhiều lợi ích, chẳng hạn như:\nTạo mới và triển khai ứng dụng Agile: gia tăng tính dễ dàng và hiệu quả của việc tạo các container image so với việc sử dụng VM image. Phát triển, tích hợp và triển khai liên tục: cung cấp khả năng build và triển khai container image thường xuyên và đáng tin cậy với việc rollbacks dễ dàng, nhanh chóng. Phân biệt giữa Dev và Ops: tạo các images của các application container tại thời điểm build/release thay vì thời gian triển khai, do đó phân tách các ứng dụng khỏi hạ tầng. Khả năng quan sát không chỉ hiển thị thông tin và các metric ở mức Hệ điều hành, mà còn cả application health và các tín hiệu khác. Tính nhất quán về môi trường trong suốt quá trình phát triển, testing và trong production: Chạy tương tự trên laptop như trên cloud. Tính khả chuyển trên cloud và các bản phân phối HĐH: Chạy trên Ubuntu, RHEL, CoreOS, on-premises, Google Kubernetes Engine và bất kì nơi nào khác. Quản lý tập trung ứng dụng: Tăng mức độ trừu tượng từ việc chạy một Hệ điều hành trên phần cứng ảo hóa sang chạy một ứng dụng trên một HĐH bằng logical resources. Các micro-services phân tán, elastic: ứng dụng được phân tách thành các phần nhỏ hơn, độc lập và thể được triển khai và quản lý một cách linh hoạt - chứ không phải một app nguyên khối (monolithic). Cô lập các tài nguyên: dự đoán hiệu năng ứng dụng. Sử dụng tài nguyên: hiệu quả. Tại sao bạn cần Kubernetes và nó có thể làm những gì? Các container là một cách tốt để đóng gói và chạy các ứng dụng của bạn. Trong môi trường production, bạn cần quản lý các container chạy các ứng dụng và đảm bảo rằng không có khoảng thời gian downtime. Ví dụ, nếu một container bị tắt đi, một container khác cần phải khởi động lên. Điều này\nsẽ dễ dàng hơn nếu được xử lý bởi một hệ thống.\nĐó là cách Kubernetes đến với chúng ta. Kubernetes cung cấp cho bạn một framework để chạy các hệ phân tán một cách mạnh mẽ. Nó đảm nhiệm việc nhân rộng và chuyển đổi dự phòng cho ứng dụng của bạn, cung cấp các mẫu deployment và hơn thế nữa. Ví dụ, Kubernetes có thể dễ dàng quản lý một triển khai canary cho hệ thống của bạn.\nKubernetes cung cấp cho bạn:\nService discovery và cân bằng tải\nKubernetes có thể expose một container sử dụng DNS hoặc địa chỉ IP của riêng nó. Nếu lượng traffic truy cập đến một container cao, Kubernetes có thể cân bằng tải và phân phối lưu lượng mạng (network traffic) để việc triển khai được ổn định. Điều phối bộ nhớ\nKubernetes cho phép bạn tự động mount một hệ thống lưu trữ mà bạn chọn, như local storages, public cloud providers, v.v. Tự động rollouts và rollbacks\nBạn có thể mô tả trạng thái mong muốn cho các container được triển khai dùng Kubernetes và nó có thể thay đổi trạng thái thực tế sang trạng thái mong muốn với tần suất được kiểm soát. Ví dụ, bạn có thể tự động hoá Kubernetes để tạo mới các container cho việc triển khai của bạn, xoá các container hiện có và áp dụng tất cả các resource của chúng vào container mới. Đóng gói tự động\nBạn cung cấp cho Kubernetes một cluster gồm các node mà nó có thể sử dụng để chạy các tác vụ được đóng gói (containerized task). Bạn cho Kubernetes biết mỗi container cần bao nhiêu CPU và bộ nhớ (RAM). Kubernetes có thể điều phối các container đến các node để tận dụng tốt nhất các resource của bạn. Tự phục hồi\nKubernetes khởi động lại các containers bị lỗi, thay thế các container, xoá các container không phản hồi lại cấu hình health check do người dùng xác định và không cho các client biết đến chúng cho đến khi chúng sẵn sàng hoạt động. Quản lý cấu hình và bảo mật\nKubernetes cho phép bạn lưu trữ và quản lý các thông tin nhạy cảm như: password, OAuth token và SSH key. Bạn có thể triển khai và cập nhật lại secret và cấu hình ứng dụng mà không cần build lại các container image và không để lộ secret trong cấu hình stack của bạn. Kubernetes không phải là gì? Kubernetes không phải là một hệ thống PaaS (Nền tảng như một Dịch vụ) truyền thống, toàn diện. Do Kubernetes hoạt động ở tầng container chứ không phải ở tầng phần cứng, nó cung cấp một số tính năng thường áp dụng chung cho các dịch vụ PaaS, như triển khai, nhân rộng, cân bằng tải, ghi nhật ký và giám sát. Tuy nhiên, Kubernetes không phải là cấu trúc nguyên khối và các giải pháp mặc định này là tùy chọn và có thể cắm được (plugg\nable).\nKubernetes:\nKhông giới hạn các loại ứng dụng được hỗ trợ. Kubernetes nhằm mục đích hỗ trợ một khối lượng công việc cực kỳ đa dạng, bao gồm cả stateless, stateful và xử lý dữ liệu. Nếu một ứng dụng có thể chạy trong một container, nó sẽ chạy rất tốt trên Kubernetes. Không triển khai mã nguồn và không build ứng dụng của bạn. Quy trình CI/CD được xác định bởi tổ chức cũng như các yêu cầu kỹ thuật. Không cung cấp các service ở mức ứng dụng, như middleware (ví dụ, các message buses), các framework xử lý dữ liệu (ví dụ, Spark), cơ sở dữ liệu (ví dụ, MySQL), bộ nhớ cache, cũng như hệ thống lưu trữ của cluster (ví dụ, Ceph). Các thành phần như vậy có thể chạy trên Kubernetes và/hoặc có thể được truy cập bởi các ứng dụng chạy trên Kubernetes thông qua các cơ chế di động, chẳng hạn như Open Service Broker. Không bắt buộc các giải pháp ghi lại nhật ký (logging), giám sát (monitoring) hoặc cảnh báo (alerting). Nó cung cấp một số sự tích hợp như proof-of-concept, và cơ chế để thu thập và xuất các số liệu. Không cung cấp, không bắt buộc một cấu hình ngôn ngữ/hệ thống (ví dụ: Jsonnet). Nó cung cấp một API khai báo có thể được targeted bởi các hình thức khai báo tùy ý. Không cung cấp cũng như áp dụng bất kỳ cấu hình toàn diện, bảo trì, quản lý hoặc hệ thống tự phục hồi. Ngoài ra, Kubernetes không phải là một hệ thống điều phối đơn thuần. Trong thực tế, nó loại bỏ sự cần thiết của việc điều phối. Định nghĩa kỹ thuật của điều phối là việc thực thi một quy trình công việc được xác định: đầu tiên làm việc A, sau đó là B rồi sau chót là C. Ngược lại, Kubernetes bao gồm một tập các quy trình kiểm soát độc lập, có thể kết hợp, liên tục điều khiển trạng thái hiện tại theo trạng thái mong muốn đã cho. Nó không phải là vấn đề làm thế nào bạn có thể đi được từ A đến C. Kiểm soát tập trung cũng không bắt buộc. Điều này dẫn đến một hệ thống dễ sử dụng hơn, mạnh mẽ hơn, linh hoạt hơn và có thể mở rộng. "
},
{
	"uri": "/vi/1-introduce/1.1-manual-scheduling/",
	"title": "Manual Scheduling",
	"tags": [],
	"description": "",
	"content": "Cách thức Lập lịch hoạt động Bạn làm gì khi bạn không có một scheduler trong cluster của bạn?\nMỗi POD có một trường gọi là NodeName mà theo mặc định không được đặt. Bạn thường không chỉ định trường này khi bạn tạo tệp biểu thị, Kubernetes tự động thêm nó. Khi xác định được nó, Kubernetes lập lịch cho POD trên node bằng cách thiết lập thuộc tính nodeName thành tên của node bằng cách tạo một đối tượng liên kết. apiVersion: v1 kind: Pod metadata: name: nginx labels: name: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 8080 nodeName: node02 Không có Scheduler Bạn có thể gán thủ công các pod cho node chính nó. Và không có một scheduler, để lập lịch cho pod là thiết lập thuộc tính nodeName trong tệp định nghĩa pod của bạn khi tạo một pod.\nCách khác apiVersion: v1 kind: Binding metadata: name: nginx target: apiVersion: v1 kind: Node name: node02 --- apiVersion: v1 kind: Pod metadata: name: nginx labels: name: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 8080 K8s Reference Docs: Kubernetes API Concepts Assign POD to Node "
},
{
	"uri": "/vi/4-graviton-arm-instances/4.1-create-graviton-nodes/",
	"title": "Tạo Graviton Nodes",
	"tags": [],
	"description": "",
	"content": "Tạo Graviton Nodes Trong bài tập này, chúng ta sẽ tạo một Amazon EKS Managed Node Groups riêng biệt với các instance dựa trên Graviton và áp dụng một taint cho nó.\nĐầu tiên, hãy xác nhận trạng thái hiện tại của các node có sẵn trong cụm của chúng ta:\n$ kubectl get nodes -L kubernetes.io/arch NAME STATUS ROLES AGE VERSION ARCH ip-192-168-102-2.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 6h56m vVAR::KUBERNETES_NODE_VERSION amd64 ip-192-168-137-20.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 6h56m vVAR::KUBERNETES_NODE_VERSION amd64 ip-192-168-19-31.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 6h56m vVAR::KUBERNETES_NODE_VERSION amd64 Đầu ra cho thấy các node hiện có của chúng tôi với các cột hiển thị kiến trúc CPU của mỗi node. Tất cả các node này hiện đang sử dụng các node amd64.\nChúng ta chưa cấu hình các taints, điều này sẽ được thực hiện sau.\nLệnh sau tạo Node Groups Graviton:\n$ aws eks create-nodegroup \\ --cluster-name $EKS_CLUSTER_NAME \\ --nodegroup-name graviton \\ --node-role $GRAVITON_NODE_ROLE \\ --subnets $PRIMARY_SUBNET_1 $PRIMARY_SUBNET_2 $PRIMARY_SUBNET_3 \\ --instance-types t4g.medium \\ --ami-type AL2_ARM_64 \\ --scaling-config minSize=1,maxSize=3,desiredSize=1 \\ --disk-size 20 Lệnh aws eks wait nodegroup-active có thể được sử dụng để chờ đợi cho đến khi một Node Groups EKS cụ thể đã hoạt động và sẵn sàng sử dụng. Lệnh này là một phần của AWS CLI và có thể được sử dụng để đảm bảo rằng Node Groups được chỉ định đã được tạo thành công và tất cả các instance liên quan đang chạy và sẵn sàng.\n$ aws eks wait nodegroup-active \\ --cluster-name $EKS_CLUSTER_NAME \\ --nodegroup-name graviton Khi Amazon EKS Managed Node Groups mới của chúng ta Active, chạy lệnh sau:\n$ kubectl get nodes \\ --label-columns eks.amazonaws.com/nodegroup,kubernetes.io/arch NAME STATUS ROLES AGE VERSION NODEGROUP ARCH ip-192-168-102-2.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 6h56m vVAR::KUBERNETES_NODE_VERSION default amd64 ip-192-168-137-20.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 6h56m vVAR::KUBERNETES_NODE_VERSION default amd64 ip-192-168-19-31.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 6h56m vVAR::KUBERNETES_NODE_VERSION default amd64 ip-10-42-172-231.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 2m5s vVAR::KUBERNETES_NODE_VERSION graviton arm64 Lệnh trên sử dụng cờ --selector để truy vấn tất cả các node có labels eks.amazonaws.com/nodegroup khớp với tên của Amazon EKS Managed Node Groups của chúng ta graviton. Cờ --label-columns cũng cho phép chúng ta hiển thị giá trị của labels eks.amazonaws.com/nodegroup cũng như kiến trúc bộ xử lý trong đầu ra. Lưu ý rằng cột ARCH hiển thị Node Groups đã bị taint chạy Graviton arm64.\nHãy khám phá cấu hình hiện tại của node của chúng ta. Lệnh sau sẽ liệt kê chi tiết của tất cả các node thuộc Amazon EKS Managed Node Groups của chúng tôi.\n$ kubectl describe nodes \\ --selector eks.amazonaws.com/nodegroup=graviton Name: ip-10-42-12-233.us-west-2.compute.internal Roles: \u0026lt;none\u0026gt; Labels: beta.kubernetes.io/instance-type=t4g.medium beta.kubernetes.io/os=linux eks.amazonaws.com/capacityType=ON_DEMAND eks.amazonaws.com/nodegroup=graviton eks.amazonaws.com/nodegroup-image=ami-0b55230f107a87100 eks.amazonaws.com/sourceLaunchTemplateId=lt-07afc97c4940b6622 kubernetes.io/arch=arm64 [...] CreationTimestamp: Wed, 09 Nov 2022 10:36:26 +0000 Taints: \u0026lt;none\u0026gt; [...] Một số điểm cần chú ý:\nEKS tự động thêm một số labels nhất định để cho phép việc lọc dễ dàng hơn, bao gồm các labels cho loại hệ điều hành, tên Amazon EKS Managed Node Groups và các loại khác. Trong khi một số labels được cung cấp sẵn với EKS, AWS cho phép các nhà điều hành cấu hình bộ labels tùy chỉnh của riêng họ tại mức độ Amazon EKS Managed Node Groups. Điều này đảm bảo rằng mỗi node trong một Node Groups sẽ có các labels nhất quán. Nhãn kubernetes.io/arch cho thấy chúng ta đang chạy một instance EC2 với kiến trúc CPU ARM64. Hiện tại không có taint được cấu hình cho node đã khám phá, như được thể hiện bởi phần Taints: \u0026lt;none\u0026gt;. Cấu hình taint cho Amazon EKS Managed Node Groups Trong khi dễ dàng thêm taint cho các node bằng cách sử dụng CLI kubectl như mô tả tại đây, một quản trị viên sẽ phải thực hiện thay đổi này mỗi khi Node Groups dưới cơ bắt đầu hoặc kết thúc. Để vượt qua thách thức này, AWS hỗ trợ thêm cả labels và taints vào các Amazon EKS Managed Node Groups, đảm bảo mọi node trong MNG sẽ có các labels và taints tương ứng được cấu hình tự động.\nBây giờ hãy thêm một taint vào Amazon EKS Managed Node Groups được thiết lập trước của chúng ta graviton. Taint này sẽ có key=frontend, value=true và effect=NO_EXECUTE. Điều này đảm bảo rằng bất kỳ pod nào đang chạy trên Amazon EKS Managed Node Groups đã bị taint của chúng tôi sẽ bị đuổi ra nếu chúng không có toleration khớp. Ngoài ra, không có pod mới nào sẽ được lên lịch trên Amazon EKS Managed Node Groups này mà không có toleration phù hợp.\nBắt đầu bằng cách thêm một taint vào Amazon EKS Managed Node Groups được thiết lập trước của chúng ta bằng lệnh aws cli sau:\n$ aws eks update-nodegroup-config \\ --cluster-name $EKS_CLUSTER_NAME --nodegroup-name graviton \\ --taints \u0026#34;addOrUpdateTaints=[{key=frontend, value=true, effect=NO_EXECUTE}]\u0026#34; { \u0026#34;update\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;488a2b7d-9194-3032-974e-2f1056ef9a1b\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;InProgress\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;ConfigUpdate\u0026#34;, \u0026#34;params\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;TaintsToAdd\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[{\\\u0026#34;effect\\\u0026#34;:\\\u0026#34;NO_EXECUTE\\\u0026#34;,\\\u0026#34;value\\\u0026#34;:\\\u0026#34;true\\\u0026#34;,\\\u0026#34;key\\\u0026#34;:\\\u0026#34;frontend\\\u0026#34;}]\u0026#34; } ], \u0026#34;createdAt\u0026#34;: \u0026#34;2022-11-09T15:20:10.519000+00:00\u0026#34;, \u0026#34;errors\u0026#34;: [] } } Chạy lệnh sau để chờ Node Groups trở thành hoạt động.\n$ aws eks wait nodegroup-active --cluster-name $EKS_CLUSTER_NAME \\ --nodegroup-name graviton Thêm, loại bỏ hoặc thay thế taints có thể được thực hiện bằng cách sử dụng lệnh CLI aws eks update-nodegroup-config để cập nhật cấu hình của Amazon EKS Managed Node Groups. Điều này có thể được thực hiện bằng cách chuyển addOrUpdateTaints hoặc removeTaints và một danh sách các taints đến cờ --taints của lệnh.\nBạn cũng có thể cấu hình taints trên một Amazon EKS Managed Node Groups bằng cách sử dụng CLI eksctl. Xem tài liệu để biết thêm thông tin.\nChúng ta đã sử dụng effect=NO_EXECUTE trong cấu hình taint của chúng ta. Hiện tại, các Amazon EKS Managed Node Groups hỗ trợ các giá trị sau cho taint effect:\nNO_SCHEDULE - Tương ứng với hiệu ứng taint Kubernetes NoSchedule. Điều này cấu hình Amazon EKS Managed Node Groups với một taint đẩy lùi tất cả các pod không có toleration khớp. Tất cả các pod đang chạy không bị đuổi ra khỏi các node của Amazon EKS Managed Node Groups. NO_EXECUTE - Tương ứng với hiệu ứng taint Kubernetes NoExecute. Cho phép các node được cấu hình với taint này không chỉ đẩy lùi các pod mới được lên lịch mà còn đuổi ra bất kỳ pod nào đang chạy mà không có toleration khớp. PREFER_NO_SCHEDULE - Tương ứng với hiệu ứng taint Kubernetes PreferNoSchedule. Nếu có thể, EKS tránh lên lịch các Pod không chịu taint này vào node. Chúng ta có thể sử dụng lệnh sau để kiểm tra taint đã được cấu hình đúng cho Amazon EKS Managed Node Groups:\n$ aws eks describe-nodegroup --cluster-name $EKS_CLUSTER_NAME \\ --nodegroup-name graviton \\ | jq .nodegroup.taints [ { \u0026#34;key\u0026#34;: \u0026#34;frontend\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;effect\u0026#34;: \u0026#34;NO_EXECUTE\u0026#34; } ] Cập nhật Amazon EKS Managed Node Groups và lan truyền các labels và taints thường mất vài phút. Nếu bạn không thấy bất kỳ taint nào được cấu hình hoặc nhận được giá trị null, hãy đợi một vài phút trước khi thử lại lệnh trên.\nXác minh bằng lệnh CLI kubectl, chúng ta cũng có thể thấy rằng taint đã được lan truyền đúng cho node tương ứng:\n$ kubectl describe nodes \\ --selector eks.amazonaws.com/nodegroup=graviton | grep Taints Taints: frontend=true:NoExecute "
},
{
	"uri": "/vi/3-managed-node-groups/3.1-add-nodes/",
	"title": "Thêm các node",
	"tags": [],
	"description": "",
	"content": "Cập nhật cấu hình Amazon EKS Managed Node Groups trong Amazon EKS Khi làm việc với cluster của bạn, có thể bạn cần cập nhật cấu hình Amazon EKS Managed Node Groups để thêm các node bổ sung để hỗ trợ nhu cầu của công việc của bạn. Có nhiều cách để mở rộng một node group, trong trường hợp của chúng ta, chúng ta sẽ sử dụng lệnh eksctl scale nodegroup.\nĐầu tiên, hãy truy xuất cấu hình mở rộng node group hiện tại và xem kích thước tối thiểu, kích thước tối đa và sức chứa mong muốn của các node bằng cách sử dụng lệnh eksctl dưới đây:\n$ eksctl get nodegroup --name $EKS_DEFAULT_MNG_NAME --cluster $EKS_CLUSTER_NAME Chúng ta sẽ mở rộng node group trong eks-workshop bằng cách thay đổi số lượng node từ 3 thành 4 cho sức chứa mong muốn bằng lệnh dưới đây:\n$ eksctl scale nodegroup --name $EKS_DEFAULT_MNG_NAME --cluster $EKS_CLUSTER_NAME \\ --nodes 4 --nodes-min 3 --nodes-max 6 Một node group cũng có thể được cập nhật bằng cách sử dụng aws CLI với lệnh sau đây. Xem tài liệu để biết thêm thông tin.\naws eks update-nodegroup-config --cluster-name $EKS_CLUSTER_NAME \\\r--nodegroup-name $EKS_DEFAULT_MNG_NAME --scaling-config minSize=4,maxSize=6,desiredSize=4 Sau khi thay đổi node group, có thể mất tới 2-3 phút để các thay đổi cấu hình cung cấp và cấu hình node có hiệu lực. Hãy truy xuất cấu hình node group một lần nữa và xem kích thước tối thiểu, kích thước tối đa và sức chứa mong muốn của các node bằng cách sử dụng lệnh eksctl dưới đây:\n$ eksctl get nodegroup --name $EKS_DEFAULT_MNG_NAME --cluster $EKS_CLUSTER_NAME Để đợi cho đến khi hoạt động cập nhật node group hoàn tất, bạn có thể chạy lệnh này:\n$ aws eks wait nodegroup-active --cluster-name $EKS_CLUSTER_NAME --nodegroup-name $EKS_DEFAULT_MNG_NAME Khi lệnh trên hoàn tất, chúng ta có thể xem lại số lượng node worker đã thay đổi bằng lệnh sau, liệt kê tất cả các node trong Amazon EKS Managed Node Groups của chúng ta bằng cách sử dụng nhãn như một bộ lọc:\nCó thể mất khoảng một phút để node xuất hiện trong đầu ra dưới đây, nếu danh sách vẫn hiển thị 3 node, hãy kiên nhẫn.\n$ kubectl get nodes -l eks.amazonaws.com/nodegroup=$EKS_DEFAULT_MNG_NAME NAME STATUS ROLES AGE VERSION ip-10-42-104-151.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 2d23h vVAR::KUBERNETES_NODE_VERSION ip-10-42-144-11.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 2d23h vVAR::KUBERNETES_NODE_VERSION ip-10-42-146-166.us-west-2.compute.internal NotReady \u0026lt;none\u0026gt; 18s vVAR::KUBERNETES_NODE_VERSION ip-10-42-182-134.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 2d23h vVAR::KUBERNETES_NODE_VERSION Lưu ý rằng node hiển thị trạng thái NotReady, điều này xảy ra khi node mới vẫn đang trong quá trình tham gia vào cluster. Chúng ta cũng có thể sử dụng kubectl wait để chờ đến khi tất cả các node báo cáo Ready:\n$ kubectl wait --for=condition=Ready nodes --all --timeout=300s "
},
{
	"uri": "/vi/2-prerequiste/",
	"title": "Các Bước Chuẩn Bị",
	"tags": [],
	"description": "",
	"content": "Các Bước Chuẩn Bị Thực hiện lệnh sau từ Cloud9 của bài đầu tiên:\n$ prepare-environment fundamentals/mng/basics Trong lab \u0026ldquo;Bắt đầu\u0026rdquo;, chúng ta đã triển khai ứng dụng mẫu của mình lên EKS và đã thấy các Pod đang chạy. Nhưng các Pod này đang chạy ở đâu?\nChúng ta có thể kiểm tra managed node group mặc định đã được chuẩn bị trước cho bạn:\n$ eksctl get nodegroup --cluster $EKS_CLUSTER_NAME --name $EKS_DEFAULT_MNG_NAME Có một số thuộc tính của các managed node group mà chúng ta có thể thấy từ đầu ra này:\nCấu hình về số lượng tối thiểu, tối đa và mong muốn của số lượng node trong nhóm này Loại instance cho managed node group này là m5.large Sử dụng loại EKS AMI AL2_x86_64 Chúng ta cũng có thể kiểm tra các node và vị trí trong các AZ.\n$ kubectl get nodes -o wide --label-columns topology.kubernetes.io/zone Bạn nên thấy:\nCác node được phân phối trên nhiều subnet trong các AZ khác nhau, cung cấp tính sẵn có cao Trong suốt module này, chúng ta sẽ thực hiện các thay đổi vào managed node group này để thể hiện các khả năng cơ bản của MNGs.\n"
},
{
	"uri": "/vi/4-graviton-arm-instances/4.2-run-pods-on-graviton/",
	"title": "Chạy các pods trên Graviton",
	"tags": [],
	"description": "",
	"content": "Chạy các pods trên Graviton Sau khi đã tạo ra Node group Graviton của chúng ta, chúng ta sẽ cần cấu hình ứng dụng của mình để tận dụng các thay đổi này. Để làm điều đó, hãy cấu hình ứng dụng của chúng ta để triển khai dịch vụ micro ui chỉ trên các nodes là phần của Amazon EKS Managed Node Groups dựa trên Graviton của chúng ta.\nTrước khi thực hiện bất kỳ thay đổi nào, hãy kiểm tra cấu hình hiện tại cho các pods UI. Hãy nhớ rằng các pods này đang được điều khiển bởi một bản triển khai kết hợp được gọi là ui.\n$ kubectl describe pod --namespace ui --selector app.kubernetes.io/name=ui Name: ui-7bdbf967f9-qzh7f Namespace: ui Priority: 0 Service Account: ui Node: ip-10-42-11-43.us-west-2.compute.internal/10.42.11.43 Start Time: Wed, 09 Nov 2022 16:40:32 +0000 Labels: app.kubernetes.io/component=service app.kubernetes.io/created-by=eks-workshop app.kubernetes.io/instance=ui app.kubernetes.io/name=ui pod-template-hash=7bdbf967f9 Status: Running [....] Controlled By: ReplicaSet/ui-7bdbf967f9 Containers: [...] Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Như dự kiến, ứng dụng đang chạy thành công trên một nodes không bị ô nhiễm. Các pod tương ứng đang trong trạng thái Running và chúng ta có thể xác nhận rằng không có tolerations tùy chỉnh nào đã được cấu hình. Lưu ý rằng Kubernetes tự động thêm tolerations cho node.kubernetes.io/not-ready và node.kubernetes.io/unreachable với tolerationSeconds=300, trừ khi bạn hoặc một bộ điều khiển cụ thể đặt những tolerations đó. Những tolerations được tự động thêm này có nghĩa là Pods vẫn được ràng buộc với Nodes trong 5 phút sau khi phát hiện vấn đề.\nHãy cập nhật bản triển khai ui của chúng ta để liên kết các pods của nó với Amazon EKS Managed Node Groups bị ô nhiễm của chúng ta. Chúng ta đã tiền cấu hình Amazon EKS Managed Node Groups bị ô nhiễm của mình với một nhãn là tainted=yes mà chúng ta có thể sử dụng với một nodeSelector. Patch Kustomize sau mô tả các thay đổi cần thiết cho cấu hình triển khai của chúng ta để kích hoạt thiết lập này:\nmodules/fundamentals/mng/graviton/nodeselector-wo-toleration/deployment.yaml\rDeployment/ui Để áp dụng các thay đổi Kustomize, hãy chạy lệnh sau:\n$ kubectl apply -k ~/environment/eks-workshop/modules/fundamentals/mng/graviton/nodeselector-wo-toleration/ namespace/ui unchanged serviceaccount/ui unchanged configmap/ui unchanged service/ui unchanged deployment.apps/ui configured Với những thay đổi mới được thực hiện gần đây, hãy kiểm tra tình trạng triển khai UI của chúng ta:\n$ kubectl --namespace ui rollout status --watch=false deployment/ui Waiting for deployment \u0026#34;ui\u0026#34; rollout to finish: 1 old replicas are pending termination... Với chiến lược RollingUpdate mặc định cho triển khai ui của chúng ta, triển khai K8s sẽ chờ cho đến khi pod mới được tạo ra để ở trạng thái Ready trước khi chấm dứt pod cũ. Triển khai triển khai dường như đang bị kẹt, vì vậy hãy điều tra sâu hơn:\n$ kubectl get pod --namespace ui -l app.kubernetes.io/name=ui NAME READY STATUS RESTARTS AGE ui-659df48c56-z496x 0/1 Pending 0 16s ui-795bd46545-mrglh 1/1 Running 0 8m Bằng cách điều tra các pod cá nhân trong namespace ui, chúng ta có thể quan sát rằng một pod đang ở trạng thái Pending. Đào sâu hơn vào chi tiết Pending Pod cung cấp một số thông tin về vấn đề gặp phải.\n$ podname=$(kubectl get pod --namespace ui --field-selector=status.phase=Pending -o json | \\ jq -r \u0026#39;.items[0].metadata.name\u0026#39;) \u0026amp;\u0026amp; \\ kubectl describe pod $podname -n ui Name: ui-659df48c56-z496x Namespace: ui [...] Node-Selectors: kubernetes.io/arch=arm64 Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 19s default-scheduler 0/4 nodes are available: 1 node(s) had untolerated taint {frontend: true}, 3 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector. preemption: 0/4 nodes are available: 4 Preemption is not helpful for scheduling. Các thay đổi của chúng ta được phản ánh trong cấu hình mới của pod Pending. Chúng ta có thể thấy rằng chúng ta đã ghim pod vào bất kỳ nodes nào có nhãn tainted=yes nhưng điều này đã gây ra một vấn đề mới khi pod của chúng ta không thể được lên lịch (PodScheduled False). Một giải thích hữu ích hơn có thể được tìm thấy trong phần events:\n0/4 nodes are available: 1 node(s) had untolerated taint {frontend: true}, 3 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector. preemption: 0/4 nodes are available: 4 Preemption is not helpful for scheduling. Để sửa chữa điều này, chúng ta cần thêm một toleration. Hãy đảm bảo rằng triển khai và các pod tương ứng có thể chịu đựng taint frontend: true. Chúng ta có thể sử dụng patch kustomize dưới đây để thực hiện các thay đổi cần thiết:\nmodules/fundamentals/mng/graviton/nodeselector-w-toleration/deployment.yaml\rDeployment/ui $ kubectl apply -k ~/environment/eks-workshop/modules/fundamentals/mng/graviton/nodeselector-w-toleration/ namespace/ui unchanged serviceaccount/ui unchanged configmap/ui unchanged service/ui unchanged deployment.apps/ui configured $ kubectl --namespace ui rollout status deployment/ui --timeout=120s Kiểm tra pod UI, chúng ta có thể thấy rằng cấu hình bây giờ bao gồm toleration đã được chỉ định (frontend=true:NoExecute) và nó đã được lên lịch thành công trên nodes có taint tương ứng. Các lệnh sau có thể được sử dụng cho việc xác thực:\n$ kubectl get pod --namespace ui -l app.kubernetes.io/name=ui NAME READY STATUS RESTARTS AGE ui-6c5c9f6b5f-7jxp8 1/1 Running 0 29s $ kubectl describe pod --namespace ui -l app.kubernetes.io/name=ui Name: ui-6c5c9f6b5f-7jxp8 Namespace: ui Priority: 0 Node: ip-10-42-10-138.us-west-2.compute.internal/10.42.10.138 Start Time: Fri, 11 Nov 2022 13:00:36 +0000 Labels: app.kubernetes.io/component=service app.kubernetes.io/created-by=eks-workshop app.kubernetes.io/instance=ui app.kubernetes.io/name=ui pod-template-hash=6c5c9f6b5f Annotations: kubernetes.io/psp: eks.privileged prometheus.io/path: /actuator/prometheus prometheus.io/port: 8080 prometheus.io/scrape: true Status: Running IP: 10.42.10.225 IPs: IP: 10.42.10.225 Controlled By: ReplicaSet/ui-6c5c9f6b5f Containers: [...] Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True [...] QoS Class: Burstable Node-Selectors: kubernetes.io/arch=arm64 Tolerations: frontend:NoExecute op=Exists node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s [...] $ kubectl describe node --selector kubernetes.io/arch=arm64 Name: ip-10-42-10-138.us-west-2.compute.internal Roles: \u0026lt;none\u0026gt; Labels: beta.kubernetes.io/instance-type=t4g.medium beta.kubernetes.io/os=linux eks.amazonaws.com/capacityType=ON_DEMAND eks.amazonaws.com/nodegroup=graviton eks.amazonaws.com/nodegroup-image=ami-03e8f91597dcf297b kubernetes.io/arch=arm64 kubernetes.io/hostname=ip-10-42-10-138.us-west-2.compute.internal kubernetes.io/os=linux node.kubernetes.io/instance-type=t4g.medium [...] Taints: frontend=true:NoExecute Unschedulable: false [...] Như bạn có thể thấy, pod ui bây giờ đang chạy trên Node Groups dựa trên Graviton. Ngoài ra, bạn có thể thấy các Taints trên lệnh kubectl describe node, và các Tolerations phù hợp trên lệnh kubectl describe pod.\nBạn đã thành công trong việc lên lịch ứng dụng ui, có thể chạy trên cả bộ xử lý Intel và ARM, để chạy trên Amazon EKS Managed Node Groups mới dựa trên Graviton chúng ta đã tạo ra ở bước trước đó. Taints và tolerations là một công cụ mạnh mẽ có thể được sử dụng để cấu hình cách các pod được lên lịch lên các nodes, cho dù đó là cho các nodes được cải thiện bởi Graviton/GPU, hoặc cho các cluster Kubernetes đa người sử dụng.\n"
},
{
	"uri": "/vi/1-introduce/1.2-labels-and-selectors/",
	"title": "Labels and Selectors",
	"tags": [],
	"description": "",
	"content": "Nhãn và Bộ chọn Nhãn và Bộ chọn là các phương pháp tiêu chuẩn trong AWS, Kubernetes và ngành IT để nhóm các đối tượng cùng nhau. Nhãn là các thuộc tính được gắn vào mỗi mục, trong khi Bộ chọn giúp bạn lọc các mục này theo nhãn.\nLàm thế nào để sử dụng nhãn và bộ chọn trong Kubernetes?\nTrong Kubernetes, chúng ta có thể tạo ra các loại đối tượng khác nhau như PODs, ReplicaSets, Deployments và nhiều hơn nữa. Nhãn giúp quản lý và lọc các đối tượng này một cách linh hoạt.\nLàm thế nào để xác định các nhãn?\nVí dụ, để xác định một Pod với nhãn, cấu hình như sau:\napiVersion: v1 kind: Pod metadata: name: simple-webapp labels: app: App1 function: Front-end spec: containers: - name: simple-webapp image: simple-webapp ports: - containerPort: 8080 Sau khi Pod được tạo, để chọn Pod với nhãn, chạy lệnh:\n$ kubectl get pods --selector app=App1 Kubernetes sử dụng nhãn để kết nối các đối tượng khác nhau, ví dụ như trong một ReplicaSet:\napiVersion: apps/v1 kind: ReplicaSet metadata: name: simple-webapp labels: app: App1 function: Front-end spec: replicas: 3 selector: matchLabels: app: App1 template: metadata: labels: app: App1 function: Front-end spec: containers: - name: simple-webapp image: simple-webapp Đối với Dịch vụ: apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: App1 ports: - protocol: TCP port: 80 targetPort: 9376 Chú thích Trong khi nhãn và bộ chọn được sử dụng để nhóm các đối tượng, chú thích được sử dụng để ghi lại các chi tiết khác cho mục đích thông tin.\napiVersion: apps/v1 kind: ReplicaSet metadata: name: simple-webapp labels: app: App1 function: Front-end annotations: buildversion: 1.34 spec: replicas: 3 selector: matchLabels: app: App1 template: metadata: labels: app: App1 function: Front-end spec: containers: - name: simple-webapp image: simple-webapp Tài liệu Tham khảo K8s: Kubernetes: Working with Objects - Labels "
},
{
	"uri": "/vi/3-managed-node-groups/3.2-pod-affinity-and-anti-affinity/",
	"title": "Pod Affinity and Anti-Affinity",
	"tags": [],
	"description": "",
	"content": "Pod Affinity và Anti-Affinity Các Pods có thể được hạn chế chạy trên các nodes cụ thể hoặc dưới các điều kiện cụ thể. Điều này có thể bao gồm các trường hợp bạn chỉ muốn một pod ứng dụng chạy trên mỗi node hoặc muốn ghép các pods lại với nhau trên một node. Ngoài ra, khi sử dụng node affinity, các pods có thể có các hạn chế ưu tiên hoặc bắt buộc.\nTrong bài học này, chúng ta sẽ tập trung vào inter-pod affinity và anti-affinity bằng cách lên lịch các pods checkout-redis chạy chỉ một instance trên mỗi node và lên lịch các pods checkout chỉ chạy một instance của nó trên các nodes nơi có pod checkout-redis tồn tại. Điều này sẽ đảm bảo rằng các pods caching (checkout-redis) của chúng ta chạy cùng địa phương với một instance pod checkout để đạt hiệu suất tốt nhất.\nĐiều đầu tiên chúng ta muốn làm là xem các pods checkout và checkout-redis đang chạy:\n$ kubectl get pods -n checkout NAME READY STATUS RESTARTS AGE checkout-698856df4d-vzkzw 1/1 Running 0 125m checkout-redis-6cfd7d8787-kxs8r 1/1 Running 0 127m Chúng ta có thể thấy cả hai ứng dụng đều có một pod đang chạy trong cluster. Bây giờ, hãy tìm hiểu chúng đang chạy ở đâu:\n$ kubectl get pods -n checkout \\ -o=jsonpath=\u0026#39;{range .items[*]}{.metadata.name}{\u0026#34;\\t\u0026#34;}{.spec.nodeName}{\u0026#34;\\n\u0026#34;}\u0026#39; checkout-698856df4d-vzkzw ip-10-42-11-142.us-west-2.compute.internal checkout-redis-6cfd7d8787-kxs8r ip-10-42-10-225.us-west-2.compute.internal Dựa vào kết quả trên, pod checkout-698856df4d-vzkzw đang chạy trên node ip-10-42-11-142.us-west-2.compute.internal và pod checkout-redis-6cfd7d8787-kxs8r đang chạy trên node ip-10-42-10-225.us-west-2.compute.internal.\nTrong môi trường của bạn, các pods có thể đang chạy trên cùng một node ban đầu\nHãy thiết lập một chính sách podAffinity và podAntiAffinity trong deployment checkout để đảm bảo một pod checkout chạy trên mỗi node, và nó chỉ sẽ chạy trên các nodes nơi có pod checkout-redis đang chạy. Chúng ta sẽ sử dụng requiredDuringSchedulingIgnoredDuringExecution để làm cho điều này trở thành yêu cầu, thay vì một hành vi ưu tiên.\nKustomization sau thêm một phần affinity vào deployment checkout chỉ định cả chính sách podAffinity và podAntiAffinity:\nmodules/fundamentals/affinity/checkout/checkout.yaml\rDeployment/checkout Để thực hiện thay đổi, chạy lệnh sau để sửa đổi deployment checkout trong cluster của bạn:\n$ kubectl delete -n checkout deployment checkout $ kubectl apply -k ~/environment/eks-workshop/modules/fundamentals/affinity/checkout/ namespace/checkout unchanged serviceaccount/checkout unchanged configmap/checkout unchanged service/checkout unchanged service/checkout-redis unchanged deployment.apps/checkout configured deployment.apps/checkout-redis unchanged $ kubectl rollout status deployment/checkout \\ -n checkout --timeout 180s Phần podAffinity đảm bảo rằng một pod checkout-redis đã đang chạy trên node — điều này là vì chúng ta có thể giả định rằng pod checkout cần checkout-redis để chạy đúng cách. Phần podAntiAffinity yêu cầu rằng không có pods checkout nào đã đang chạy trên node bằng cách khớp với nhãn app.kubernetes.io/component=service. Bây giờ, hãy mở rộng deployment để kiểm tra cấu hình đang hoạt động:\n$ kubectl scale -n checkout deployment/checkout --replicas 2 Bây giờ xác nhận mỗi pod đang chạy ở đâu:\n$ kubectl get pods -n checkout \\ -o=jsonpath=\u0026#39;{range .items[*]}{.metadata.name}{\u0026#34;\\t\u0026#34;}{.spec.nodeName}{\u0026#34;\\n\u0026#34;}\u0026#39; checkout-6c7c9cdf4f-p5p6q ip-10-42-10-120.us-west-2.compute.internal checkout-6c7c9cdf4f-wwkm4 checkout-redis-6cfd7d8787-gw59j ip-10-42-10-120.us-west-2.compute.internal Trong ví dụ này, pod checkout đầu tiên chạy trên cùng pod với pod checkout-redis hiện tại, vì nó đáp ứng quy tắc podAffinity mà chúng ta đã đặt. Cái thứ hai vẫn đang chờ, bởi vì quy tắc podAntiAffinity mà chúng ta đã định nghĩa không cho phép hai pod checkout khởi động trên cùng một node. Do node thứ hai không có pod checkout-redis đang chạy, nó sẽ ở trạng thái chờ.\nTiếp theo, chúng ta sẽ mở rộng checkout-redis thành hai instances cho hai nodes của chúng ta, nhưng trước hết hãy sửa đổi chính sách deployment checkout-redis để phân tán các instances checkout-redis của chúng ta qua mỗi node. Để làm điều này, chúng ta chỉ cần tạo một quy tắc podAntiAffinity.\nmodules/fundamentals/affinity/checkout-redis/checkout-redis.yaml\rDeployment/checkout-redis Áp dụng nó với lệnh sau:\n$ kubectl delete -n checkout deployment checkout-redis $ kubectl apply -k ~/environment/eks-workshop/modules/fundamentals/affinity/checkout-redis/ namespace/checkout unchanged serviceaccount/checkout unchanged configmap/checkout unchanged service/checkout unchanged service/checkout-redis unchanged deployment.apps/checkout unchanged deployment.apps/checkout-redis configured $ kubectl rollout status deployment/checkout-redis \\ -n checkout --timeout 180s Phần podAntiAffinity yêu cầu rằng không có pods checkout-redis nào đã đang chạy trên node bằng cách khớp với nhãn app.kubernetes.io/component=redis.\n$ kubectl scale -n checkout deployment/checkout-redis --replicas 2 Kiểm tra các pods đang chạy để xác minh rằng giờ đây có hai cái của mỗi loại đang chạy:\n$ kubectl get pods -n checkout NAME READY STATUS RESTARTS AGE checkout-5b68c8cddf-6ddwn 1/1 Running 0 4m14s checkout-5b68c8cddf-rd7xf 1/1 Running 0 4m12s checkout-redis-7979df659-cjfbf 1/1 Running 0 19s checkout-redis-7979df659-pc6m9 1/1 Running 0 22s Chúng ta cũng có thể xác minh xem các pods đang chạy ở đâu để đảm bảo các chính sách podAffinity và podAntiAffinity đang được tuân thủ:\n$ kubectl get pods -n checkout \\ -o=jsonpath=\u0026#39;{range .items[*]}{.metadata.name}{\u0026#34;\\t\u0026#34;}{.spec.nodeName}{\u0026#34;\\n\u0026#34;}\u0026#39; checkout-5b68c8cddf-bn8bp ip-10-42-11-142.us-west-2.compute.internal checkout-5b68c8cddf-clnps ip-10-42-12-31.us-west-2.compute.internal checkout-redis-7979df659-57xcb ip-10-42-11-142.us-west-2.compute.internal checkout-redis-7979df659-r7kkm ip-10-42-12-31.us-west-2.compute.internal Mọi thứ trông tốt với việc lên lịch các pod, nhưng chúng ta có thể xác minh thêm bằng cách mở rộng pod checkout một lần nữa để xem pod thứ ba sẽ được triển khai ở đâu:\n$ kubectl scale --replicas=3 deployment/checkout --namespace checkout Nếu chúng ta kiểm tra các pods đang chạy, chúng ta có thể thấy rằng pod checkout thứ ba đã được đặt trong trạng thái Pending vì hai node đã có pod được triển khai và node thứ ba không có pod checkout-redis đang chạy.\n$ kubectl get pods -n checkout NAME READY STATUS RESTARTS AGE checkout-5b68c8cddf-bn8bp 1/1 Running 0 4m59s checkout-5b68c8cddf-clnps 1/1 Running 0 6m9s checkout-5b68c8cddf-lb69n 0/1 Pending 0 6s checkout-redis-7979df659-57xcb 1/1 Running 0 35s checkout-redis-7979df659-r7kkm 1/1 Running 0 2m10s Hãy kết thúc phần này bằng cách loại bỏ pod đang chờ:\n$ kubectl scale --replicas=2 deployment/checkout --namespace checkout "
},
{
	"uri": "/vi/6-fargate/6.2-scheduling-on-fargate/",
	"title": "Scheduling on Fargate",
	"tags": [],
	"description": "",
	"content": "Scheduling on Fargate Trong hệ thống của chúng ta, dịch vụ checkout là một phần không thể thiếu trong quy trình thanh toán của khách hàng. Vậy tại sao dịch vụ này lại chưa được chạy trên AWS Fargate? Hãy cùng tìm hiểu qua các nhãn (labels) của nó.\nĐể kiểm tra, chúng ta sẽ sử dụng lệnh sau trong Kubernetes (k8s):\n$ kubectl get pod -n checkout -l app.kubernetes.io/component=service -o json | jq \u0026#39;.items[0].metadata.labels\u0026#39; Có vẻ như Pod của chúng ta thiếu nhãn fargate=yes. Vì vậy, chúng ta sẽ khắc phục điều này bằng cách cập nhật deployment cho dịch vụ đó sao cho phần đặc tả (spec) của Pod bao gồm nhãn cần thiết cho việc lên lịch trên Fargate.\nCập nhật deployment như sau:\nmodules/fundamentals/fargate/enabling/deployment.yaml\rDeployment/checkout Áp dụng kustomization vào cluster:\n$ kubectl apply -k ~/environment/eks-workshop/modules/fundamentals/fargate/enabling [...] $ kubectl rollout status -n checkout deployment/checkout --timeout=200s Điều này sẽ khiến cho đặc tả Pod của dịch vụ checkout được cập nhật và kích hoạt một deployment mới, thay thế tất cả các Pods. Khi các Pods mới được lập lịch, bộ lên lịch Fargate sẽ so khớp nhãn mới được áp dụng bởi kustomization với hồ sơ mục tiêu của chúng ta và can thiệp để đảm bảo Pod của chúng ta được lập lịch trên tài nguyên do Fargate quản lý.\nLàm thế nào để chúng ta xác nhận nó đã hoạt động? Mô tả Pod mới đã được tạo và xem phần Events:\n$ kubectl describe pod -n checkout -l fargate=yes [...] Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning LoggingDisabled 10m fargate-scheduler Disabled logging because aws-logging configmap was not found. configmap \u0026#34;aws-logging\u0026#34; not found Normal Scheduled 9m48s fargate-scheduler Successfully assigned checkout/checkout-78fbb666b-fftl5 to fargate-ip-10-42-11-96.us-west-2.compute.internal Normal Pulling 9m48s kubelet Pulling image \u0026#34;public.ecr.aws/aws-containers/retail-store-sample-checkout:0.4.0\u0026#34; Normal Pulled 9m5s kubelet Successfully pulled image \u0026#34;public.ecr.aws/aws-containers/retail-store-sample-checkout:0.4.0\u0026#34; in 43.258137629s Normal Created 9m5s kubelet Created container checkout Normal Started 9m4s kubelet Started container checkout Các sự kiện từ fargate-scheduler cho chúng ta một số cái nhìn về những gì đã xảy ra. Sự kiện chính mà chúng ta quan tâm ở giai đoạn này của lab là sự kiện với lý do Scheduled. Kiểm tra kỹ lưỡng cho chúng ta biết tên của thể hiện Fargate được cung cấp cho Pod này, trong trường hợp của ví dụ trên là fargate-ip-10-42-11-96.us-west-2.compute.internal.\nChúng ta có thể kiểm tra nút này từ kubectl để lấy thêm thông tin về tài nguyên được cung cấp cho Pod này:\n$ NODE_NAME=$(kubectl get pod -n checkout -l app.kubernetes.io/component=service -o json | jq -r \u0026#39;.items[0].spec.nodeName\u0026#39;) $ kubectl describe node $NODE_NAME Name: fargate-ip-10-42-11-96.us-west-2.compute.internal Roles: \u0026lt;none\u0026gt; Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux eks.amazonaws.com/compute-type=fargate failure-domain.beta.kubernetes.io/region=us-west-2 failure-domain.beta.kubernetes.io/zone=us-west-2b kubernetes.io/arch=amd64 kubernetes.io/hostname=ip-10-42-11-96.us-west-2.compute.internal kubernetes.io/os=linux topology.kubernetes.io/region=us-west-2 topology.kubernetes.io/zone=us-west-2b [...] Điều này cung cấp cho chúng ta một số cái nhìn về bản chất của thể hiện tính toán cơ bản:\nNhãn eks.amazonaws.com/compute-type xác nhận rằng một thể hiện Fargate đã được cung cấp. Một nhãn khác topology.kubernetes.io/zone chỉ định khu vực khả dụng mà pod đang chạy. Trong phần System Info (không được hiển thị ở trên), chúng ta có thể thấy thể hiện đang chạy Amazon Linux 2, cũng như thông tin phiên bản cho các thành phần hệ thống như container, kubelet và kube-proxy. "
},
{
	"uri": "/vi/5-spot-instances/5.2-create-spot-capacity/",
	"title": "Tạo dung lượng dạng Spot",
	"tags": [],
	"description": "",
	"content": "Tạo dung lượng dạng Spot Chúng ta sẽ triển khai một Amazon EKS Managed Node Groups tạo ra các instance Spot, sau đó sửa đổi thành phần catalog hiện có của ứng dụng của chúng ta để chạy trên các instance Spot mới tạo.\nChúng ta có thể bắt đầu bằng cách liệt kê tất cả các node trong cụm EKS hiện có của chúng ta. Lệnh kubectl get nodes có thể được sử dụng để liệt kê các node trong cụm Kubernetes của bạn, nhưng để có thêm chi tiết về loại dung lượng, chúng ta sẽ sử dụng tham số -L eks.amazonaws.com/capacityType.\nLệnh sau đây cho thấy rằng các node của chúng tôi hiện đang là các instance on-demand.\n$ kubectl get nodes -L eks.amazonaws.com/capacityType NAME STATUS ROLES AGE VERSION CAPACITYTYPE ip-10-42-103-103.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 133m vVAR::KUBERNETES_NODE_VERSION ON_DEMAND ip-10-42-142-197.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 133m vVAR::KUBERNETES_NODE_VERSION ON_DEMAND ip-10-42-161-44.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 133m vVAR::KUBERNETES_NODE_VERSION ON_DEMAND Nếu bạn muốn lấy các node dựa trên một loại dung lượng cụ thể, như các instance on-demand, bạn có thể sử dụng \u0026ldquo;label selectors\u0026rdquo;. Trong tình huống cụ thể này, bạn có thể đạt được điều này bằng cách đặt label selector thành capacityType=ON_DEMAND.\n$ kubectl get nodes -l eks.amazonaws.com/capacityType=ON_DEMAND NAME STATUS ROLES AGE VERSION ip-10-42-10-119.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 3d10h vVAR::KUBERNETES_NODE_VERSION ip-10-42-10-200.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 3d10h vVAR::KUBERNETES_NODE_VERSION ip-10-42-11-94.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 3d10h vVAR::KUBERNETES_NODE_VERSION ip-10-42-12-235.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 4h34m vVAR::KUBERNETES_NODE_VERSION Trong sơ đồ dưới đây, có hai \u0026ldquo;Node Groups\u0026rdquo; riêng biệt đại diện cho các Amazon EKS Managed Node Groups trong cụm. Hộp Node Groups đầu tiên đại diện cho Node Groups chứa các instance On-Demand trong khi hộp thứ hai đại diện cho Node Groups chứa các instance Spot. Cả hai đều liên kết với cụm EKS được chỉ định.\nHãy tạo một Node Groups với các instance Spot. Lệnh sau tạo ra một Node Groups mới có tên managed-spot.\n$ aws eks create-nodegroup \\ --cluster-name $EKS_CLUSTER_NAME \\ --nodegroup-name managed-spot \\ --node-role $SPOT_NODE_ROLE \\ --subnets $PRIMARY_SUBNET_1 $PRIMARY_SUBNET_2 $PRIMARY_SUBNET_3 \\ --instance-types c5.large c5d.large c5a.large c5ad.large c6a.large \\ --capacity-type SPOT \\ --scaling-config minSize=2,maxSize=3,desiredSize=2 \\ --disk-size 20 Đối số --capacity-type SPOT chỉ định rằng tất cả dung lượng trong Amazon EKS Managed Node Groups này nên là Spot.\nLệnh aws eks wait nodegroup-active có thể được sử dụng để đợi cho đến khi một Node Groups EKS cụ thể đã hoạt động và sẵn sàng sử dụng. Lệnh này là một phần của AWS CLI và có thể được sử dụng để đảm bảo rằng Node Groups được chỉ định đã được tạo thành công và tất cả các instance liên quan đều đang chạy và sẵn sàng.\n$ aws eks wait nodegroup-active \\ --cluster-name $EKS_CLUSTER_NAME \\ --nodegroup-name managed-spot Khi Amazon EKS Managed Node Groups mới của chúng tôi là Active, chạy lệnh sau.\n$ kubectl get nodes -L eks.amazonaws.com/capacityType,eks.amazonaws.com/nodegroup NAME STATUS ROLES AGE VERSION CAPACITYTYPE NODEGROUP ip-10-42-103-103.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 3h38m vVAR::KUBERNETES_NODE_VERSION ON_DEMAND default ip-10-42-142-197.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 3h38m vVAR::KUBERNETES_NODE_VERSION ON_DEMAND default ip-10-42-161-44.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 3h38m vVAR::KUBERNETES_NODE_VERSION ON_DEMAND default ip-10-42-178-46.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 103s vVAR::KUBERNETES_NODE_VERSION SPOT managed-spot ip-10-42-97-19.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 104s vVAR::KUBERNETES_NODE_VERSION SPOT managed-spot Đầu ra cho thấy rằng hai node bổ sung đã được cung cấp trong Node Groups managed-spot với loại dung lượng là SPOT.\n"
},
{
	"uri": "/vi/3-managed-node-groups/",
	"title": "Amazon EKS Managed Node Groups",
	"tags": [],
	"description": "",
	"content": "Tạo Cluster EKS và Quản lý Node Sử dụng Amazon EKS Managed Node Groups Một cluster EKS chứa một hoặc nhiều node EC2 mà các Pod được lên lịch trên đó. Các node EKS chạy trong tài khoản AWS của bạn và kết nối với mặt điều khiển của cụm của bạn thông qua điểm cuối máy chủ API của cụm. Bạn triển khai một hoặc nhiều node vào một node group. Một node group là một hoặc nhiều trường hợp EC2 được triển khai trong một EC2 AutoScaling.\nCác node EKS là các trường hợp tiêu chuẩn của Amazon EC2. Bạn được tính phí cho chúng dựa trên giá của EC2. Để biết thêm thông tin, hãy xem Giá của Amazon EC2.\nCác node group được quản lý của Amazon EKS tự động hóa việc cung cấp và quản lý vòng đời của các node cho các cụm Amazon EKS. Điều này rất giúp đỡ trong các hoạt động vận hành như cập nhật liên tục cho các AMI mới hoặc triển khai phiên bản Kubernetes mới.\nCác ưu điểm của việc chạy các node group được quản lý của Amazon EKS bao gồm:\nTạo, cập nhật tự động hoặc chấm dứt các node với một hoạt động duy nhất bằng cách sử dụng bảng điều khiển Amazon EKS, eksctl, AWS CLI, AWS API, hoặc các công cụ mã hạ tầng như AWS CloudFormation và Terraform. Các node được cung cấp chạy bằng các AMI tối ưu mới nhất của Amazon EKS. Các node được cung cấp như là một phần của MNG tự động được gắn thẻ với siêu dữ liệu như AZ, kiến trúc CPU và loại trường hợp. Cập nhật và chấm dứt node tự động và một cách lịch sự giúp đảm bảo rằng ứng dụng của bạn vẫn khả dụng. Không có chi phí bổ sung để sử dụng các node group được quản lý của Amazon EKS, chỉ thanh toán cho các tài nguyên AWS được cung cấp. Các thực hành trong phần này đề cập đến các cách khác nhau mà các node group được quản lý của EKS có thể được sử dụng để cung cấp khả năng tính toán cho một cụm.\n"
},
{
	"uri": "/vi/5-spot-instances/5.3-running-a-workload-on-spot/",
	"title": "Chạy một workloads trên Spot",
	"tags": [],
	"description": "",
	"content": "Chạy một workloads trên Spot Tiếp theo, chúng ta sẽ sửa đổi ứng dụng cửa hàng bán lẻ mẫu của chúng ta để chạy thành phần catalog trên các Spot instances mới được tạo ra. Để làm điều này, chúng ta sẽ sử dụng Kustomize để áp dụng một bản vá vào catalog Deployment, thêm một trường nodeSelector với eks.amazonaws.com/capacityType: SPOT.\nmodules/fundamentals/mng/spot/deployment/deployment.yaml\rDeployment/catalog Áp dụng patch Kustomize với lệnh sau.\n$ kubectl apply -k ~/environment/eks-workshop/modules/fundamentals/mng/spot/deployment namespace/catalog không thay đổi serviceaccount/catalog không thay đổi configmap/catalog không thay đổi secret/catalog-db không thay đổi service/catalog không thay đổi service/catalog-mysql không thay đổi deployment.apps/catalog được cấu hình statefulset.apps/catalog-mysql không thay đổi Đảm bảo việc triển khai ứng dụng của bạn thành công với lệnh sau.\n$ kubectl rollout status deployment/catalog -n catalog --timeout=5m Cuối cùng, hãy xác minh rằng các pod catalog đang chạy trên các Spot instances. Chạy hai lệnh sau.\n$ kubectl get pods -l app.kubernetes.io/component=service -n catalog -o wide NAME READY STATUS RESTARTS AGE IP NODE catalog-6bf46b9654-9klmd 1/1 Running 0 7m13s 10.42.118.208 ip-10-42-99-254.us-east-2.compute.internal $ kubectl get nodes -l eks.amazonaws.com/capacityType=SPOT NAME STATUS ROLES AGE VERSION ip-10-42-139-140.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 16m vVAR::KUBERNETES_NODE_VERSION ip-10-42-99-254.us-east-2.compute.internal Ready \u0026lt;none\u0026gt; 16m vVAR::KUBERNETES_NODE_VERSION Lệnh đầu tiên cho chúng ta biết rằng pod catalog đang chạy trên node ip-10-42-99-254.us-east-2.compute.internal, mà chúng ta xác minh là một Spot instance bằng cách so khớp nó với đầu ra của lệnh thứ hai.\nTrong lab này, bạn triển khai một nhóm node quản lý tạo ra các Spot instances, và sau đó sửa đổi deployment catalog để chạy trên các Spot instances mới tạo ra. Theo quy trình này, bạn có thể sửa đổi bất kỳ deployment nào đang chạy trên cụm bằng cách thêm tham số nodeSelector, như đã chỉ định trong patch Kustomization ở trên.\n"
},
{
	"uri": "/vi/3-managed-node-groups/3.3-upgrading-amis/",
	"title": "Nâng cấp AMI",
	"tags": [],
	"description": "",
	"content": "Nâng cấp AMI Amazon EKS Optimized Amazon Linux AMI được xây dựng trên nền tảng Amazon Linux 2 và được cấu hình để phục vụ như là base image cho các node của Amazon EKS. Được coi là một thực hành tốt nhất khi sử dụng phiên bản mới nhất của EKS-Optimized AMI khi bạn thêm node vào một cluster EKS, vì các bản phát hành mới bao gồm các bản vá và cập nhật bảo mật cho Kubernetes. Việc nâng cấp các node đã được cung cấp trước đó trong cluster EKS cũng rất quan trọng.\nAmazon EKS Managed Node Groups cung cấp khả năng tự động hóa việc cập nhật AMI đang được sử dụng bởi các node mà nó quản lý. Nó sẽ tự động dỡ bỏ node bằng cách sử dụng Kubernetes API và tuân thủ Pod disruption budgets mà bạn đặt cho các Pod của mình để đảm bảo rằng ứng dụng của bạn luôn sẵn sàng.\nQuá trình nâng cấp Amazon EKS Managed Node Groups của Amazon EKS gồm 4 giai đoạn:\nThiết lập:\nTạo ra một phiên bản Amazon EC2 Launch Template mới được liên kết với Auto Scaling group với AMI mới nhất Chỉ định Auto Scaling group của bạn sử dụng phiên bản mới nhất của launch template Xác định số lượng node tối đa để nâng cấp song song sử dụng thuộc tính updateconfig cho Node Group. Tăng quy mô:\nTrong quá trình nâng cấp, các node nâng cấp sẽ được triển khai trong cùng một AZ với những node đang được nâng cấp Tăng kích thước tối đa và kích thước mong muốn của Auto Scaling Group để hỗ trợ các node bổ sung Sau khi tăng quy mô Auto Scaling Group, nó sẽ kiểm tra xem các node sử dụng cấu hình mới nhất có hiện diện trong Node Group hay không. Áp dụng một eks.amazonaws.com/nodegroup=unschedulable:NoSchedule taint trên mọi node trong Node Group không có nhãn mới nhất. Điều này ngăn không cho các node đã được cập nhật từ một lần cập nhật trước đó bị gắn nhãn. Nâng cấp:\nChọn ngẫu nhiên một node và dỡ bỏ các Pod khỏi node đó. Chuyển trạng thái node sang cordon sau khi mọi Pod đã được dỡ bỏ và đợi 60 giây Gửi yêu cầu hủy bỏ tới Auto Scaling Group cho node đã được cordon. Áp dụng tương tự cho tất cả các node thuộc Amazon EKS Managed Node Groups, đảm bảo không có node nào sử dụng phiên bản cũ hơn Giảm quy mô:\nGiai đoạn giảm quy mô sẽ giảm kích thước tối đa và kích thước mong muốn của Auto Scaling group xuống một đơn vị cho đến khi các giá trị trở về như trước khi bắt đầu quá trình cập nhật.\nĐể tìm hiểu thêm về hành vi cập nhật Amazon EKS Managed Node Groups, hãy xem các giai đoạn cập nhật Amazon EKS Managed Node Groups.\nNâng cấp một Amazon EKS Managed Node Groups Việc nâng cấp Node Group sẽ mất ít nhất 10 phút, chỉ thực hiện phần này nếu bạn có đủ thời gian\nCluster EKS đã được cung cấp cho bạn cố tình có các Amazon EKS Managed Node Groups không chạy phiên bản AMI mới nhất. Bạn có thể xem phiên bản AMI mới nhất là gì bằng cách truy vấn SSM:\n$ EKS_VERSION=$(aws eks describe-cluster --name $EKS_CLUSTER_NAME --query \u0026#34;cluster.version\u0026#34; --output text) $ aws ssm get-parameter --name /aws/service/eks/optimized-ami/$EKS_VERSION/amazon-linux-2/recommended/image_id --region $AWS_REGION --query \u0026#34;Parameter.Value\u0026#34; --output text ami-0fcd72f3118e0dd88 Khi bạn khởi tạo một cập nhật Amazon EKS Managed Node Groups, Amazon EKS sẽ tự động cập nhật các node của bạn, hoàn thành các bước được liệt kê ở trên. Nếu bạn đang sử dụng một Amazon EKS Optimized AMI, Amazon EKS sẽ tự động áp dụng các bản vá bảo mật mới nhất và cập nhật hệ điều hành cho các node của bạn như một phần của phiên bản phát hành AMI mới nhất.\nBạn có thể khởi tạo một cập nhật của Amazon EKS Managed Node Groups được sử dụng để lưu trữ ứng dụng mẫu của chúng ta như sau:\n$ aws eks update-nodegroup-version --cluster-name $EKS_CLUSTER_NAME --nodegroup-name $EKS_DEFAULT_MNG_NAME Bạn có thể theo dõi hoạt động trên các node bằng cách sử dụng kubectl:\n$ kubectl get nodes --watch Nếu bạn muốn đợi cho đến khi MNG được cập nhật, bạn có thể chạy lệnh sau:\n$ aws eks wait nodegroup-active --cluster-name $EKS_CLUSTER_NAME --nodegroup-name $EKS_DEFAULT_MNG_NAME Một khi quá trình này hoàn tất, bạn có thể tiếp tục bước tiếp theo.\n"
},
{
	"uri": "/vi/1-introduce/1.3-node-selectors/",
	"title": "Node Selectors",
	"tags": [],
	"description": "",
	"content": "Node Selectors trong Kubernetes Node Selectors là một tính năng quan trọng trong Kubernetes, cho phép bạn hạn chế pod chỉ được lên lịch chạy trên một nhóm các Node cụ thể dựa trên các nhãn (labels) được chỉ định. Điều này giúp tối ưu hóa việc sử dụng tài nguyên và đảm bảo rằng các ứng dụng chạy trên cơ sở hạ tầng phù hợp nhất.\nĐể sử dụng Node Selector, bạn cần thêm một thuộc tính mới vào phần spec của định nghĩa Pod và chỉ định nhãn phù hợp. apiVersion: v1 kind: Pod metadata: name: myapp-pod spec: containers: - name: data-processor image: data-processor nodeSelector: size: Large Bạn cũng cần gán nhãn cho các Node mà bạn muốn chọn. Điều này có thể được thực hiện sử dụng lệnh kubectl label. $ kubectl label nodes \u0026lt;node-name\u0026gt; \u0026lt;label-key\u0026gt;=\u0026lt;label-value\u0026gt; Ví dụ:\n$ kubectl label nodes node-1 size=Large Sau khi đã gán nhãn cho Node và định nghĩa Node Selector trong Pod, bạn có thể tạo Pod bằng cách sử dụng: apiVersion: v1 kind: Pod metadata: name: myapp-pod spec: containers: - name: data-processor image: data-processor nodeSelector: size: Large $ kubectl create -f pod-definition.yml Node Selector - Hạn chế Mặc dù Node Selector rất hữu ích, nhưng nó chỉ hỗ trợ lọc dựa trên các nhãn đơn giản. Đối với các yêu cầu phức tạp hơn, Kubernetes cung cấp các cơ chế Node Affinity và Anti Affinity, cho phép bạn tạo ra các quy tắc lên lịch phức tạp hơn, bao gồm cả các điều kiện \u0026ldquo;nếu thì\u0026rdquo; và ưu tiên.\nTài liệu tham khảo K8s: Node Selectors trong Kubernetes "
},
{
	"uri": "/vi/6-fargate/6.3-resource-allocation/",
	"title": "Resource allocation",
	"tags": [],
	"description": "",
	"content": "Resource allocation Khi xem xét giải pháp container hóa cho các ứng dụng, AWS Fargate và Kubernetes là hai công nghệ được nhiều người chọn lựa. Đặc biệt, việc hiểu rõ cách thức tính toán chi phí của Fargate và cách cấu hình tài nguyên cho Pod trong Kubernetes là cực kỳ quan trọng để tối ưu hóa hiệu suất và chi phí.\nĐiểm Đến của Chi Phí Fargate Theo trang giá của Fargate, chi phí Fargate chủ yếu dựa trên CPU và bộ nhớ được sử dụng. Số lượng tài nguyên được cấp phát cho một thể hiện Fargate phụ thuộc vào yêu cầu tài nguyên được chỉ định bởi Pod. Có một bộ cấu hình đã được tài liệu về các tổ hợp CPU và bộ nhớ hợp lệ cho Fargate, điều này cần được cân nhắc khi đánh giá một tải công việc có phù hợp với Fargate hay không.\nChúng ta có thể xác định tài nguyên nào đã được cung cấp cho Pod của mình từ lần triển khai trước bằng cách kiểm tra các annotation của nó:\n$ kubectl get pod -n checkout -l app.kubernetes.io/component=service -o json | jq -r \u0026#39;.items[0].metadata.annotations\u0026#39; { \u0026#34;CapacityProvisioned\u0026#34;: \u0026#34;0.25vCPU 0.5GB\u0026#34;, \u0026#34;Logging\u0026#34;: \u0026#34;LoggingDisabled: LOGGING_CONFIGMAP_NOT_FOUND\u0026#34;, \u0026#34;kubernetes.io/psp\u0026#34;: \u0026#34;eks.privileged\u0026#34;, \u0026#34;prometheus.io/path\u0026#34;: \u0026#34;/metrics\u0026#34;, \u0026#34;prometheus.io/port\u0026#34;: \u0026#34;8080\u0026#34;, \u0026#34;prometheus.io/scrape\u0026#34;: \u0026#34;true\u0026#34; } Trong ví dụ trên, ta có thể thấy rằng annotation CapacityProvisioned chỉ ra rằng chúng tôi đã được cấp phát 0.25 vCPU và 0.5 GB bộ nhớ, đó là kích thước thể hiện Fargate tối thiểu. Nhưng nếu Pod của chúng tôi cần nhiều tài nguyên hơn thì sao? May mắn thay, Fargate cung cấp nhiều lựa chọn tùy thuộc vào yêu cầu tài nguyên mà chúng tôi có thể thử nghiệm.\nTrong ví dụ tiếp theo, chúng tôi sẽ tăng số lượng tài nguyên mà thành phần checkout yêu cầu và xem cách bộ lập lịch Fargate thích nghi. Kustomization mà chúng tôi sẽ áp dụng tăng tài nguyên yêu cầu lên 1 vCPU và 2.5G bộ nhớ:\nmodules/fundamentals/fargate/sizing/deployment.yaml\rDeployment/checkout Áp dụng kustomization và chờ cho quá trình triển khai hoàn tất:\n$ kubectl apply -k ~/environment/eks-workshop/modules/fundamentals/fargate/sizing [...] $ kubectl rollout status -n checkout deployment/checkout --timeout=200s Bây giờ, hãy kiểm tra lại tài nguyên được Fargate cấp phát. Dựa trên những thay đổi được nêu trên, bạn mong đợi thấy gì?\n$ kubectl get pod -n checkout -l app.kubernetes.io/component=service - o json | jq -r \u0026#39;.items[0].metadata.annotations\u0026#39; { \u0026#34;CapacityProvisioned\u0026#34;: \u0026#34;1vCPU 3GB\u0026#34;, \u0026#34;Logging\u0026#34;: \u0026#34;LoggingDisabled: LOGGING_CONFIGMAP_NOT_FOUND\u0026#34;, \u0026#34;kubernetes.io/psp\u0026#34;: \u0026#34;eks.privileged\u0026#34;, \u0026#34;prometheus.io/path\u0026#34;: \u0026#34;/metrics\u0026#34;, \u0026#34;prometheus.io/port\u0026#34;: \u0026#34;8080\u0026#34;, \u0026#34;prometheus.io/scrape\u0026#34;: \u0026#34;true\u0026#34; } Tài nguyên được yêu cầu bởi Pod đã được làm tròn lên tới cấu hình Fargate gần nhất được mô tả trong bộ tổ hợp hợp lệ trên.\n"
},
{
	"uri": "/vi/4-graviton-arm-instances/",
	"title": "Graviton (ARM) instances",
	"tags": [],
	"description": "",
	"content": "Graviton (ARM) instances Chuẩn bị môi trường cho phần này:\n$ prepare-environment fundamentals/mng/graviton Cho dù bạn đang sử dụng các phiên bản On-demand hoặc Spot instances, AWS cung cấp 3 loại bộ xử lý cho EC2 cũng như các Amazon EKS Managed Node Groups EC2-backed EKS. Khách hàng có sự lựa chọn của bộ xử lý Intel, AMD và ARM (AWS Graviton). Bộ xử lý AWS Graviton được thiết kế bởi AWS để cung cấp hiệu suất giá tốt nhất cho các khối công việc đám mây của bạn chạy trên Amazon EC2.\nCác instance dựa trên Graviton có thể được xác định bằng chữ g trong phần Processor family của Qui tắc đặt tên loại instance.\nCác bộ xử lý AWS Graviton được xây dựng trên Hệ thống AWS Nitro. AWS đã xây dựng Hệ thống AWS Nitro để gần như tất cả các tài nguyên tính toán và bộ nhớ của phần cứng máy chủ được giao cho các instance của bạn. Điều này được đạt được bằng cách phân rã các chức năng hypervisor và khả năng quản lý từ các máy chủ và giao chúng cho phần cứng và phần mềm có chức năng riêng biệt. Điều này dẫn đến hiệu suất tổng thể tốt hơn, khác với các nền tảng ảo hóa truyền thống chạy phần mềm hypervisor trên cùng một máy chủ vật lý như các máy ảo, điều này có nghĩa là các máy ảo không thể sử dụng 100% tài nguyên của máy chủ. Hệ thống AWS Nitro được hỗ trợ bởi các hệ điều hành Linux phổ biến cùng với nhiều ứng dụng và dịch vụ phổ biến từ AWS và các Nhà cung cấp Phần mềm Độc lập.\nKiến trúc đa kiến trúc với Bộ xử lý Graviton AWS Graviton yêu cầu container images tương thích với ARM, lý tưởng là đa kiến trúc (ARM64 và AMD64) cho phép tương thích chéo với cả các loại instance Graviton và x86.\nCác bộ xử lý Graviton tăng cường trải nghiệm EKS cho các Amazon EKS Managed Node Groups với các instance mang lại giá thấp hơn đến 20%, hiệu suất giá tốt hơn đến 40%, và tiêu thụ năng lượng ít hơn đến 60% so với các instance x86 thế hệ thứ năm tương đương. Các Amazon EKS Managed Node Groups EKS dựa trên Graviton khởi động một nhóm EC2 Auto Scaling với các bộ xử lý Graviton.\nThêm các instance dựa trên Graviton vào Amazon EKS Managed Node Groups EKS của bạn giới thiệu một cơ sở hạ tầng đa kiến trúc và nhu cầu cho ứng dụng của bạn chạy trên các CPU khác nhau. Điều này có nghĩa là mã ứng dụng của bạn cần phải có sẵn trong các triển khai Kiến trúc Tập lệnh Hướng dẫn (ISA) khác nhau. Có nhiều tài nguyên khác nhau để giúp các nhóm lập kế hoạch và chuyển ứng dụng sang các instance dựa trên Graviton. Hãy kiểm tra kế hoạch chuyển đổi Graviton và Cố vấn Chuyển đổi cho Graviton để có các tài nguyên hữu ích.\nKiến trúc ứng dụng web mẫu của cửa hàng bán lẻ chứa container images đã được xây sẵn cho cả hai kiến trúc CPU x86-64 và ARM64.\nKhi sử dụng các instance Graviton, chúng ta cần đảm bảo rằng chỉ các container được xây dựng cho các kiến trúc CPU ARM mới được lên lịch trên các instance Graviton. Đây là nơi mà taints và tolerations trở nên hữu ích.\nTaints và Toleration Taints là một thuộc tính của một node để đẩy lùi một số pods nhất định. Tolerations được áp dụng cho các pods để cho phép chúng được lên lịch trên các node với taints phù hợp. Taints và tolerations hoạt động cùng nhau để đảm bảo rằng các pods không được lên lịch trên các node không phù hợp.\nCấu hình các node tainted hữu ích trong các tình huống khi chúng ta cần đảm bảo rằng chỉ các pods cụ thể được lên lịch trên các nhóm node cụ thể có phần cứng đặc biệt (như các instance dựa trên Graviton hoặc GPU đính kèm). Trong bài tập thực hành này, chúng ta sẽ tìm hiểu cách cấu hình taints cho các Amazon EKS Managed Node Groups của chúng ta và cách thiết lập ứng dụng của chúng ta để sử dụng các node tainted chạy các bộ xử lý dựa trên Graviton.\n"
},
{
	"uri": "/vi/6-fargate/6.4-scaling-the-workload/",
	"title": "Scaling the workload",
	"tags": [],
	"description": "",
	"content": "Scaling the workload Một trong những lợi ích nổi bật của Fargate đó là mô hình mở rộng quy mô ngang cách đơn giản mà nó mang lại. Khi sử dụng EC2 cho việc tính toán, việc mở rộng quy mô các Pods đòi hỏi phải xem xét không chỉ các Pods sẽ được mở rộng quy mô như thế nào mà còn cả phần cứng tính toán phía dưới. Bởi vì Fargate tách biệt phần cứng tính toán cơ bản, bạn chỉ cần quan tâm đến việc mở rộng quy mô chính các Pods.\nTrong các ví dụ mà chúng ta đã xem xét cho đến nay, chỉ sử dụng một bản sao Pod duy nhất. Điều gì sẽ xảy ra nếu chúng ta mở rộng quy mô theo cách ngang như chúng ta thường mong đợi trong một kịch bản thực tế? Hãy mở rộng quy mô dịch vụ checkout và tìm hiểu:\nmodules/fundamentals/fargate/scaling/deployment.yaml Deployment/checkout Áp dụng kustomization và chờ đợi cho đến khi việc triển khai hoàn tất:\n$ kubectl apply -k ~/environment/eks-workshop/modules/fundamentals/fargate/scaling [...] $ kubectl rollout status -n checkout deployment/checkout --timeout=200s Một khi việc triển khai hoàn tất, chúng ta có thể kiểm tra số lượng Pods:\n$ kubectl get pod -n checkout -l app.kubernetes.io/component=service NAME READY STATUS RESTARTS AGE checkout-585c9b45c7-2c75m 1/1 Running 0 2m12s checkout-585c9b45c7-c456l 1/1 Running 0 2m12s checkout-585c9b45c7-xmx2t 1/1 Running 0 40m Mỗi Pod này được lên lịch trên một thể hiện Fargate riêng biệt. Bạn có thể xác nhận điều này bằng cách thực hiện các bước tương tự như trước đây và xác định node của một Pod cụ thể.\n"
},
{
	"uri": "/vi/1-introduce/1.4-taints-and-tolerations/",
	"title": "Taints and Tolerations",
	"tags": [],
	"description": "",
	"content": "Trong phần này, chúng ta sẽ tìm hiểu về taints và tolerations.\nMối quan hệ giữa Pod và node và cách bạn có thể hạn chế Pod được đặt trên node nào. Taints và Tolerations được sử dụng để đặt các hạn chế về việc Pod nào có thể được lên lịch trên một node. Chỉ có các Pod chịu đựng được taint cụ thể trên một node mới có thể được lên lịch trên node đó. Taints Sử dụng lệnh kubectl taint nodes để tạo một taint trên một node.\nCú pháp\n$ kubectl taint nodes \u0026lt;tên-node\u0026gt; key=value:taint-effect Ví dụ $ kubectl taint nodes node1 app=blue:NoSchedule Taint effect xác định điều gì sẽ xảy ra với các Pod nếu chúng không chịu đựng được taint.\nCó 3 tác động taint:\nNoSchedule PreferNoSchedule NoExecute Tolerations Tolerations được thêm vào các Pod bằng cách thêm một phần tolerations trong định nghĩa của Pod. apiVersion: v1\rkind: Pod\rmetadata:\rname: myapp-pod\rspec:\rcontainers:\r- name: nginx-container\rimage: nginx\rtolerations:\r- key: \u0026#34;app\u0026#34;\roperator: \u0026#34;Equal\u0026#34;\rvalue: \u0026#34;blue\u0026#34;\reffect: \u0026#34;NoSchedule\u0026#34; Taints và Tolerations không chỉ định cho Pod đi tới một node cụ thể. Thay vào đó, chúng chỉ định cho node chỉ chấp nhận các Pod có các toleration cụ thể. Để xem taint này, chạy lệnh dưới đây: $ kubectl describe node kubemaster |grep Taint Tài liệu tham khảo Kubernetes Kubernetes Official Documentation on Taints and Tolerations "
},
{
	"uri": "/vi/5-spot-instances/",
	"title": "Spot instances",
	"tags": [],
	"description": "",
	"content": "Spot instances Chuẩn bị môi trường cho phần này\n$ prepare-environment fundamentals/mng/spot Tất cả các node tính toán hiện tại của chúng tôi đều đang sử dụng Dung lượng On-Demand. Tuy nhiên, có nhiều \u0026ldquo;tùy chọn mua\u0026rdquo; khả dụng cho khách hàng EC2 để chạy các EKS workloads của họ.\nMột Spot Instance sử dụng dung lượng EC2 dự phòng có sẵn với giá thấp hơn giá On-Demand. Bởi vì Spot Instances cho phép bạn yêu cầu các máy EC2 không sử dụng với giảm giá đáng kể, bạn có thể giảm chi phí Amazon EC2 của mình đáng kể. Giá hàng giờ cho một Spot Instance được gọi là giá Spot. Giá Spot của mỗi loại instance trong mỗi AZ được đặt bởi Amazon EC2, và được điều chỉnh dần dần dựa trên cung cấp và cầu cảng dài hạn cho các Spot Instances. Spot Instance của bạn chạy bất cứ khi nào có dung lượng có sẵn.\nSpot Instances là lựa chọn phù hợp cho các ứng dụng linh hoạt, không trạng thái, có khả năng chịu lỗi. Điều này bao gồm các công việc đào tạo hệ thống phân tán và học máy, các công việc ETL dữ liệu lớn như Apache Spark, các ứng dụng xử lý hàng đợi và các API endpoint không trạng thái. Bởi vì Spot là dung lượng EC2 dự phòng, có thể thay đổi theo thời gian, chúng tôi khuyến nghị bạn sử dụng dung lượng Spot cho các công việc có khả năng chịu gián đoạn. Cụ thể hơn, dung lượng Spot phù hợp cho các công việc có thể chịu được các thời gian mà dung lượng cần không có sẵn.\nTrong bài thực hành này, chúng ta sẽ xem xét cách chúng ta có thể tận dụng dung lượng EC2 Spot với các Amazon EKS Managed Node Groups.\n"
},
{
	"uri": "/vi/1-introduce/1.5-node-affinity/",
	"title": "Node Affinity",
	"tags": [],
	"description": "",
	"content": "Node Affinity trong Kubernetes Node Affinity là một tính năng quan trọng trong Kubernetes, giúp đảm bảo rằng các pod được lưu trữ trên các node cụ thể, phù hợp với yêu cầu của ứng dụng. Tính năng này cho phép các nhà phát triển và quản trị hệ thống tinh chỉnh việc lên lịch chạy các pod dựa trên các thuộc tính của node, như kích thước, vùng địa lý, hoặc loại CPU.\nSo với Node Selectors, Node Affinity cung cấp khả năng sử dụng các biểu thức tiên tiến, cho phép xác định các quy tắc phức tạp hơn trong việc lựa chọn node cho pod.\nCú pháp YAML Ví dụ về Node Selector:\napiVersion: v1 kind: Pod metadata: name: myapp-pod spec: containers: - name: data-processor image: data-processor nodeSelector: size: Large Ví dụ về Node Affinity - Sử dụng In:\napiVersion: v1 kind: Pod metadata: name: myapp-pod spec: containers: - name: data-processor image: data-processor affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: size operator: In values: - Large - Medium Ví dụ về Node Affinity - Sử dụng NotIn:\napiVersion: v1 kind: Pod metadata: name: myapp-pod spec: containers: - name: data-processor image: data-processor affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: size operator: NotIn values: - Small Ví dụ về Node Affinity - Sử dụng Exists:\napiVersion: v1 kind: Pod metadata: name: myapp-pod spec: containers: - name: data-processor image: data-processor affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: size operator: Exists Các Loại Node Affinity requiredDuringSchedulingIgnoredDuringExecution: Đảm bảo pod chỉ được lên lịch trên node phù hợp khi khởi tạo. Sau khi pod được khởi tạo, việc di dời pod do thay đổi thuộc tính node không được thực hiện. preferredDuringSchedulingIgnoredDuringExecution: Hệ thống sẽ cố gắng tuân theo quy tắc này nhưng không đảm bảo. Kế Hoạch (Planned) Các loại Node Affinity đang được kế hoạch hóa để triển khai trong tương lai bao gồm:\nrequiredDuringSchedulingRequiredDuringExecution preferredDuringSchedulingRequiredDuringExecution K8s Reference Docs Node Affinity on Kubernetes.io Advanced Scheduling in Kubernetes - Kubernetes Blog "
},
{
	"uri": "/vi/6-fargate/",
	"title": "Fargate",
	"tags": [],
	"description": "",
	"content": "Fargate Chuẩn bị môi trường cho phần này: $ prepare-environment fundamentals/fargate Điều này sẽ thực hiện các thay đổi sau vào môi trường thực hành của bạn:\nTạo một vai trò IAM để sử dụng bởi Fargate Trong module trước đó, chúng ta đã thấy cách cung cấp các máy tính EC2 để chạy Pods trong cụm EKS của chúng ta, và cách các nhóm nút được quản lý giúp giảm bớt gánh nặng vận hành. Tuy nhiên, trong mô hình này, bạn vẫn phải chịu trách nhiệm về tính sẵn có, công suất và bảo dưỡng của cơ sở hạ tầng cơ bản.\nAWS Fargate là một công nghệ cung cấp dung lượng tính toán được kích thước đúng theo nhu cầu cho các container. Với AWS Fargate, bạn không cần phải cấu hình, cấu hình hoặc mở rộng các nhóm máy ảo một cách tự động để chạy các container. Bạn cũng không cần phải chọn loại máy chủ, quyết định khi nào để mở rộng các nhóm nút của bạn, hoặc tối ưu hóa việc đóng gói cụm. Bạn có thể kiểm soát các Pod nào bắt đầu trên Fargate và cách chúng chạy với các hồ sơ Fargate. Các hồ sơ Fargate được xác định như một phần của cụm Amazon EKS của bạn.\nAmazon EKS tích hợp Kubernetes với AWS Fargate bằng cách sử dụng các bộ điều khiển được xây dựng bởi AWS bằng cách sử dụng mô hình mở rộng được cung cấp bởi Kubernetes. Những bộ điều khiển này chạy như một phần của mặt trận điều khiển Kubernetes quản lý của Amazon EKS và chịu trách nhiệm về lập lịch các Pod Kubernetes nguyên bản vào Fargate. Các bộ điều khiển Fargate bao gồm một trình lập lịch mới chạy song song với trình lập lịch Kubernetes mặc định cùng với một số bộ điều khiển nhập và xác nhận. Khi bạn bắt đầu một Pod đáp ứng các tiêu chí để chạy trên Fargate, các bộ điều khiển Fargate đang chạy trong cụm nhận ra, cập nhật và lên lịch cho Pod trên Fargate.\nCác lợi ích của Fargate bao gồm:\nAWS Fargate cho phép bạn tập trung vào ứng dụng của bạn. Bạn xác định nội dung ứng dụng, mạng, lưu trữ và yêu cầu co dãn của bạn. Không cần phải cấu hình, vá lỗi, quản lý công suất cụm, hoặc quản lý cơ sở hạ tầng. AWS Fargate hỗ trợ tất cả các trường hợp sử dụng container phổ biến bao gồm ứng dụng kiến trúc microservices, xử lý theo lô, ứng dụng học máy và di chuyển ứng dụng từ trên cơ sở sang đám mây. Chọn AWS Fargate vì mô hình cô lập và bảo mật của nó. Bạn cũng nên chọn Fargate nếu bạn muốn khởi chạy các container mà không cần phải cấu hình hoặc quản lý các máy EC2. Nếu bạn cần kiểm soát lớn hơn đối với các máy EC2 của bạn hoặc các tùy chọn tùy chỉnh rộng hơn, sau đó sử dụng ECS hoặc EKS mà không có Fargate. "
},
{
	"uri": "/vi/1-introduce/1.6-resource-limits/",
	"title": "Giới Hạn Tài Nguyên",
	"tags": [],
	"description": "",
	"content": "Giới Hạn Tài Nguyên Trong phần này, chúng ta sẽ xem xét về Giới Hạn Tài Nguyên.\nHãy xem xét một cụm Kubernetes gồm 3 nút. Mỗi nút có một tập hợp các tài nguyên CPU, Bộ nhớ và Ổ đĩa có sẵn.\nNếu không có đủ tài nguyên nào trên các nút, Kubernetes sẽ giữ việc lên lịch cho pod. Bạn sẽ thấy pod ở trạng thái đang chờ. Nếu bạn xem các sự kiện, bạn sẽ thấy lý do là CPU không đủ.\nYêu Cầu Tài Nguyên Mặc định, K8s giả định rằng một pod hoặc container trong một pod yêu cầu 0.5 CPU và 256Mi của bộ nhớ. Điều này được biết đến là Yêu Cầu Tài Nguyên cho một container.\nNếu ứng dụng của bạn trong pod yêu cầu nhiều hơn các tài nguyên mặc định, bạn cần phải thiết lập chúng trong tệp định nghĩa pod.\napiVersion: v1 kind: Pod metadata: name: simple-webapp-color labels: name: simple-webapp-color spec: containers: - name: simple-webapp-color image: simple-webapp-color ports: - containerPort: 8080 resources: requests: memory: \u0026#34;1Gi\u0026#34; cpu: \u0026#34;1\u0026#34; Giới Hạn Tài Nguyên Mặc định, K8s thiết lập giới hạn tài nguyên là 1 CPU và 512Mi của bộ nhớ.\nBạn có thể thiết lập giới hạn tài nguyên trong tệp định nghĩa pod.\napiVersion: v1 kind: Pod metadata: name: simple-webapp-color labels: name: simple-webapp-color spec: containers: - name: simple-webapp-color image: simple-webapp-color ports: - containerPort: 8080 resources: requests: memory: \u0026#34;1Gi\u0026#34; cpu: \u0026#34;1\u0026#34; limits: memory: \u0026#34;2Gi\u0026#34; cpu: \u0026#34;2\u0026#34; Ghi chú: Hãy nhớ Yêu Cầu và Giới Hạn cho các tài nguyên được thiết lập cho mỗi container trong pod.\nVượt Quá Giới Hạn Điều gì xảy ra khi một pod cố gắng vượt qua các tài nguyên vượt quá giới hạn của nó?\nK8s Reference Docs: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\n"
},
{
	"uri": "/vi/1-introduce/1.7-daemonsets/",
	"title": "DaemonSets",
	"tags": [],
	"description": "",
	"content": "DaemonSets trong Kubernetes Trong phần này, chúng ta sẽ tìm hiểu về DaemonSets.\nDaemonSets giống như ReplicaSets, vì nó giúp triển khai nhiều phiên bản của pod. Nhưng DaemonSets chạy một bản sao của pod của bạn trên mỗi node trong cụm của bạn.\nCác Trường Hợp Sử Dụng của DaemonSets Tình huống sử dụng DaemonSets thích hợp cho việc chạy một bản sao của pod trên mỗi node để thực hiện các nhiệm vụ như:\nGiám sát hệ thống Log collection Deployment của network policy Tình huống sử dụng - Khái quát Trong một số trường hợp, DaemonSets được sử dụng để triển khai các ứng dụng cần phải chạy trên tất cả hoặc một số nodes nhất định để cung cấp các dịch vụ cần thiết cho các pods khác.\nTình huống sử dụng - Các vấn đề cần lưu ý Khi sử dụng DaemonSets, cần lưu ý đến khả năng tài nguyên và bảo mật, vì việc triển khai một pod trên mỗi node có thể ảnh hưởng đến tài nguyên hệ thống.\nĐịnh Nghĩa về DaemonSets Tạo một DaemonSet tương tự như quá trình tạo ReplicaSet.\nĐối với DaemonSets, chúng ta bắt đầu với apiVersion, kind là DaemonSet thay vì ReplicaSet, metadata và spec.\napiVersion: apps/v1 kind: DaemonSet metadata: name: monitoring-daemon labels: app: nginx spec: selector: matchLabels: app: monitoring-agent template: metadata: labels: app: monitoring-agent spec: containers: - name: monitoring-agent image: monitoring-agent Để tạo một daemonset từ một tệp định nghĩa:\n$ kubectl create -f daemon-set-definition.yaml Xem Danh Sách các DaemonSets Để liệt kê các daemonsets:\n$ kubectl get daemonsets Để xem thêm thông tin chi tiết về các daemonsets:\n$ kubectl describe daemonsets monitoring-daemon K8s Reference Docs https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#writing-a-daemonset-spec\n"
},
{
	"uri": "/vi/1-introduce/1.8-static-pods/",
	"title": "Static Pods",
	"tags": [],
	"description": "",
	"content": "Trong phần này, chúng ta sẽ xem xét về Static Pods.\nLàm thế nào để cung cấp một tập tin định nghĩa pod cho kubelet mà không cần có kube-apiserver? Bạn có thể cấu hình kubelet để đọc các tập tin định nghĩa pod từ một thư mục trên máy chủ được chỉ định để lưu thông tin về các pod.\nCấu hình Static Pod Thư mục được chỉ định có thể là bất kỳ thư mục nào trên máy chủ và vị trí của thư mục đó được truyền vào kubelet dưới dạng một tùy chọn trong quá trình chạy dịch vụ.\nTùy chọn được gọi là --pod-manifest-path.\nCách cấu hình static pod khác Thay vì chỉ định tùy chọn trực tiếp trong tệp kubelet.service, bạn có thể cung cấp một đường dẫn đến một tệp cấu hình khác bằng cách sử dụng tùy chọn cấu hình và xác định đường dẫn thư mục là staticPodPath trong tệp đó.\nXem các static pod Để xem các static pod\n$ docker ps Kubelet có thể tạo cả hai loại pod - các static pod và các pod từ máy chủ api cùng một lúc.\nStatic Pods - Use Case Static Pods trong Kubernetes là các pod được quản lý trực tiếp bởi kubelet daemon trên một node cụ thể, không qua apiserver. Điều này có nghĩa là chúng không thể được quản lý hoặc điều khiển từ xa bởi các bộ phận khác của Kubernetes như kubectl hoặc apiserver. Static Pods thường được sử dụng trong các trường hợp cụ thể, ví dụ như khởi chạy các thành phần hệ thống của Kubernetes như kube-proxy, etcd, hoặc kube-apiserver chính trên cluster.\n1. Use Case: Quản lý cụm Kubernetes trên các môi trường không có hoặc giới hạn truy cập mạng Khi triển khai K#ubernetes trong một môi trường không có hoặc có truy cập mạng giới hạn (ví dụ: mạng riêng, môi trường sản xuất ngoại tuyến), việc sử dụng Static Pods cho phép các nhà quản trị triển khai và quản lý các thành phần cốt lõi của Kubernetes mà không cần kết nối mạng đến apiserver. Điều này giúp đảm bảo rằng cụm có thể tiếp tục hoạt động ngay cả khi mất kết nối mạng.\n2. Use Case: Bootstrapping cụm Kubernetes Trong quá trình khởi tạo cụm Kubernetes, các Static Pods thường được sử dụng để chạy các thành phần cốt lõi của cụm như kube-apiserver, etcd, và controller manager trước khi cụm có thể tự quản lý qua apiserver. Điều này giúp khởi tạo cụm một cách tự động mà không cần can thiệp thủ công.\n3. Use Case: Chạy các ứng dụng cần chế độ khả dụng cao Vì Static Pods được quản lý trực tiếp bởi kubelet, chúng sẽ được tự động khởi động lại bởi kubelet nếu chúng thất bại. Điều này có thể hữu ích cho các ứng dụng cần độ khả dụng cao và tự phục hồi mà không phụ thuộc vào các cơ chế điều khiển của Kubernetes thông thường.\n4. Use Case: Triển khai đơn giản trên môi trường có tài nguyên hạn chế Trong một số trường hợp, việc triển khai một cụm Kubernetes đầy đủ có thể không khả thi do hạn chế về tài nguyên hệ thống. Trong những trường hợp này, việc sử dụng Static Pods cho phép triển khai các ứng dụng một cách đơn giản mà không cần các thành phần điều khiển phức tạp, giảm bớt yêu cầu về tài nguyên.\nCách tạo Static Pod Để tạo Static Pod, bạn cần đặt tệp YAML hoặc JSON mô tả pod vào một thư mục mà kubelet được cấu hình để theo dõi (thông qua tùy chọn --pod-manifest-path của kubelet). Kubelet sẽ tự động tạo và quản lý các Pod dựa trên các tệp mô tả này.\nTrong kịch bản triển khai thực tế, việc sử dụng Static Pods yêu cầu sự hiểu biết vững chắc về cách kubelet và các thành phần khác của Kubernetes hoạt động. Mặc dù chúng không phải là giải pháp phù hợp cho mọi tình huống, nhưng Static Pods cung cấp một công cụ linh hoạt và mạnh mẽ cho các tình huống đặc biệt.\nStatic Pods vs DaemonSets Tiêu Chí Static Pods DaemonSets Khởi tạo Khởi tạo bởi kubelet trên mỗi node Khởi tạo bởi API server qua một bản mô tả DaemonSet Quản lý Quản lý trực tiếp bởi kubelet trên node mà nó chạy Quản lý thông qua API server với các cơ chế điều khiển khác Cập nhật Cập nhật bằng cách sửa đổi file cấu hình trên node Cập nhật thông qua cập nhật bản mô tả DaemonSet trong API server Phạm vi hoạt động Chỉ chạy trên một node cụ thể, không được quản lý bởi API server Chạy trên tất cả các node hoặc các node được chọn lọc thông qua mô hình nhãn (labels) Mục đích sử dụng Phù hợp cho các dịch vụ cấp thấp và cơ bản của hệ thống, như kube-proxy, kube-dns Phù hợp cho các dịch vụ cần chạy trên mỗi node, như log collectors, monitoring agents Tự động khởi động lại Kubelet sẽ tự động khởi động lại Pod nếu nó bị lỗi API server đảm bảo Pod luôn được chạy trên các node như mô tả Tính linh hoạt Ít linh hoạt hơn, vì cần sửa đổi file cấu hình và quản lý trực tiếp trên mỗi node Linh hoạt hơn, có thể dễ dàng cập nhật và quản lý từ một nơi central thông qua API server Khả năng quản lý Quản lý tập trung khó khăn hơn do cần can thiệp trực tiếp vào mỗi node Dễ dàng quản lý tập trung thông qua kubectl và các công cụ khác qua API server Tài liệu tham khảo Kubernetes https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/\n"
},
{
	"uri": "/vi/1-introduce/1.9-multiple-schedulers/",
	"title": "Multiple Schedulers",
	"tags": [],
	"description": "",
	"content": "Trong bài viết này, chúng ta sẽ khám phá cách triển khai nhiều scheduler trong một cluster Kubernetes. Việc có nhiều scheduler cho phép bạn tùy chỉnh cách các pod được lên lịch trên cluster của mình, tối ưu hóa việc sử dụng tài nguyên hoặc đáp ứng các yêu cầu phức tạp về cách triển khai.\nCustom Schedulers Một cluster Kubernetes có thể cấu hình để lên lịch cho các pod sử dụng nhiều scheduler. Điều này cho phép sử dụng scheduler mặc định của Kubernetes cùng với một hoặc nhiều scheduler tùy chỉnh.\nTriển khai scheduler bổ sung Tải xuống binary Để thêm một scheduler mới, bạn cần tải xuống binary phù hợp:\n$ wget https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kube-scheduler Triển khai scheduler bổ sung - kubeadm Sau khi có binary, bạn có thể triển khai scheduler mới sử dụng kubectl:\n$ kubectl create -f my-custom-scheduler.yaml Xem danh sách Schedulers Bạn có thể xem danh sách các scheduler đang hoạt động trong cluster bằng cách liệt kê các pod trong namespace kube-system:\n$ kubectl get pods -n kube-system Sử dụng Custom Scheduler Để sử dụng một scheduler tùy chỉnh cho một pod cụ thể, bạn chỉ cần thêm một trường schedulerName vào định nghĩa pod và chỉ định tên của scheduler bạn muốn sử dụng.\napiVersion: v1\rkind: Pod\rmetadata:\rname: nginx\rspec:\rcontainers:\r- image: nginx\rname: nginx\rschedulerName: my-custom-scheduler Để tạo pod sử dụng định nghĩa trên:\n$ kubectl create -f pod-definition.yaml Và sau đó bạn có thể kiểm tra danh sách các pod để xem pod mới đã được lên lịch chưa:\n$ kubectl get pods Xem Sự kiện Bạn cũng có thể xem các sự kiện liên quan đến lên lịch pod để kiểm tra xem có lỗi nào không:\n$ kubectl get events Xem Log của Scheduler Cuối cùng, để kiểm tra hoạt động của scheduler tùy chỉnh, bạn có thể xem log của nó:\n$ kubectl logs my-custom-scheduler -n kube-system K8s Reference Docs Để biết thêm thông tin về cách cấu hình và sử dụng nhiều scheduler trong Kubernetes, bạn có thể tham khảo tại tài liệu chính thức của Kubernetes.\n"
},
{
	"uri": "/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]