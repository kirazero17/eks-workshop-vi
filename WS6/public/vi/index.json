[
{
	"uri": "/vi/3-network-policies/3.1-setup/",
	"title": "Cài đặt",
	"tags": [],
	"description": "",
	"content": "Trong lab này, chúng ta sẽ triển khai các chính sách mạng cho ứng dụng mẫu được triển khai trong cụm lab. Kiến ​​trúc thành phần ứng dụng mẫu được hiển thị như dưới đây.\nMỗi thành phần trong ứng dụng mẫu được triển khai trong không gian tên riêng của nó. Ví dụ, thành phần \u0026lsquo;ui\u0026rsquo; được triển khai trong không gian tên \u0026lsquo;ui\u0026rsquo;, trong khi dịch vụ web \u0026lsquo;catalog\u0026rsquo; và cơ sở dữ liệu MySQL \u0026lsquo;catalog\u0026rsquo; được triển khai trong không gian tên \u0026lsquo;catalog\u0026rsquo;.\nHiện tại, không có chính sách mạng nào được xác định, và bất kỳ thành phần nào trong ứng dụng mẫu đều có thể giao tiếp với bất kỳ thành phần nào khác hoặc bất kỳ dịch vụ ngoại vi nào. Ví dụ, thành phần \u0026lsquo;catalog\u0026rsquo; có thể giao tiếp trực tiếp với thành phần \u0026lsquo;checkout\u0026rsquo;. Chúng ta có thể xác minh điều này bằng các lệnh dưới đây:\n$ kubectl exec deployment/catalog -n catalog -- curl -s http://checkout.checkout/health {\u0026#34;status\u0026#34;:\u0026#34;ok\u0026#34;,\u0026#34;info\u0026#34;:{},\u0026#34;error\u0026#34;:{},\u0026#34;details\u0026#34;:{}} Hãy bắt đầu bằng cách triển khai một số quy tắc mạng để chúng ta có thể kiểm soát tốt hơn luồng dữ liệu cho ứng dụng mẫu.\n"
},
{
	"uri": "/vi/1-introduce/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Hiểu về mạng trong Kubernetes Hiểu về mạng trong Kubernetes là vô cùng quan trọng để vận hành cụm và ứng dụng của bạn một cách hiệu quả. Trong chương này, chúng ta sẽ đào sâu vào các khía cạnh khác nhau của mạng trong Kubernetes, bao gồm mạng Pod, mạng dịch vụ, và giao tiếp dịch vụ.\nTrong Amazon EKS, mạng Pod, còn được gọi là mạng cụm, được giải quyết thông qua việc sử dụng một plugin CNI của Kubernetes gọi là Amazon VPC CNI. Chúng tôi rất khuyến khích khám phá các tùy chọn khác nhau có sẵn với Amazon VPC CNI trước khi chuyển sang Amazon VPC Lattice.\nCác phần cần tìm hiểu Switching, Routing and Gateways Switching Routing Default Gateway DNS DNS Configuration on Linux CoreDNS Network Namespace Docker Networking CNI Cluster Networking Pod Networking CNI in Kubernetes CoreDNS Ingress "
},
{
	"uri": "/vi/4-security-groups-for-pods/4.1-introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Sử dụng Amazon EKS cho ứng dụng Catalog của bạn Trong kiến trúc của chúng tôi, thành phần catalog sử dụng một cơ sở dữ liệu MySQL làm backend lưu trữ. Cách mà API catalog hiện đang triển khai sử dụng một cơ sở dữ liệu được triển khai dưới dạng một Pod trong cụm EKS.\nBạn có thể thấy điều này bằng cách chạy lệnh sau:\n$ kubectl -n catalog get pod NAME READY STATUS RESTARTS AGE catalog-5d7fc9d8f-xm4hs 1/1 Running 0 14m catalog-mysql-0 1/1 Running 0 14m Trong trường hợp trên, Pod catalog-mysql-0 là một Pod MySQL. Chúng ta có thể xác minh ứng dụng catalog của chúng ta đang sử dụng điều này bằng cách kiểm tra môi trường của nó:\n$ kubectl -n catalog exec deployment/catalog -- env \\ | grep DB_ENDPOINT DB_ENDPOINT=catalog-mysql:3306 Chúng tôi muốn di chuyển ứng dụng của mình để sử dụng dịch vụ Amazon RDS được quản lý đầy đủ để tận dụng hoàn toàn quy mô và đáng tin cậy mà nó cung cấp.\n"
},
{
	"uri": "/vi/",
	"title": "Kubernetes trên AWS",
	"tags": [],
	"description": "",
	"content": "Kubernetes trên AWS Kubernetes là một nền tảng mã nguồn mở, linh hoạt, có khả năng mở rộng, phục vụ việc quản lý các ứng dụng được đóng gói và các dịch vụ liên quan, giúp việc cấu hình và tự động hóa quá trình triển khai ứng dụng trở nên thuận tiện hơn. Được biết đến như một hệ sinh thái lớn và phát triển nhanh chóng, Kubernetes cung cấp sự hỗ trợ rộng rãi qua các dịch vụ và công cụ đa dạng.\nTên Kubernetes bắt nguồn từ tiếng Hy Lạp, nghĩa là người lái tàu hoặc hoa tiêu. Kubernetes được Google công bố mã nguồn vào năm 2014, dựa trên gần một thập kỷ kinh nghiệm quản lý workload lớn trong thực tế của Google, kết hợp với các ý tưởng và best practices từ cộng đồng.\nQuay ngược thời gian Hãy xem xét tại sao Kubernetes lại quan trọng thông qua việc nhìn lại quá khứ.\nThời kỳ triển khai truyền thống: Ban đầu, các ứng dụng được chạy trực tiếp trên máy chủ vật lý, khiến việc phân bổ tài nguyên gặp khó khăn do không có cơ chế xác định ranh giới tài nguyên cho từng ứng dụng. Cách tiếp cận này dẫn đến nguy cơ một ứng dụng có thể sử dụng quá nhiều tài nguyên, ảnh hưởng đến hoạt động của các ứng dụng khác. Giải pháp là chạy mỗi ứng dụng trên một máy chủ vật lý riêng biệt, nhưng điều này lại không hiệu quả về mặt chi phí và tài nguyên.\nThời kỳ triển khai ảo hóa: Ảo hóa được giới thiệu như một giải pháp cho phép chạy nhiều Máy ảo (VM) trên cùng một máy chủ vật lý, giúp cô lập ứng dụng và tăng cường bảo mật. Ảo hóa cũng giúp cải thiện hiệu quả sử dụng tài nguyên và khả năng mở rộng.\nThời kỳ triển khai Container: Container giống như VM nhưng nhẹ hơn và chia sẻ Hệ điều hành (HĐH) với nhau. Container mang lại nhiều lợi ích như tạo mới và triển khai ứng dụng nhanh chóng, phát triển và triển khai liên tục, phân biệt rõ ràng giữa quá trình phát triển và vận hành, cung cấp tính nhất quán qua các môi trường, khả năng di chuyển giữa các cloud và HĐH, và quản lý ứng dụng tập trung.\nTại sao bạn cần Kubernetes và nó có thể làm gì? Container là phương tiện hiệu quả để đóng gói và chạy ứng dụng của bạn. Trong môi trường sản xuất, cần có cơ chế quản lý các container một cách hiệu quả, đảm bảo không có downtime. Kubernetes giúp quản lý các hệ thống phân tán mạnh mẽ, tự động hóa việc nhân rộng, cung cấp các mẫu triển khai và nhiều hơn nữa.\nKubernetes mang lại:\nPhát hiện dịch vụ và cân bằng tải Điều phối bộ nhớ Tự động rollouts và rollbacks Đóng gói tự động Tự phục hồi Quản lý cấu hình và bảo mật Những gì Kubernetes không phải là Kubernetes không phải là một hệ thống PaaS truyền thống, toàn diện. Nó hoạt động ở tầng container, cung cấp tính năng giống như PaaS như triển khai, nhân rộng, cân bằng tải, nhưng là một giải pháp linh hoạt và có thể mở rộng, không giới hạn loại ứng dụng được hỗ trợ, không triển khai mã nguồn hoặc build ứng dụng, không cung cấp dịch vụ ứng dụng cấp cao như middleware, databases, không bắt buộc sử dụng các giải pháp ghi nhật ký, giám sát hoặc cảnh báo, và không cung cấp hoặc áp dụng bất kỳ cấu hình toàn diện, bảo trì, quản lý hoặc hệ thống tự phục hồi. Kubernetes loại bỏ nhu cầu về điều phối truyền thống, thay vào đó là kiểm soát liên tục từ trạng thái hiện tại sang trạng thái mong muốn.\n"
},
{
	"uri": "/vi/1-introduce/1.1-switching-routing-gateways/",
	"title": "Switching Routing Gateways",
	"tags": [],
	"description": "",
	"content": "Switching Routing Gateways Trong phần này, chúng ta sẽ xem xét Chuyển đổi, Định tuyến và Cổng kết nối (Switching, Routing and Gateways)\nChuyển đổi (Switching) Để xem giao diện trên hệ thống máy chủ $ ip link Để xem địa chỉ IP của các giao diện. $ ip addr Định tuyến (Routing) Để xem bảng định tuyến hiện tại trên hệ thống máy chủ. $ route $ ip route show\rhoặc\r$ ip route list Để thêm các mục vào bảng định tuyến. $ ip route add 192.168.1.0/24 via 192.168.2.1 Cổng kết nối (Gateways) Để thêm một tuyến mặc định. $ ip route add default via 192.168.2.1 Để kiểm tra xem chuyển tiếp IP đã được bật trên máy chủ chưa. $ cat /proc/sys/net/ipv4/ip_forward\r0\r$ echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward Bật chuyển tiếp gói tin cho IPv4. $ cat /etc/sysctl.conf\r# Bỏ dấu chú thích trong dòng\rnet.ipv4.ip_forward=1 Để xem các biến sysctl. $ sysctl -a Để tải lại cấu hình sysctl. $ sysctl --system "
},
{
	"uri": "/vi/5-custom-networking/5.1-vpc-architecture/",
	"title": "VPC architecture",
	"tags": [],
	"description": "",
	"content": "VPC architecture Khám phá VPC đã được thiết lập Chúng ta có thể bắt đầu bằng cách kiểm tra VPC đã được thiết lập. Ví dụ, mô tả VPC:\n$ aws ec2 describe-vpcs --vpc-ids $VPC_ID { \u0026#34;Vpcs\u0026#34;: [ { \u0026#34;CidrBlock\u0026#34;: \u0026#34;10.42.0.0/16\u0026#34;, \u0026#34;DhcpOptionsId\u0026#34;: \u0026#34;dopt-0b9864a5c5bbe59bf\u0026#34;, \u0026#34;State\u0026#34;: \u0026#34;available\u0026#34;, \u0026#34;VpcId\u0026#34;: \u0026#34;vpc-0512db3d3af8fa5b0\u0026#34;, \u0026#34;OwnerId\u0026#34;: \u0026#34;188130284088\u0026#34;, \u0026#34;InstanceTenancy\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;CidrBlockAssociationSet\u0026#34;: [ { \u0026#34;AssociationId\u0026#34;: \u0026#34;vpc-cidr-assoc-04cf2a625fa24724b\u0026#34;, \u0026#34;CidrBlock\u0026#34;: \u0026#34;10.42.0.0/16\u0026#34;, \u0026#34;CidrBlockState\u0026#34;: { \u0026#34;State\u0026#34;: \u0026#34;associated\u0026#34; } }, { \u0026#34;AssociationId\u0026#34;: \u0026#34;vpc-cidr-assoc-0453603b1ab691914\u0026#34;, \u0026#34;CidrBlock\u0026#34;: \u0026#34;100.64.0.0/16\u0026#34;, \u0026#34;CidrBlockState\u0026#34;: { \u0026#34;State\u0026#34;: \u0026#34;associated\u0026#34; } } ], \u0026#34;IsDefault\u0026#34;: false, \u0026#34;Tags\u0026#34;: [ { \u0026#34;Key\u0026#34;: \u0026#34;created-by\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;eks-workshop-v2\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;env\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;cluster\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;Name\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;eks-workshop-vpc\u0026#34; } ] } ] } Ở đây, chúng ta thấy có hai phạm vi CIDR được liên kết với VPC:\nPhạm vi 10.42.0.0/16 là CIDR \u0026ldquo;chính\u0026rdquo; Phạm vi 100.64.0.0/16 là CIDR \u0026ldquo;phụ\u0026rdquo; Bạn cũng có thể xem điều này trong bảng điều khiển AWS:\nhttps://console.aws.amazon.com/vpc/home#vpcs:tag:created-by=eks-workshop-v2\nViệc mô tả các subnet liên kết với VPC sẽ hiển thị 9 subnet:\n$ aws ec2 describe-subnets --filters \u0026#34;Name=tag:created-by,Values=eks-workshop-v2\u0026#34; \\ --query \u0026#34;Subnets[*].CidrBlock\u0026#34; [ \u0026#34;10.42.64.0/19\u0026#34;, \u0026#34;100.64.32.0/19\u0026#34;, \u0026#34;100.64.0.0/19\u0026#34;, \u0026#34;100.64.64.0/19\u0026#34;, \u0026#34;10.42.160.0/19\u0026#34;, \u0026#34;10.42.0.0/19\u0026#34;, \u0026#34;10.42.96.0/19\u0026#34;, \u0026#34;10.42.128.0/19\u0026#34;, \u0026#34;10.42.32.0/19\u0026#34; ] Các subnet này được chia thành:\nSubnet công cộng: Một cho mỗi khu vực có sẵn sử dụng một CIDR block từ phạm vi CIDR chính Subnet riêng: Một cho mỗi khu vực có sẵn sử dụng một CIDR block từ phạm vi CIDR chính Subnet riêng phụ: Một cho mỗi khu vực có sẵn sử dụng một CIDR block từ phạm vi CIDR phụ Bạn có thể xem các subnet này trong bảng điều khiển AWS:\nhttps://console.aws.amazon.com/vpc/home#subnets:tag:created-by=eks-workshop-v2;sort=desc:CidrBlock\nHiện tại, các pods của chúng ta đang tận dụng các subnet riêng 10.42.96.0/19, 10.42.128.0/19 và 10.42.160.0/19. Trong bài thực hành này, chúng ta sẽ di chuyển chúng để sử dụng địa chỉ IP từ các subnet 100.64.\n"
},
{
	"uri": "/vi/2-prerequiste/",
	"title": "Các Bước Chuẩn Bị",
	"tags": [],
	"description": "",
	"content": "Các Bước Chuẩn Bị IAM Roles cho Service Accounts TRƯỚC KHI BẮT ĐẦU Chuẩn bị môi trường cho phần này:\nprepare-environment security/irsa Điều này sẽ thực hiện các thay đổi sau vào môi trường lab của bạn:\nTạo một bảng Amazon DynamoDB Tạo một vai trò IAM cho các tải trọng công việc AmazonEKS để truy cập vào bảng DynamoDB Cài đặt AWS Load Balancer Controller trong cụm Amazon EKS Ứng dụng trong các container của Pod có thể sử dụng AWS SDK hoặc AWS CLI để thực hiện các yêu cầu API đến các dịch vụ AWS bằng quyền IAM. Ví dụ, ứng dụng có thể cần tải lên các tệp vào một bucket S3 hoặc truy vấn một bảng DynamoDB. Để làm điều này, các ứng dụng phải ký các yêu cầu API AWS của họ bằng thông tin đăng nhập AWS. IAM Roles cho Service Accounts (IRSA) cung cấp khả năng quản lý thông tin đăng nhập cho các ứng dụng của bạn, tương tự như cách các Hồ sơ Thể hiện IAM cung cấp thông tin đăng nhập cho các thể hiện Amazon EC2. Thay vì tạo và phân phối thông tin đăng nhập AWS của bạn cho các container hoặc phụ thuộc vào Hồ sơ Thể hiện Amazon EC2 để cấp quyền, bạn kết hợp một Vai trò IAM với một Tài khoản Dịch vụ Kubernetes và cấu hình Pods của bạn để sử dụng Tài khoản Dịch vụ đó.\nTrong chương này, chúng tôi sẽ cấu hình lại một trong các thành phần ứng dụng mẫu để tận dụng một API AWS và cung cấp cho nó xác thực phù hợp.\n"
},
{
	"uri": "/vi/5-custom-networking/5.2-configure-amazon-vpc-cni/",
	"title": "Cấu hình Amazon VPC CNI",
	"tags": [],
	"description": "",
	"content": "Cấu hình Amazon VPC CNI Chúng ta sẽ bắt đầu bằng việc cấu hình Amazon VPC CNI. VPC của chúng ta đã được cấu hình lại với sự thêm vào của một CIDR phụ với phạm vi 100.64.0.0/16:\n$ aws ec2 describe-vpcs --vpc-ids $VPC_ID | jq \u0026#39;.Vpcs[0].CidrBlockAssociationSet\u0026#39; [ { \u0026#34;AssociationId\u0026#34;: \u0026#34;vpc-cidr-assoc-0ef3fae4a0abc4a42\u0026#34;, \u0026#34;CidrBlock\u0026#34;: \u0026#34;10.42.0.0/16\u0026#34;, \u0026#34;CidrBlockState\u0026#34;: { \u0026#34;State\u0026#34;: \u0026#34;associated\u0026#34; } }, { \u0026#34;AssociationId\u0026#34;: \u0026#34;vpc-cidr-assoc-0a6577e1404081aef\u0026#34;, \u0026#34;CidrBlock\u0026#34;: \u0026#34;100.64.0.0/16\u0026#34;, \u0026#34;CidrBlockState\u0026#34;: { \u0026#34;State\u0026#34;: \u0026#34;associated\u0026#34; } } ] Điều này có nghĩa là chúng ta hiện có một phạm vi CIDR riêng biệt mà chúng ta có thể sử dụng bổ sung vào phạm vi CIDR mặc định, trong đầu ra trên là 10.42.0.0/16. Từ phạm vi CIDR mới này, chúng ta đã thêm 3 mạng con mới vào VPC sẽ được sử dụng để chạy các pods của chúng ta:\n$ echo \u0026#34;Mạng con phụ trong AZ $SUBNET_AZ_1 là $SECONDARY_SUBNET_1\u0026#34; $ echo \u0026#34;Mạng con phụ trong AZ $SUBNET_AZ_2 là $SECONDARY_SUBNET_2\u0026#34; $ echo \u0026#34;Mạng con phụ trong AZ $SUBNET_AZ_3 là $SECONDARY_SUBNET_3\u0026#34; Để kích hoạt mạng tùy chỉnh, chúng ta phải thiết lập biến môi trường AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG thành true trong aws-node DaemonSet.\n$ kubectl set env daemonset aws-node -n kube-system AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true Sau đó, chúng ta sẽ tạo một tài nguyên tùy chỉnh ENIConfig cho mỗi mạng con mà các pod sẽ được triển khai vào:\nmanifests/modules/networking/custom-networking/provision/eniconfigs.yaml Hãy áp dụng chúng vào cụm của chúng ta:\n$ kubectl kustomize ~/environment/eks-workshop/modules/networking/custom-networking/provision \\ | envsubst | kubectl apply -f- Xác nhận rằng các đối tượng ENIConfig đã được tạo:\n$ kubectl get ENIConfigs Cuối cùng, chúng ta sẽ cập nhật aws-node DaemonSet để tự động áp dụng ENIConfig cho một Zone khả dụng cho bất kỳ node Amazon EC2 mới nào được tạo trong cụm EKS.\n$ kubectl set env daemonset aws-node -n kube-system ENI_CONFIG_LABEL_DEF=topology.kubernetes.io/zone "
},
{
	"uri": "/vi/1-introduce/1.2-dns/",
	"title": "DNS",
	"tags": [],
	"description": "",
	"content": "Name Resolution Với sự giúp đỡ của lệnh ping. Kiểm tra khả năng tiếp cận của Địa chỉ IP trên Mạng. $ ping 172.17.0.64 PING 172.17.0.64 (172.17.0.64) 56(84) bytes of data. 64 bytes from 172.17.0.64: icmp_seq=1 ttl=64 time=0.384 ms 64 bytes from 172.17.0.64: icmp_seq=2 ttl=64 time=0.415 ms Kiểm tra với tên máy chủ của họ $ ping web ping: unknown host web Thêm mục nhập trong tệp /etc/hosts để giải quyết bằng tên máy chủ của họ. $ cat \u0026gt;\u0026gt; /etc/hosts 172.17.0.64 web # Nhấn Ctrl + c để thoát Nó sẽ tìm trong tệp /etc/hosts. $ ping web PING web (172.17.0.64) 56(84) bytes of data. 64 bytes from web (172.17.0.64): icmp_seq=1 ttl=64 time=0.491 ms 64 bytes from web (172.17.0.64): icmp_seq=2 ttl=64 time=0.636 ms $ ssh web $ curl http://web DNS Mỗi máy có tệp cấu hình giải quyết DNS tại /etc/resolv.conf. $ cat /etc/resolv.conf nameserver 127.0.0.53 options edns0 Để thay đổi thứ tự giải quyết dns, chúng ta cần thay đổi trong tệp /etc/nsswitch.conf. $ cat /etc/nsswitch.conf hosts: files dns networks: files Nếu nó thất bại trong một số điều kiện. $ ping wwww.github.com ping: www.github.com: Temporary failure in name resolution Thêm máy chủ tên công cộng được biết đến vào tệp /etc/resolv.conf. $ cat /etc/resolv.conf nameserver 127.0.0.53 nameserver 8.8.8.8 options edns0 $ ping www.github.com PING github.com (140.82.121.3) 56(84) bytes of data. 64 bytes from 140.82.121.3 (140.82.121.3): icmp_seq=1 ttl=57 time=7.07 ms 64 bytes from 140.82.121.3 (140.82.121.3): icmp_seq=2 ttl=57 time=5.42 ms Domain Names Search Domain Record Types Networking Tools Công cụ mạng hữu ích để kiểm tra giải quyết tên dns. nslookup $ nslookup www.google.com Server: 127.0.0.53 Address: 127.0.0.53#53 Non-authoritative answer: Name: www.google.com Address: 172.217.18.4 Name: www.google.com dig $ dig www.google.com ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.11.3-1 ... ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 8738 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 65494 ;; QUESTION SECTION: ;www.google.com. IN A ;; ANSWER SECTION: www.google.com. 63 IN A 216.58.206.4 ;; Query time: 6 msec ;; SERVER: 127.0.0.53#53(127.0.0.53) "
},
{
	"uri": "/vi/3-network-policies/3.2-egress-controls/",
	"title": "Implementing Egress Controls",
	"tags": [],
	"description": "",
	"content": "Implementing Egress Controls Như đã hiển thị trong biểu đồ kiến trúc ở trên, thành phần \u0026lsquo;ui\u0026rsquo; là ứng dụng phía trước. Vì vậy, chúng ta có thể bắt đầu triển khai các điều khiển mạng cho thành phần \u0026lsquo;ui\u0026rsquo; bằng cách xác định một chính sách mạng sẽ chặn tất cả lưu lượng ra từ không gian tên \u0026lsquo;ui\u0026rsquo;.\nmanifests/modules/networking/network-policies/apply-network-policies/default-deny.yaml Lưu ý : Không có không gian tên nào được chỉ định trong chính sách mạng, vì đó là một chính sách chung có thể được áp dụng cho bất kỳ không gian tên nào trong cụm của chúng ta.\n$ kubectl apply -n ui -f ~/environment/eks-workshop/modules/networking/network-policies/apply-network-policies/default-deny.yaml Bây giờ chúng ta hãy thử truy cập thành phần \u0026lsquo;catalog\u0026rsquo; từ thành phần \u0026lsquo;ui\u0026rsquo;,\n$ kubectl exec deployment/ui -n ui -- curl -s http://catalog.catalog/health --connect-timeout 5 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0:00:03 --:--:-- 0 curl: (28) Resolving timed out after 5000 milliseconds lệnh đã kết thúc với mã thoát 28 Khi thực thi lệnh curl, đầu ra hiển thị phải có câu sau, cho thấy rằng thành phần \u0026lsquo;ui\u0026rsquo; bây giờ không thể giao tiếp trực tiếp với thành phần \u0026lsquo;catalog\u0026rsquo;.\ncurl: (28) Resolving timed out after 3000 milliseconds Việc triển khai chính sách trên cũng sẽ khiến ứng dụng mẫu không hoạt động đúng cách nữa vì thành phần \u0026lsquo;ui\u0026rsquo; cần truy cập vào dịch vụ \u0026lsquo;catalog\u0026rsquo; và các thành phần dịch vụ khác. Để xác định một chính sách egress hiệu quả cho thành phần \u0026lsquo;ui\u0026rsquo; đòi hỏi hiểu biết về các phụ thuộc mạng cho thành phần đó.\nTrong trường hợp của thành phần \u0026lsquo;ui\u0026rsquo;, nó cần giao tiếp với tất cả các thành phần dịch vụ khác, chẳng hạn như \u0026lsquo;catalog\u0026rsquo;, \u0026lsquo;orders\u0026rsquo;, v.v. Ngoài ra, \u0026lsquo;ui\u0026rsquo; cũng sẽ cần có khả năng giao tiếp với các thành phần trong các không gian tên hệ thống cụm. Ví dụ, để thành phần \u0026lsquo;ui\u0026rsquo; hoạt động, nó cần có khả năng thực hiện tìm kiếm DNS, điều này đòi hỏi nó phải giao tiếp với dịch vụ CoreDNS trong không gian tên `kube-system``.\nChính sách mạng dưới đây đã được thiết kế dựa trên các yêu cầu trên. Nó có hai phần chính:\nPhần đầu tiên tập trung vào cho phép lưu lượng egress tới tất cả các thành phần dịch vụ như \u0026lsquo;catalog\u0026rsquo;, \u0026lsquo;orders\u0026rsquo;, v.v. mà không cung cấp quyền truy cập vào các thành phần cơ sở dữ liệu thông qua một sự kết hợp của namespaceSelector, cho phép lưu lượng egress tới bất kỳ không gian tên nào miễn là nhãn pod phù hợp với \u0026ldquo;app.kubernetes.io/component: service\u0026rdquo;. Phần thứ hai tập trung vào cho phép lưu lượng egress tới tất cả các thành phần trong không gian tên kube-system, cho phép tìm kiếm DNS và các giao tiếp khác chính với các thành phần trong không gian tên hệ thống. manifests/modules/networking/network-policies/apply-network-policies/allow-ui-egress.yaml Hãy áp dụng chính sách bổ sung này:\n$ kubectl apply -f ~/environment/eks-workshop/modules/networking/network-policies/apply-network-policies/allow-ui-egress.yaml Bây giờ, chúng ta có thể thử nghiệm xem liệu chúng ta có thể kết nối được với dịch vụ \u0026lsquo;catalog\u0026rsquo; không:\n$ kubectl exec deployment/ui -n ui -- curl http://catalog.catalog/health OK Như bạn có thể thấy từ đầu ra, chúng ta hiện có thể kết nối với dịch vụ \u0026lsquo;catalog\u0026rsquo; nhưng không phải là cơ sở dữ liệu vì nó không có nhãn app.kubernetes.io/component: service:\n$ kubectl exec deployment/ui -n ui -- curl -v telnet://catalog-mysql.catalog:3306 --connect-timeout 5 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed * Không thể kết nối đến catalog-mysql.catalog cổng 3306 sau 5000 ms: Đã đạt đến thời gian chờ * Đóng kết nối 0 curl: (28) Không thể kết nối đến catalog-mysql.catalog cổng 3306 sau 5000 ms: Đã đạt đến thời gian chờ l "
},
{
	"uri": "/vi/4-security-groups-for-pods/4.2-using-amazon-rds/",
	"title": "Using Amazon RDS",
	"tags": [],
	"description": "",
	"content": "Một cơ sở dữ liệu RDS đã được tạo trong tài khoản của chúng ta, hãy lấy địa chỉ kết nối và mật khẩu để sử dụng sau này:\n$ export CATALOG_RDS_ENDPOINT_QUERY=$(aws rds describe-db-instances --db-instance-identifier $EKS_CLUSTER_NAME-catalog --query \u0026#39;DBInstances[0].Endpoint\u0026#39;) $ export CATALOG_RDS_ENDPOINT=$(echo $CATALOG_RDS_ENDPOINT_QUERY | jq -r \u0026#39;.Address+\u0026#34;:\u0026#34;+(.Port|tostring)\u0026#39;) $ echo $CATALOG_RDS_ENDPOINT eks-workshop-catalog.cluster-cjkatqd1cnrz.us-west-2.rds.amazonaws.com:3306 $ export CATALOG_RDS_PASSWORD=$(aws ssm get-parameter --name $EKS_CLUSTER_NAME-catalog-db --region $AWS_REGION --query \u0026#34;Parameter.Value\u0026#34; --output text --with-decryption) Bước đầu tiên trong quy trình này là cấu hình lại dịch vụ danh mục để sử dụng một cơ sở dữ liệu Amazon RDS đã được tạo trước đó. Ứng dụng tải hầu hết cấu hình của mình từ một ConfigMap, hãy xem nó:\n$ kubectl -n catalog get -o yaml cm catalog apiVersion: v1 data: DB_ENDPOINT: catalog-mysql:3306 DB_READ_ENDPOINT: catalog-mysql:3306 kind: ConfigMap metadata: name: catalog namespace: catalog Kustomization sau đây ghi đè ConfigMap, thay đổi điểm kết nối MySQL để ứng dụng sẽ kết nối với cơ sở dữ liệu Amazon RDS đã được tạo sẵn cho chúng ta đang được lấy từ biến môi trường CATALOG_RDS_ENDPOINT.\nmodules/networking/securitygroups-for-pods/rds/kustomization.yaml\rConfigMap/catalog Hãy áp dụng thay đổi này để sử dụng cơ sở dữ liệu RDS:\n$ kubectl kustomize ~/environment/eks-workshop/modules/networking/securitygroups-for-pods/rds \\ | envsubst | kubectl apply -f- Kiểm tra xem ConfigMap đã được cập nhật với các giá trị mới chưa:\n$ kubectl get -n catalog cm catalog -o yaml apiVersion: v1 data: DB_ENDPOINT: eks-workshop-catalog.cluster-cjkatqd1cnrz.us-west-2.rds.amazonaws.com:3306 DB_READ_ENDPOINT: eks-workshop-catalog.cluster-cjkatqd1cnrz.us-west-2.rds.amazonaws.com:3306 kind: ConfigMap metadata: labels: app: catalog name: catalog namespace: catalog Bây giờ chúng ta cần tái chế các Pods danh mục để lấy nội dung ConfigMap mới của chúng ta:\n$ kubectl delete pod -n catalog -l app.kubernetes.io/component=service pod \u0026#34;catalog-788bb5d488-9p6cj\u0026#34; deleted $ kubectl rollout status -n catalog deployment/catalog --timeout 30s Waiting for deployment \u0026#34;catalog\u0026#34; rollout to finish: 1 old replicas are pending termination... error: timed out waiting for the condition Chúng ta nhận được một lỗi, dường như các Pods danh mục của chúng ta đã không khởi động lại kịp thời. Điều gì đã sai? Hãy kiểm tra log của Pod để xem đã xảy ra gì:\n$ kubectl -n catalog logs deployment/catalog 2022/12/19 17:43:05 Error: Failed to prep migration dial tcp 10.42.11.72:3306: i/o timeout 2022/12/19 17:43:05 Error: Failed to run migration dial tcp 10.42.11.72:3306: i/o timeout 2022/12/19 17:43:05 dial tcp 10.42.11.72:3306: i/o timeout Pod của chúng ta không thể kết nối được đến cơ sở dữ liệu RDS. Chúng ta có thể kiểm tra Security group EC2 đã được áp dụng cho cơ sở dữ liệu RDS như sau:\n$ aws ec2 describe-security-groups \\ --filters Name=vpc-id,Values=$VPC_ID Name=tag:Name,Values=$EKS_CLUSTER_NAME-catalog-rds | jq \u0026#39;.\u0026#39; { \u0026#34;SecurityGroups\u0026#34;: [ { \u0026#34;Description\u0026#34;: \u0026#34;Catalog RDS security group\u0026#34;, \u0026#34;GroupName\u0026#34;: \u0026#34;eks-workshop-catalog-rds-20221220135004125100000005\u0026#34;, \u0026#34;IpPermissions\u0026#34;: [ { \u0026#34;FromPort\u0026#34;: 3306, \u0026#34;IpProtocol\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;IpRanges\u0026#34;: [], \u0026#34;Ipv6Ranges\u0026#34;: [], \u0026#34;PrefixListIds\u0026#34;: [], \u0026#34;ToPort\u0026#34;: 3306, \u0026#34;UserIdGroupPairs\u0026#34;: [ { \u0026#34;Description\u0026#34;: \u0026#34;MySQL access from within VPC\u0026#34;, \u0026#34;GroupId\u0026#34;: \u0026#34;sg-037ec36e968f1f5e7\u0026#34;, \u0026#34;UserId\u0026#34;: \u0026#34;1234567890\u0026#34; } ] } ], \u0026#34;OwnerId\u0026#34;: \u0026#34;1234567890\u0026#34;, \u0026#34;GroupId\u0026#34;: \u0026#34;sg-0b47cdc59485262ea\u0026#34;, \u0026#34;IpPermissionsEgress\u0026#34;: [], \u0026#34;Tags\u0026#34;: [ { \u0026#34;Key\u0026#34;: \u0026#34;Name\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;eks-workshop-catalog-rds\u0026#34; } ], \u0026#34;VpcId\u0026#34;: \u0026#34;vpc-077ca8c89d111b3c1\u0026#34; } ] } Bạn cũng có thể xem Security group của RDS instance thông qua bảng điều khiển AWS:\nhttps://console.aws.amazon.com/rds/home#database:id=eks-workshop-catalog;is-cluster=false\nSecurity group này chỉ cho phép lưu lượng truy cập vào cơ sở dữ liệu RDS trên cổng 3306 nếu nó đến từ một nguồn có một Security group cụ thể, trong ví dụ trên là sg-037ec36e968f1f5e7.\n"
},
{
	"uri": "/vi/4-security-groups-for-pods/4.3-applying-a-security-group/",
	"title": "Applying a Security Group",
	"tags": [],
	"description": "",
	"content": "Applying a Security Group Kết nối Pod Catalog với RDS Instance trên AWS Để Pod catalog của chúng ta kết nối thành công với RDS instance, chúng ta cần sử dụng security group đúng. Tuy nhiên, việc áp dụng security group này trực tiếp vào các node worker của EKS sẽ dẫn đến tất cả các công việc trong cluster của chúng ta đều có quyền truy cập mạng vào RDS instance. Thay vào đó, chúng ta sẽ áp dụng Security Groups cho Pods để cho phép các Pod catalog của chúng ta truy cập vào RDS instance.\nMột security group cho phép truy cập vào cơ sở dữ liệu RDS đã được thiết lập sẵn cho bạn và bạn có thể xem như sau:\n$ export CATALOG_SG_ID=$(aws ec2 describe-security-groups \\ --filters Name=vpc-id,Values=$VPC_ID Name=group-name,Values=$EKS_CLUSTER_NAME-catalog \\ --query \u0026#34;SecurityGroups[0].GroupId\u0026#34; --output text) $ aws ec2 describe-security-groups \\ --group-ids $CATALOG_SG_ID | jq \u0026#39;.\u0026#39; { \u0026#34;SecurityGroups\u0026#34;: [ { \u0026#34;Description\u0026#34;: \u0026#34;Applied to catalog application pods\u0026#34;, \u0026#34;GroupName\u0026#34;: \u0026#34;eks-workshop-catalog\u0026#34;, \u0026#34;IpPermissions\u0026#34;: [ { \u0026#34;FromPort\u0026#34;: 8080, \u0026#34;IpProtocol\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;IpRanges\u0026#34;: [ { \u0026#34;CidrIp\u0026#34;: \u0026#34;10.42.0.0/16\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Allow inbound HTTP API traffic\u0026#34; } ], \u0026#34;Ipv6Ranges\u0026#34;: [], \u0026#34;PrefixListIds\u0026#34;: [], \u0026#34;ToPort\u0026#34;: 8080, \u0026#34;UserIdGroupPairs\u0026#34;: [] } ], \u0026#34;OwnerId\u0026#34;: \u0026#34;1234567890\u0026#34;, \u0026#34;GroupId\u0026#34;: \u0026#34;sg-037ec36e968f1f5e7\u0026#34;, \u0026#34;IpPermissionsEgress\u0026#34;: [ { \u0026#34;IpProtocol\u0026#34;: \u0026#34;-1\u0026#34;, \u0026#34;IpRanges\u0026#34;: [ { \u0026#34;CidrIp\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Allow all egress\u0026#34; } ], \u0026#34;Ipv6Ranges\u0026#34;: [], \u0026#34;PrefixListIds\u0026#34;: [], \u0026#34;UserIdGroupPairs\u0026#34;: [] } ], \u0026#34;VpcId\u0026#34;: \u0026#34;vpc-077ca8c89d111b3c1\u0026#34; } ] } Security group này:\nCho phép lưu lượng vào cho HTTP API được phục vụ bởi Pod trên cổng 8080 Cho phép tất cả lưu lượng ra Sẽ được phép truy cập vào cơ sở dữ liệu RDS như chúng ta đã thấy trước đó Để Pod của chúng ta sử dụng security group này, chúng ta cần sử dụng CRD SecurityGroupPolicy để thông báo cho EKS security group nào sẽ được ánh xạ vào một tập hợp cụ thể của Pods. Đây là những gì chúng ta sẽ cấu hình:\nmanifests/modules/networking/securitygroups-for-pods/sg/policy.yaml Áp dụng điều này vào cluster sau đó tái khởi động các Pod catalog một lần nữa:\n$ kubectl kustomize ~/environment/eks-workshop/modules/networking/securitygroups-for-pods/sg \\ | envsubst | kubectl apply -f- namespace/catalog unchanged serviceaccount/catalog unchanged configmap/catalog unchanged configmap/catalog-env-97g7bft95f unchanged configmap/catalog-sg-env-54k244c6t7 created secret/catalog-db unchanged service/catalog unchanged service/catalog-mysql unchanged service/ui-nlb unchanged deployment.apps/catalog unchanged statefulset.apps/catalog-mysql unchanged securitygrouppolicy.vpcresources.k8s.aws/catalog-rds-access created $ kubectl delete pod -n catalog -l app.kubernetes.io/component=service pod \u0026#34;catalog-6ccc6b5575-glfxc\u0026#34; deleted $ kubectl rollout status -n catalog deployment/catalog --timeout 30s deployment \u0026#34;catalog\u0026#34; successfully rolled out Lần này, Pod catalog sẽ được khởi động và quá trình triển khai sẽ thành công. Bạn có thể kiểm tra log để xác nhận rằng nó đang kết nối với cơ sở dữ liệu RDS:\n$ kubectl -n catalog logs deployment/catalog | grep Connect 2022/12/20 20:52:10 Connecting to catalog_user:xxxxxxxxxx@tcp(eks-workshop-catalog.cjkatqd1cnrz.us-west-2.rds.amazonaws.com:3306)/catalog?timeout=5s 2022/12/20 20:52:10 Connected 2022/12/20 20:52:10 Connecting to catalog_user:xxxxxxxxxx@tcp(eks-workshop-catalog.cjkatqd1cnrz.us-west-2.rds.amazonaws.com:3306)/catalog?timeout=5s 2022/12/20 20:52:10 Connected "
},
{
	"uri": "/vi/1-introduce/1.3-coredns/",
	"title": "CoreDNS",
	"tags": [],
	"description": "",
	"content": "CoreDNS Tiền điều kiện CoreDNS Trong phần này, chúng ta sẽ xem xét CoreDNS\nCài đặt CoreDNS $ wget https://github.com/coredns/coredns/releases/download/v1.7.0/coredns_1.7.0_linux_amd64.tgz\rcoredns_1.7.0_linux_amd64.tgz Giải nén tệp tar $ tar -xzvf coredns_1.7.0_linux_amd64.tgz\rcoredns Chạy tệp thực thi Chạy tệp thực thi để bắt đầu một máy chủ DNS. Theo mặc định, nó lắng nghe trên cổng 53, đó là cổng mặc định cho một máy chủ DNS. $ ./coredns Cấu hình tệp hosts Thêm các mục vào tệp /etc/hosts. CoreDNS sẽ chọn các IP và tên từ tệp /etc/hosts trên máy chủ. $ cat \u0026gt; /etc/hosts\r192.168.1.10 web\r192.168.1.11 db\r192.168.1.15 web-1\r192.168.1.16 db-1\r192.168.1.21 web-2\r192.168.1.22 db-2 Thêm vào tệp Corefile $ cat \u0026gt; Corefile\r. {\rhosts /etc/hosts\r} Chạy tệp thực thi $ ./coredns Tài liệu tham khảo https://github.com/kubernetes/dns/blob/master/docs/specification.md https://coredns.io/plugins/kubernetes/ https://github.com/coredns/coredns/releases "
},
{
	"uri": "/vi/3-network-policies/3.3-ingress-controls/",
	"title": "Implementing Ingress Controls",
	"tags": [],
	"description": "",
	"content": "Implementing Ingress Controls Như đã thể hiện trong sơ đồ kiến trúc, namespace \u0026lsquo;catalog\u0026rsquo; chỉ nhận lưu lượng từ namespace \u0026lsquo;ui\u0026rsquo; và không nhận lưu lượng từ bất kỳ namespace nào khác. Ngoài ra, thành phần cơ sở dữ liệu \u0026lsquo;catalog\u0026rsquo; chỉ có thể nhận lưu lượng từ thành phần dịch vụ \u0026lsquo;catalog\u0026rsquo;.\nChúng ta có thể bắt đầu triển khai các quy tắc mạng trên bằng cách sử dụng một chính sách mạng ingress sẽ kiểm soát lưu lượng đến namespace \u0026lsquo;catalog\u0026rsquo;.\nTrước khi áp dụng chính sách, dịch vụ \u0026lsquo;catalog\u0026rsquo; có thể truy cập được từ cả thành phần \u0026lsquo;ui\u0026rsquo;:\n$ kubectl exec deployment/ui -n ui -- curl -v catalog.catalog/health --connect-timeout 5 Trying XXX.XXX.XXX.XXX:80... * Connected to catalog.catalog (XXX.XXX.XXX.XXX) port 80 (#0) \u0026gt; GET /catalogue HTTP/1.1 \u0026gt; Host: catalog.catalog \u0026gt; User-Agent: curl/7.88.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK ... Cũng như từ thành phần \u0026lsquo;orders\u0026rsquo;:\n$ kubectl exec deployment/orders -n orders -- curl -v catalog.catalog/health --connect-timeout 5 Trying XXX.XXX.XXX.XXX:80... * Connected to catalog.catalog (XXX.XXX.XXX.XXX) port 80 (#0) \u0026gt; GET /catalogue HTTP/1.1 \u0026gt; Host: catalog.catalog \u0026gt; User-Agent: curl/7.88.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK ... Bây giờ, chúng ta sẽ định nghĩa một chính sách mạng sẽ cho phép lưu lượng đến thành phần dịch vụ \u0026lsquo;catalog\u0026rsquo; chỉ từ thành phần \u0026lsquo;ui\u0026rsquo;:\nmanifests/modules/networking/network-policies/apply-network-policies/allow-catalog-ingress-webservice.yaml Áp dụng chính sách:\n$ kubectl apply -f ~/environment/eks-workshop/modules/networking/network-policies/apply-network-policies/allow-catalog-ingress-webservice.yaml Bây giờ, chúng ta có thể xác minh chính sách bằng cách xác nhận rằng chúng ta vẫn có thể truy cập vào thành phần \u0026lsquo;catalog\u0026rsquo; từ \u0026lsquo;ui\u0026rsquo;:\n$ kubectl exec deployment/ui -n ui -- curl -v catalog.catalog/health --connect-timeout 5 Trying XXX.XXX.XXX.XXX:80... * Connected to catalog.catalog (XXX.XXX.XXX.XXX) port 80 (#0) \u0026gt; GET /catalogue HTTP/1.1 \u0026gt; Host: catalog.catalog \u0026gt; User-Agent: curl/7.88.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK ... Nhưng không thể từ thành phần \u0026lsquo;orders\u0026rsquo;:\n$ kubectl exec deployment/orders -n orders -- curl -v catalog.catalog/health --connect-timeout 5 * Trying XXX.XXX.XXX.XXX:80... * ipv4 connect timeout after 4999ms, move on! * Failed to connect to catalog.catalog port 80 after 5001 ms: Timeout was reached * Closing connection 0 curl: (28) Failed to connect to catalog.catalog port 80 after 5001 ms: Timeout was reached ... Như bạn đã thấy từ các đầu ra trên, chỉ có thành phần \u0026lsquo;ui\u0026rsquo; có thể giao tiếp với thành phần dịch vụ \u0026lsquo;catalog\u0026rsquo;, và thành phần dịch vụ \u0026lsquo;orders\u0026rsquo; không thể.\nNhưng điều này vẫn để mở cửa cho thành phần cơ sở dữ liệu \u0026lsquo;catalog\u0026rsquo;, vì vậy hãy triển khai một chính sách mạng để đảm bảo chỉ có thành phần dịch vụ \u0026lsquo;catalog\u0026rsquo; mới có thể giao tiếp với thành phần cơ sở dữ liệu \u0026lsquo;catalog\u0026rsquo;.\nmanifests/modules/networking/network-policies/apply-network-policies/allow-catalog-ingress-db.yaml Áp dụng chính sách:\n$ kubectl apply -f ~/environment/eks-workshop/modules/networking/network-policies/apply-network-policies/allow-catalog-ingress-db.yaml Hãy xác minh chính sách mạng bằng cách xác nhận rằng chúng ta không thể kết nối với cơ sở dữ liệu \u0026lsquo;catalog\u0026rsquo; từ thành phần \u0026lsquo;orders\u0026rsquo;:\n$ kubectl exec deployment/orders -n orders -- curl -v telnet://catalog-mysql.catalog:3306 --connect-timeout 5 * Trying XXX.XXX.XXX.XXX:3306... * ipv4 connect timeout after 4999ms, move on! * Failed to connect to catalog-mysql.catalog port 3306 after 5001 ms: Timeout was reached * Closing connection 0 curl: (28) Failed to connect to catalog-mysql.catalog port 3306 after 5001 ms: Timeout was reached command terminated with exit code 28 ... Nhưng nếu chúng ta khởi động lại pod \u0026lsquo;catalog\u0026rsquo; thì vẫn có thể kết nối:\n$ kubectl rollout restart deployment/catalog -n catalog $ kubectl rollout status deployment/catalog -n catalog --timeout=2m Như bạn đã thấy từ các đầu ra trên, chỉ có thành phần dịch vụ \u0026lsquo;catalog\u0026rsquo; mới có thể giao tiếp với thành phần cơ sở dữ liệu \u0026lsquo;catalog\u0026rsquo;.\nBây giờ chúng ta đã triển khai một chính sách ingress hiệu quả cho namespace \u0026lsquo;catalog\u0026rsquo;, chúng ta mở rộng cùng một logic cho các namespace và thành phần khác trong ứng dụng mẫu, từ đó giảm thiểu đáng kể bề mặt tấn công cho ứng dụng mẫu và tăng cường bảo mật mạng.\n"
},
{
	"uri": "/vi/3-network-policies/",
	"title": "Network Policies",
	"tags": [],
	"description": "",
	"content": "Network Policies Chuẩn bị môi trường của bạn cho phần này:\n$ prepare-environment networking/network-policies :::\nMặc định, Kubernetes cho phép tất cả các pod giao tiếp tự do với nhau mà không có hạn chế nào. Network Policies của Kubernetes cho phép bạn xác định và áp dụng các quy tắc về luồng dữ liệu giữa các pod, namespace và các block IP (phạm vi CIDR). Chúng hoạt động như một tường lửa ảo, cho phép bạn phân đoạn và bảo mật cụm của mình bằng cách chỉ định các quy tắc lưu lượng mạng ingress (đầu vào) và egress (đầu ra) dựa trên các tiêu chí khác nhau như nhãn pod, namespace, địa chỉ IP và cổng.\nDưới đây là một ví dụ về một network policy,\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: test-network-policy namespace: default spec: podSelector: matchLabels: role: db policyTypes: - Ingress - Egress ingress: - from: - ipBlock: cidr: 172.17.0.0/16 except: - 172.17.1.0/24 - namespaceSelector: matchLabels: project: myproject - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379 egress: - to: - ipBlock: cidr: 10.0.0.0/24 ports: - protocol: TCP port: 5978 Phần mô tả network policy chứa các phần quan trọng sau:\nmetadata: tương tự như các đối tượng Kubernetes khác, cho phép bạn chỉ định tên và namespace cho network policy cụ thể. spec.podSelector: cho phép lựa chọn các pod cụ thể dựa trên các nhãn của chúng trong namespace mà network policy cụ thể sẽ được áp dụng. Nếu một pod selector trống hoặc matchLabels được chỉ định trong phần mô tả, thì policy sẽ được áp dụng cho tất cả các pod trong namespace. spec.policyTypes: chỉ định xem policy sẽ được áp dụng cho lưu lượng ingress, lưu lượng egress hoặc cả hai cho các pod được chọn. Nếu bạn không chỉ định trường này, thì hành vi mặc định là áp dụng network policy cho lưu lượng ingress mà không chỉ định phần egress, trừ khi network policy có một phần egress, trong trường hợp đó network policy sẽ được áp dụng cho cả lưu lượng ingress và egress. ingress: cho phép cấu hình các quy tắc ingress mà chỉ định từ các pod (podSelector), namespace (namespaceSelector) hoặc phạm vi CIDR (ipBlock) mà lưu lượng được phép đến các pod được chọn và cổng hoặc phạm vi cổng nào có thể được sử dụng. Nếu không chỉ định cổng hoặc phạm vi cổng, bất kỳ cổng nào cũng có thể được sử dụng cho việc giao tiếp. Để biết thêm thông tin về những khả năng được phép hoặc bị hạn chế của network policies trong Kubernetes, xem tài liệu Kubernetes.\nNgoài network policies, Amazon VPC CNI ở chế độ IPv4 cung cấp một tính năng mạnh mẽ được gọi là \u0026ldquo;security group s for Pods\u0026rdquo;. Tính năng này cho phép bạn sử dụng các security group Amazon EC2 để xác định các quy tắc toàn diện điều chỉnh lưu lượng mạng đến và đi từ các pod triển khai trên các nút của bạn. Trong khi có sự trùng lặp về khả năng giữa các security group cho các pod và các network policy, nhưng có một số khác biệt quan trọng.\nCác security group cho phép điều khiển lưu lượng ingress và egress đến các phạm vi CIDR, trong khi network policies cho phép điều khiển lưu lượng ingress và egress đến các pod, namespace cũng như các phạm vi CIDR. Các security group cho phép điều khiển lưu lượng ingress và egress từ các security group khác, điều này không có sẵn cho các network policies. Amazon EKS mạnh mẽ khuyến khích sử dụng network policies kết hợp với các security group để hạn chế giao tiếp mạng giữa các pod, từ đó giảm bề mặt tấn công và giảm thiểu các lỗ hổng tiềm ẩn.\n"
},
{
	"uri": "/vi/5-custom-networking/5.3-node-group/",
	"title": "Provision a new Node Group",
	"tags": [],
	"description": "",
	"content": "Provision a new Node Group Tạo một nhóm node được quản lý EKS:\n$ aws eks create-nodegroup --region $AWS_REGION \\ --cluster-name $EKS_CLUSTER_NAME \\ --nodegroup-name custom-networking \\ --instance-types t3.medium --node-role $CUSTOM_NETWORKING_NODE_ROLE \\ --subnets $PRIMARY_SUBNET_1 $PRIMARY_SUBNET_2 $PRIMARY_SUBNET_3 \\ --labels type=customnetworking \\ --scaling-config minSize=1,maxSize=1,desiredSize=1 Quá trình tạo nhóm node mất vài phút. Bạn có thể chờ quá trình tạo nhóm node hoàn thành bằng cách sử dụng lệnh này:\n$ aws eks wait nodegroup-active --cluster-name $EKS_CLUSTER_NAME --nodegroup-name custom-networking Khi quá trình này hoàn tất, chúng ta có thể thấy các node mới đã được đăng ký trong cụm EKS:\n$ kubectl get nodes -L eks.amazonaws.com/nodegroup NAME STATUS ROLES AGE VERSION NODEGROUP ip-10-42-104-242.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 84m vVAR::KUBERNETES_NODE_VERSION default ip-10-42-110-28.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 61s vVAR::KUBERNETES_NODE_VERSION custom-networking ip-10-42-139-60.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 65m vVAR::KUBERNETES_NODE_VERSION default ip-10-42-180-105.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 65m vVAR::KUBERNETES_NODE_VERSION default Bạn có thể thấy 1 node mới được cung cấp được gắn nhãn với tên của nhóm node mới.\n"
},
{
	"uri": "/vi/3-network-policies/3.4-debugging/",
	"title": "Debugging",
	"tags": [],
	"description": "",
	"content": "Debugging Debugging Network Policies with Amazon VPC CNI Logs Đến nay, chúng ta đã có thể áp dụng các chính sách mạng mà không gặp vấn đề hoặc lỗi. Nhưng điều gì sẽ xảy ra nếu có lỗi hoặc vấn đề? Làm thế nào để chúng ta có thể gỡ lỗi những vấn đề này?\nAmazon VPC CNI cung cấp các bản ghi nhật ký có thể được sử dụng để gỡ lỗi vấn đề trong quá trình triển khai các chính sách mạng. Ngoài ra, bạn có thể giám sát các bản ghi này thông qua các dịch vụ như Amazon CloudWatch, nơi bạn có thể tận dụng CloudWatch Container Insights để cung cấp thông tin về việc sử dụng của bạn liên quan đến NetworkPolicy.\nBây giờ, hãy thử triển khai một chính sách mạng ingress sẽ hạn chế quyền truy cập vào thành phần dịch vụ \u0026lsquo;orders\u0026rsquo; từ thành phần \u0026lsquo;ui\u0026rsquo; chỉ, tương tự như chúng ta đã làm trước đó với thành phần dịch vụ \u0026lsquo;catalog\u0026rsquo;.\nmanifests/modules/networking/network-policies/apply-network-policies/allow-order-ingress-fail-debug.yaml Hãy áp dụng chính sách này:\n$ kubectl apply -f ~/environment/eks-workshop/modules/networking/network-policies/apply-network-policies/allow-order-ingress-fail-debug.yaml Và kiểm tra:\n$ kubectl exec deployment/ui -n ui -- curl -v orders.orders/orders --connect-timeout 5 * Trying XXX.XXX.XXX.XXX:80... * ipv4 connect timeout after 4999ms, move on! * Failed to connect to orders.orders port 80 after 5000 ms: Timeout was reached * Closing connection 0 curl: (28) Failed to connect to orders.orders port 80 after 5000 ms: Timeout was reached ... Như bạn có thể thấy từ các đầu ra, có điều gì đó đã đi sai ở đây. Cuộc gọi từ thành phần \u0026lsquo;ui\u0026rsquo; nên đã thành công, nhưng thay vào đó, nó đã thất bại. Để gỡ lỗi điều này, chúng ta có thể tận dụng các nhật ký của agent chính sách mạng để xem vấn đề ở đâu.\nNhật ký của agent chính sách mạng có sẵn trong tập tin /var/log/aws-routed-eni/network-policy-agent.log trên mỗi nút worker. Hãy xem liệu có bất kỳ câu lệnh DENY nào được đăng nhập trong tập tin đó không:\n$ POD_HOSTIP_1=$(kubectl get po --selector app.kubernetes.io/component=service -n orders -o json | jq -r \u0026#39;.items[0].spec.nodeName\u0026#39;) $ kubectl debug node/$POD_HOSTIP_1 -it --image=ubuntu # Chạy các lệnh này bên trong pod $ grep DENY /host/var/log/aws-routed-eni/network-policy-agent.log | tail -5 {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2023-11-03T23:02:17.916Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;ebpf-client\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Flow Info: \u0026#34;,\u0026#34;Src IP\u0026#34;:\u0026#34;10.42.190.65\u0026#34;,\u0026#34;Src Port\u0026#34;:55986,\u0026#34;Dest IP\u0026#34;:\u0026#34;10.42.117.209\u0026#34;,\u0026#34;Dest Port\u0026#34;:8080,\u0026#34;Proto\u0026#34;:\u0026#34;TCP\u0026#34;,\u0026#34;Verdict\u0026#34;:\u0026#34;DENY\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2023-11-03T23:02:18.920Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;ebpf-client\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Flow Info: \u0026#34;,\u0026#34;Src IP\u0026#34;:\u0026#34;10.42.190.65\u0026#34;,\u0026#34;Src Port\u0026#34;:55986,\u0026#34;Dest IP\u0026#34;:\u0026#34;10.42.117.209\u0026#34;,\u0026#34;Dest Port\u0026#34;:8080,\u0026#34;Proto\u0026#34;:\u0026#34;TCP\u0026#34;,\u0026#34;Verdict\u0026#34;:\u0026#34;DENY\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2023-11-03T23:02:20.936Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;ebpf-client\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Flow Info: \u0026#34;,\u0026#34;Src IP\u0026#34;:\u0026#34;10.42.190.65\u0026#34;,\u0026#34;Src Port\u0026#34;:55986,\u0026#34;Dest IP\u0026#34;:\u0026#34;10.42.117.209\u0026#34;,\u0026#34;Dest Port\u0026#34;:8080,\u0026#34;Proto\u0026#34;:\u0026#34;TCP\u0026#34;,\u0026#34;Verdict\u0026#34;:\u0026#34;DENY\u0026#34;} $ exit Như bạn có thể thấy từ các đầu ra, các cuộc gọi từ thành phần \u0026lsquo;ui\u0026rsquo; đã bị từ chối. Trên phân tích tiếp theo, chúng ta có thể thấy rằng trong chính sách mạng của chúng ta, trong phần ingress, chúng ta chỉ có podSelector và không có namespaceSelector. Khi namespaceSelector rỗng, nó sẽ mặc định là không gian tên của chính sách mạng, đó là \u0026lsquo;orders\u0026rsquo;. Do đó, chính sách sẽ được hiểu là cho phép các pod khớp với nhãn \u0026lsquo;app.kubernetes.io/name: ui\u0026rsquo; từ không gian tên \u0026lsquo;orders\u0026rsquo;, dẫn đến việc từ chối lưu lượng từ thành phần \u0026lsquo;ui\u0026rsquo;.\nHãy sửa chính sách mạng và thử lại.\n$ kubectl apply -f ~/environment/eks-workshop/modules/networking/network-policies/apply-network-policies/allow-order-ingress-success-debug.yaml Bây giờ kiểm tra xem \u0026lsquo;ui\u0026rsquo; có thể kết nối không:\n$ kubectl exec deployment/ui -n ui -- curl -v orders.orders/orders --connect-timeout 5 * Trying XXX.XXX.XXX.XXX:80... * Connected to orders.orders (172.20.248.36) port 80 (#0) \u0026gt; GET /orders HTTP/1.1 \u0026gt; Host: orders.orders \u0026gt; User-Agent: curl/7.88.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 ... Như bạn có thể thấy từ các đầu ra, bây giờ thành phần \u0026lsquo;ui\u0026rsquo; có thể gọi thành phần dịch vụ \u0026lsquo;orders\u0026rsquo;, và vấn đề đã được giải quyết.\n"
},
{
	"uri": "/vi/4-security-groups-for-pods/4.4-inspecting-the-pod/",
	"title": "Inspecting the Pod",
	"tags": [],
	"description": "",
	"content": "Inspecting the Pod Bây giờ khi Catalog Pod đang chạy và sử dụng cơ sở dữ liệu Amazon RDS của chúng ta một cách thành công, hãy xem xét kỹ hơn để xem các tín hiệu liên quan đến SG cho các Pods.\nĐiều đầu tiên chúng ta có thể làm là kiểm tra các chú thích của Pod:\n$ kubectl get pod -n catalog -l app.kubernetes.io/component=service -o yaml \\ | yq \u0026#39;.items[0].metadata.annotations\u0026#39; kubernetes.io/psp: eks.privileged prometheus.io/path: /metrics prometheus.io/port: \u0026#34;8080\u0026#34; prometheus.io/scrape: \u0026#34;true\u0026#34; vpc.amazonaws.com/pod-eni: \u0026#39;[{\u0026#34;eniId\u0026#34;:\u0026#34;eni-0eb4769ea066fa90c\u0026#34;,\u0026#34;ifAddress\u0026#34;:\u0026#34;02:23:a2:af:a2:1f\u0026#34;,\u0026#34;privateIp\u0026#34;:\u0026#34;10.42.10.154\u0026#34;,\u0026#34;vlanId\u0026#34;:2,\u0026#34;subnetCidr\u0026#34;:\u0026#34;10.42.10.0/24\u0026#34;}]\u0026#39; Chú thích vpc.amazonaws.com/pod-eni hiển thị dữ liệu về các thứ như ENI nhánh đã được sử dụng cho Pod này, địa chỉ IP riêng của nó và như vậy.\nCác sự kiện Kubernetes cũng sẽ hiển thị bộ điều khiển tài nguyên VPC thực hiện hành động phản hồi các cấu hình mà chúng ta đã thêm:\n$ kubectl get events -n catalog | grep SecurityGroupRequested 5m Normal SecurityGroupRequested pod/catalog-6ccc6b5575-w2fvm Pod sẽ nhận các Nhóm Bảo mật sau [sg-037ec36e968f1f5e7] Cuối cùng, bạn có thể xem các ENI được quản lý bởi bộ điều khiển tài nguyên VPC trong bảng điều khiển:\nConsole AWS EC2\nĐiều này sẽ cho phép bạn xem thông tin về ENI nhánh như nhóm bảo mật được chỉ định.\n"
},
{
	"uri": "/vi/1-introduce/1.4-network-namespaces/",
	"title": "Network Namespaces",
	"tags": [],
	"description": "",
	"content": "Network Namespaces Trong phần này, chúng ta sẽ xem xét Network Namespaces\nProcess Namespace Trên container\n$ ps aux Trên máy chủ\n$ ps aux Network Namespace $ route $ arp Tạo Network Namespace $ ip netns add red\r$ ip netns add blue Liệt kê các network namespace $ ip netns Thực thi trong Network Namespace Liệt kê các giao diện trên máy chủ $ ip link Thực thi bên trong network namespace $ ip netns exec red ip link\r1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000\rlink/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\r$ ip netns exec blue ip link\r1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000\rlink/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 Bạn cũng có thể thử với các tùy chọn khác. Cả hai đều hoạt động như nhau. $ ip -n red link\r1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000\rlink/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 ARP và Bảng Định Tuyến Trên máy chủ\n$ arp\rAddress HWtype HWaddress Flags Mask Iface\r172.17.0.21 ether 02:42:ac:11:00:15 C ens3\r172.17.0.55 ether 02:42:ac:11:00:37 C ens3 Trên Network Namespace\n$ ip netns exec red arp\rAddress HWtype HWaddress Flags Mask Iface\r$ ip netns exec blue arp\rAddress HWtype HWaddress Flags Mask Iface Trên máy chủ\n$ route Trên Network Namespace\n$ ip netns exec red route\rKernel IP routing table\rDestination Gateway Genmask Flags Metric Ref Use Iface\r$ ip netns exec blue route\rKernel IP routing table\rDestination Gateway Genmask Flags Metric Ref Use Iface Cáp ảo Để tạo một cáp ảo $ ip link add veth-red type veth peer name veth-blue Để kết nối với các network namespace $ ip link set veth-red netns red\r$ ip link set veth-blue netns blue Để thêm một địa chỉ IP $ ip -n red addr add 192.168.15.1/24 dev veth-red\r$ ip -n blue addr add 192.168.15.2/24 dev veth-blue Để bật lên các giao diện ns $ ip -n red link set veth-red up\r$ ip -n blue link set veth-blue up Kiểm tra tính khả thi $ ip netns exec red ping 192.168.15.2\rPING 192.168.15.2 (192.168.15.2) 56(84) bytes of data.\r64 bytes from 192.168.15.2: icmp_seq=1 ttl=64 time=0.035 ms\r64 bytes from 192.168.15.2: icmp_seq=2 ttl=64 time=0.046 ms\r$ ip netns exec red arp\rAddress HWtype HWaddress Flags Mask Iface\r192.168.15.2 ether da:a7:29:c4:5a:45 C veth-red\r$ ip netns exec blue arp\rAddress HWtype HWaddress Flags Mask Iface\r192.168.15.1 ether 92:d1:52:38:c8:bc C veth-blue Xóa liên kết. $ ip -n red link del veth-red Trên máy chủ\n# Không có sẵn\r$ arp\rAddress HWtype HWaddress Flags Mask Iface\r172.16.0.72 ether 06:fe:61:1a:75:47 C ens3\r172.17.0.68 ether 02:42:ac:11:00:44 C ens3\r172.17.0.74 ether 02:42:ac:11:00:4a C ens3\r172.17.0.75 ether 02:42:ac:11:00:4b C ens3 Linux Bridge Tạo một không gian mạng $ ip netns add red $ ip netns add blue Để tạo một mạng cầu ảo nội bộ, chúng ta thêm một giao diện mới vào máy chủ $ ip link add v-net-0 type bridge Hiển thị trên máy chủ $ ip link 8: v-net-0: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether fa:fd:d4:9b:33:66 brd ff:ff:ff:ff:ff:ff Hiện tại nó đã tắt, vì vậy bật nó lên $ ip link set dev v-net-0 up Để kết nối không gian mạng vào cầu. Tạo một cáp ảo $ ip link add veth-red type veth peer name veth-red-br $ ip link add veth-blue type veth peer name veth-blue-br Thiết lập với các không gian mạng $ ip link set veth-red netns red $ ip link set veth-blue netns blue $ ip link set veth-red-br master v-net-0 $ ip link set veth-blue-br master v-net-0 Để thêm một địa chỉ IP $ ip -n red addr add 192.168.15.1/24 dev veth-red $ ip -n blue addr add 192.168.15.2/24 dev veth-blue Để bật lên các giao diện ns $ ip -n red link set veth-red up $ ip -n blue link set veth-blue up Để thêm một địa chỉ IP $ ip addr add 192.168.15.5/24 dev v-net-0 Bật lên các giao diện được thêm vào trên máy chủ $ ip link set dev veth-red-br up $ ip link set dev veth-blue-br up Trên máy chủ\n$ ping 192.168.15.1 Trong không gian mạng\n$ ip netns exec blue ping 192.168.1.1 Connect: Network is unreachable $ ip netns exec blue route $ ip netns exec blue ip route add 192.168.1.0/24 via 192.168.15.5 # Kiểm tra địa chỉ IP của máy chủ $ ip a $ ip netns exec blue ping 192.168.1.1 PING 192.168.1.1 (192.168.1.1) 56(84) bytes of data. $ iptables -t nat -A POSTROUTING -s 192.168.15.0/24 -j MASQUERADE $ ip netns exec blue ping 192.168.1.1 $ ip netns exec blue ping 8.8.8.8 $ ip netns exec blue route $ ip netns exec blue ip route add default via 192.168.15.5 $ ip netns exec blue ping 8.8.8.8 Thêm luật chuyển tiếp cổng vào iptables $ iptables -t nat -A PREROUTING --dport 80 --to-destination 192.168.15.2:80 -j DNAT $ iptables -nvL -t nat "
},
{
	"uri": "/vi/5-custom-networking/5.4-re-deploy-workload/",
	"title": "Re-deploy workload",
	"tags": [],
	"description": "",
	"content": "Để kiểm tra các cập nhật mạng tùy chỉnh mà chúng ta đã thực hiện cho đến nay, hãy cập nhật deployment checkout để chạy các pods trên node mới mà chúng ta đã cung cấp ở bước trước đó.\nĐể thực hiện thay đổi, hãy chạy lệnh sau để sửa đổi deployment checkout trong cụm của bạn:\n$ kubectl apply -k ~/environment/eks-workshop/modules/networking/custom-networking/sampleapp $ kubectl rollout status deployment/checkout -n checkout --timeout 180s Lệnh này thêm một nodeSelector vào deployment checkout.\nmodules/networking/custom-networking/sampleapp/checkout.yaml\rDeployment/checkout Hãy xem lại các dịch vụ microservices được triển khai trong không gian tên \u0026ldquo;checkout\u0026rdquo;.\n$ kubectl get pods -n checkout -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES checkout-5fbbc99bb7-brn2m 1/1 Running 0 98s 100.64.10.16 ip-10-42-10-14.us-west-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; checkout-redis-6cfd7d8787-8n99n 1/1 Running 0 49m 10.42.12.33 ip-10-42-12-155.us-west-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Bạn có thể thấy rằng pod checkout được gán một địa chỉ IP từ khối CIDR 100.64.0.0 đã được thêm vào VPC. Các pods chưa được triển khai lại vẫn được gán địa chỉ từ khối CIDR 10.42.0.0, vì đó là duy nhất CIDR block ban đầu được liên kết với VPC. Trong ví dụ này, pod checkout-redis vẫn có một địa chỉ từ dải này.\n"
},
{
	"uri": "/vi/4-security-groups-for-pods/",
	"title": "Security Groups for Pods",
	"tags": [],
	"description": "",
	"content": "Chuẩn bị môi trường cho phần này: $ prepare-environment networking/securitygroups-for-pods Thao tác này sẽ thực hiện các thay đổi sau vào môi trường lab của bạn:\nTạo một trường hợp Amazon Relational Database Service Tạo một nhóm bảo mật Amazon EC2 để cho phép truy cập vào trường hợp RDS Bạn có thể xem Terraform áp dụng các thay đổi này tại đây.\nNhóm bảo mật, hoạt động như các tường lửa mạng cấp độ trường hợp, là một trong những khối xây dựng quan trọng và thường được sử dụng trong bất kỳ triển khai đám mây AWS nào. Các ứng dụng container thường cần truy cập vào các dịch vụ khác đang chạy trong cụm cũng như các dịch vụ AWS bên ngoài, chẳng hạn như Amazon Relational Database Service (Amazon RDS) hoặc Amazon ElastiCache. Trên AWS, việc kiểm soát truy cập mạng cấp độ giữa các dịch vụ thường được thực hiện thông qua các nhóm bảo mật EC2.\nMặc định, Amazon VPC CNI sẽ sử dụng các nhóm bảo mật được liên kết với ENI chính trên nút. Cụ thể hơn, mỗi ENI được liên kết với trường hợp sẽ có các Nhóm Bảo Mật EC2 giống nhau. Do đó, mỗi Pod trên một nút sẽ chia sẻ các nhóm bảo mật giống như nút mà nó chạy trên. Nhóm bảo mật cho các pod giúp dễ dàng đạt được tuân thủ bảo mật mạng bằng cách chạy các ứng dụng có yêu cầu bảo mật mạng khác nhau trên tài nguyên tính toán chia sẻ. Các quy tắc bảo mật mạng mà liên kết từ pod đến pod và từ pod đến lưu lượng dịch vụ AWS bên ngoài có thể được xác định ở một nơi duy nhất với các nhóm bảo mật EC2, và áp dụng cho các ứng dụng với các API native của Kubernetes. Sau khi áp dụng các nhóm bảo mật ở mức pod, kiến ​​trúc ứng dụng và nhóm nút của bạn có thể được đơn giản hóa như được hiển thị (dưới đây).\nBạn có thể kích hoạt nhóm bảo mật cho Pods bằng cách thiết lập ENABLE_POD_ENI=true cho VPC CNI. Khi bạn kích hoạt Pod ENI, VPC Resource Controller chạy trên mặt trái kiểm soát (được quản lý bởi EKS) tạo và đính kèm một giao diện cụm gọi là \u0026ldquo;aws-k8s-trunk-eni\u0026rdquo; vào nút. Giao diện cụm hoạt động như một giao diện mạng tiêu chuẩn được đính kèm vào trường hợp.\nBộ điều khiển cũng tạo ra các giao diện nhánh có tên \u0026ldquo;aws-k8s-branch-eni\u0026rdquo; và liên kết chúng với giao diện cụm. Pods được gán một nhóm bảo mật bằng cách sử dụng SecurityGroupPolicy tài nguyên tùy chỉnh và được liên kết với một giao diện nhánh. Vì nhóm bảo mật được chỉ định với các giao diện mạng, chúng ta hiện có thể lên lịch cho các Pods yêu cầu các nhóm bảo mật cụ thể trên các giao diện mạng bổ sung này. Xem hướng dẫn thực hành tốt nhất EKS để biết các khuyến nghị và Hướng dẫn Sử dụng EKS, cho các yêu cầu triển khai trước.\nTrong chương này, chúng tôi sẽ cấu hình lại một trong các thành phần ứng dụng mẫu để tận dụng nhóm bảo mật cho Pods để truy cập vào một tài nguyên mạng bên ngoài.\n"
},
{
	"uri": "/vi/5-custom-networking/",
	"title": "Custom Networking",
	"tags": [],
	"description": "",
	"content": "Custom Networking Chuẩn bị môi trường cho phần này: $ prepare-environment networking/custom-networking Điều này sẽ thực hiện các thay đổi sau đây trong môi trường thí nghiệm của bạn:\nGắn một dải CIDR phụ vào VPC Tạo ba mạng con bổ sung từ dải CIDR phụ Bạn có thể xem Terraform áp dụng các thay đổi này tại đây.\nMặc định, Amazon VPC CNI sẽ gán cho Pods một địa chỉ IP được chọn từ mạng con chính. Mạng con chính là dải CIDR mà ENI chính được gắn vào, thường là mạng con của nút/ máy chủ.\nNếu dải CIDR của mạng con quá nhỏ, CNI có thể không thể có được đủ địa chỉ IP phụ để gán cho Pods của bạn. Điều này là một thách thức phổ biến đối với các cụm IPv4 EKS.\nMạng tùy chỉnh là một giải pháp cho vấn đề này.\nMạng tùy chỉnh giải quyết vấn đề cạn kiệt địa chỉ IP bằng cách gán các địa chỉ IP Pods từ không gian địa chỉ VPC phụ (CIDR). Mạng tùy chỉnh hỗ trợ tài nguyên tùy chỉnh ENIConfig. ENIConfig bao gồm một phạm vi CIDR mạng con thay thế (tách từ CIDR VPC phụ), cùng với các nhóm bảo mật mà các Pods sẽ thuộc về. Khi mạng tùy chỉnh được kích hoạt, VPC CNI tạo các ENI phụ trong mạng con được xác định trong ENIConfig. CNI gán cho Pods các địa chỉ IP từ một dải CIDR được xác định trong một tài nguyên CRD ENIConfig.\n"
},
{
	"uri": "/vi/1-introduce/1.5-docker-networking/",
	"title": "Docker Networking",
	"tags": [],
	"description": "",
	"content": "Docker Networking Trong phần này, chúng ta sẽ xem xét Docker Networking\nNone Network Chạy container Docker với mạng none $ docker run --network none nginx Host Network Chạy container Docker với mạng host $ docker run --network host nginx Bridge Network Chạy container Docker với mạng bridge $ docker run --network bridge nginx Liệt kê Docker Network $ docker network ls\rNETWORK ID NAME DRIVER SCOPE\r4974cba36c8e bridge bridge local\r0e7b30a6c996 host host local\ra4b19b17d2c5 none null local Để xem Thiết bị Mạng trên Máy Chủ $ ip link\rhoặc\r$ ip link show docker0\r3: docker0: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default\rlink/ether 02:42:cf:c3:df:f5 brd ff:ff:ff:ff:ff:ff Với sự giúp đỡ của lệnh ip link add để thiết lập loại bridge cho docker0 $ ip link add docker0 type bridge Để xem Địa chỉ IP của giao diện docker0 $ ip addr\rhoặc\r$ ip addr show docker0\r3: docker0: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc noqueue state DOWN group default\rlink/ether 02:42:cf:c3:df:f5 brd ff:ff:ff:ff:ff:ff\rinet 172.18.0.1/24 brd 172.18.0.255 scope global docker0\rvalid_lft forever preferred_lft forever Chạy lệnh để tạo một Docker Container $ docker run nginx Để liệt kê Namespace Mạng $ ip netns\r1c452d473e2a (id: 2)\rdb732004aa9b (id: 1)\r04acb487a641 (id: 0)\rdefault\r# Kiểm tra Docker Container\r$ docker inspect \u0026lt;container-id\u0026gt;\r# Để xem giao diện được gắn với local bridge docker0\r$ ip link\r3: docker0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default\rlink/ether 02:42:c8:3a:ea:67 brd ff:ff:ff:ff:ff:ff\r5: vetha3e33331@if3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default\rlink/ether e2:b2:ad:c9:8b:98 brd ff:ff:ff:ff:ff:ff link-netnsid 0\r# với tùy chọn -n với namespace mạng để xem phía kết nối khác của giao diện\r$ ip -n 04acb487a641 link\r1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000\rlink/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\r3: eth0@if5: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default\rlink/ether c6:f3:ca:12:5e:74 brd ff:ff:ff:ff:ff:ff link-netnsid 0\r# Để xem Địa chỉ IP được gán cho giao diện này $ ip -n 04acb487a641 addr\r3: eth0@if5: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default\rlink/ether c6:f3:ca:12:5e:74 brd ff:ff:ff:ff:ff:ff link-netnsid 0\rinet 10.244.0.2/24 scope global eth0\rvalid_lft forever preferred_lft forever Port Mapping Tạo một container Docker. $ docker run -itd --name nginx nginx d74ca9d57c1d8983db2c590df2fdd109e07e1972d6b361a6ecad8a942af5bf7e Kiểm tra container Docker để xem IPAddress. $ docker inspect nginx | grep -w IPAddress \u0026#34;IPAddress\u0026#34;: \u0026#34;172.18.0.6\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;172.18.0.6\u0026#34;, Truy cập trang web bằng lệnh curl. $ curl --head http://172.18.0.6:80 HTTP/1.1 200 OK Server: nginx/1.19.2 Port Mapping cho container Docker. $ docker run -itd --name nginx -p 8080:80 nginx e7387bbb2e2b6cc1d2096a080445a6b83f2faeb30be74c41741fe7891402f6b6 Kiểm tra container Docker để xem các cổng được gán. $ docker inspect nginx | grep -w -A5 Ports \u0026#34;Ports\u0026#34;: { \u0026#34;80/tcp\u0026#34;: [ { \u0026#34;HostIp\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;HostPort\u0026#34;: \u0026#34;8080\u0026#34; } Để xem địa chỉ IP của hệ thống máy chủ $ ip a # Truy cập trang nginx với lệnh curl $ curl --head http://192.168.10.11:8080 HTTP/1.1 200 OK Server: nginx/1.19.2 Cấu hình các luật iptables nat $ iptables \\ -t nat \\ -A PREROUTING \\ -j DNAT \\ --dport 8080 \\ --to-destination 80 $ iptables \\ -t nat \\ -A DOCKER \\ -j DNAT \\ --dport 8080 \\ --to-destination 172.18.0.6:80 Liệt kê các luật Iptables $ iptables -nvL -t nat Tài liệu tham khảo https://docs.docker.com/network/ https://linux.die.net/man/8/iptables https://linux.die.net/man/8/ip "
},
{
	"uri": "/vi/1-introduce/1.6-cni/",
	"title": "CNI",
	"tags": [],
	"description": "",
	"content": "Trong phần này, chúng ta sẽ xem xét Giao diện Mạng Contain được yêu cầu trước (Pre-requisite Container Network Interface(CNI)).\nCác Nhà Cung Cấp Plugin Mạng Bên Thứ Ba Weave Calico Flannel Cilium Để xem các Plugin Mạng CNI CNI đi kèm với bộ các plugin mạng được hỗ trợ. $ ls /opt/cni/bin/ bridge dhcp flannel host-device host-local ipvlan loopback macvlan portmap ptp sample tuning vlan Tài liệu Tham Khảo https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/ "
},
{
	"uri": "/vi/6-prefix-delegation/",
	"title": "Phân Cấp Tiền Tố",
	"tags": [],
	"description": "",
	"content": "Phân Cấp Tiền Tố Chuẩn bị môi trường của bạn cho phần này:\n$ prepare-environment networking/prefix Amazon VPC CNI gán các tiền tố mạng cho giao diện mạng Amazon EC2 để tăng số lượng địa chỉ IP có sẵn cho các nút và tăng mật độ pod trên mỗi nút. Bạn có thể cấu hình phiên bản 1.9.0 hoặc mới hơn của tiện ích bổ sung Amazon VPC CNI để gán một tiền tố thay vì gán các địa chỉ IP phụ cá nhân cho các giao diện mạng.\nVới chế độ gán tiền tố, số lượng giao diện mạng linh hoạt tối đa trên mỗi loại instance vẫn giữ nguyên, nhưng bây giờ bạn có thể cấu hình Amazon VPC CNI để gán tiền tố địa chỉ IPv4 /28 (16 địa chỉ IP), thay vì gán các địa chỉ IPv4 cá nhân cho các khe trên các giao diện mạng trên loại instance EC2 nitro. Khi ENABLE_PREFIX_DELEGATION được đặt thành true, VPC CNI sẽ phân bổ một địa chỉ IP cho một Pod từ tiền tố được gán cho một ENI.\nTrong quá trình khởi tạo nút worker, VPC CNI gán một hoặc nhiều tiền tố cho ENI chính. CNI tiền gán trước một tiền tố để khởi động pod nhanh hơn bằng cách duy trì một bể ấm.\nKhi có nhiều Pods được lên lịch, các tiền tố bổ sung sẽ được yêu cầu cho các ENI hiện có. Đầu tiên, VPC CNI cố gắng phân bổ một tiền tố mới cho một ENI hiện có. Nếu ENI đã đầy, VPC CNI sẽ cố gắng phân bổ một ENI mới cho nút. ENI mới sẽ được đính kèm cho đến khi đạt đến giới hạn ENI tối đa (được xác định bởi loại instance). Khi một ENI mới được đính kèm, ipamd sẽ phân bổ một hoặc nhiều tiền tố cần thiết để duy trì các cài đặt bể ấm.\nVui lòng truy cập hướng dẫn thực hành tốt nhất cho EKS để xem danh sách các khuyến nghị về việc sử dụng VPC CNI ở chế độ tiền tố.\n"
},
{
	"uri": "/vi/1-introduce/1.7-cluster-networking/",
	"title": "Cluster Networking",
	"tags": [],
	"description": "",
	"content": "Cluster Networking Trong phần này, chúng ta sẽ xem xét Tiền điều kiện của Mạng lưới Cụm\nThiết lập tên máy duy nhất. Lấy địa chỉ IP của hệ thống (nút chủ và nút công việc). Kiểm tra các cổng. Địa chỉ IP và Tên máy chủ Để xem tên máy chủ $ hostname Để xem địa chỉ IP của hệ thống $ ip a Thiết lập tên máy chủ $ hostnamectl set-hostname \u0026lt;tên-máy-chủ\u0026gt;\r$ exec bash Xem các Cổng Đang Nghe của hệ thống $ netstat -nltp Tài liệu Tham khảo https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports https://kubernetes.io/docs/concepts/cluster-administration/networking/ "
},
{
	"uri": "/vi/1-introduce/1.8-pod-networking/",
	"title": "Pod Networking",
	"tags": [],
	"description": "",
	"content": "Pod Networking Trong phần này, chúng ta sẽ xem xét Mạng Pod\nĐể thêm mạng cầu trên mỗi nút node01\n$ ip link add v-net-0 type bridge node02\n$ ip link add v-net-0 type bridge node03\n$ ip link add v-net-0 type bridge Hiện tại nó đang tắt, hãy bật nó lên. node01\n$ ip link set dev v-net-0 up node02\n$ ip link set dev v-net-0 up node03\n$ ip link set dev v-net-0 up Đặt địa chỉ IP cho giao diện cầu node01\n$ ip addr add 10.244.1.1/24 dev v-net-0 node02\n$ ip addr add 10.244.2.1/24 dev v-net-0 node03\n$ ip addr add 10.244.3.1/24 dev v-net-0 Kiểm tra khả năng kết nối $ ping 10.244.2.2\rConnect: Network is unreachable Thêm route vào bảng định tuyến $ ip route add 10.244.2.2 via 192.168.1.12 node01\n$ ip route add 10.244.2.2 via 192.168.1.12\r$ ip route add 10.244.3.2 via 192.168.1.13 node02\n$ ip route add 10.244.1.2 via 192.168.1.11\r$ ip route add 10.244.3.2 via 192.168.1.13 node03\n$ ip route add 10.244.1.2 via 192.168.1.11\r$ ip route add 10.244.2.2 via 192.168.1.12 Thêm một mạng lớn duy nhất Giao diện Mạng Container Tài liệu Tham khảo "
},
{
	"uri": "/vi/1-introduce/1.9-cni-in-kubernetes/",
	"title": "CNI in Kubernetes",
	"tags": [],
	"description": "",
	"content": "CNI in Kubernetes Trong phần này, chúng ta sẽ xem xét Giao diện Mạng Container (CNI) trong Kubernetes\nCấu hình CNI Kiểm tra trạng thái của Dịch vụ Kubelet $ systemctl status kubelet.service Xem Các Tuỳ chọn Kubelet $ ps -aux | grep kubelet Kiểm tra Các Plugin Hỗ trợ Để kiểm tra tất cả các plugin hỗ trợ có sẵn trong thư mục /opt/cni/bin. $ ls /opt/cni/bin Kiểm tra Các Plugin CNI Để kiểm tra các plugin cni mà kubelet cần sử dụng. ls /etc/cni/net.d Định dạng của Tập tin Cấu hình Tài liệu Tham khảo https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/ https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/ "
},
{
	"uri": "/vi/1-introduce/1.10-cni-weave/",
	"title": "CNI weave",
	"tags": [],
	"description": "",
	"content": "CNI weave Trong phần này, chúng ta sẽ tìm hiểu về \u0026ldquo;CNI Weave trong Cụm Kubernetes\u0026rdquo;\nTriển khai Weave Cài đặt weave net vào cụm Kubernetes bằng một lệnh duy nhất. $ kubectl apply -f \u0026#34;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d \u0026#39;\\n\u0026#39;)\u0026#34; serviceaccount/weave-net đã được tạo clusterrole.rbac.authorization.k8s.io/weave-net đã được tạo clusterrolebinding.rbac.authorization.k8s.io/weave-net đã được tạo role.rbac.authorization.k8s.io/weave-net đã được tạo rolebinding.rbac.authorization.k8s.io/weave-net đã được tạo daemonset.apps/weave-net đã được tạo Weave Peers $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-66bff467f8-894jf 1/1 Running 0 52m coredns-66bff467f8-nck5f 1/1 Running 0 52m etcd-controlplane 1/1 Running 0 52m kube-apiserver-controlplane 1/1 Running 0 52m kube-controller-manager-controlplane 1/1 Running 0 52m kube-keepalived-vip-mbr7d 1/1 Running 0 52m kube-proxy-p2mld 1/1 Running 0 52m kube-proxy-vjcwp 1/1 Running 0 52m kube-scheduler-controlplane 1/1 Running 0 52m weave-net-jgr8x 2/2 Running 0 45m weave-net-tb9tz 2/2 Running 0 45m Xem nhật ký của Pod Weave $ kubectl logs weave-net-tb9tz weave -n kube-system Xem đường mạng mặc định trong Pod $ kubectl run test --image=busybox --command -- sleep 4500 pod/test đã được tạo $ kubectl exec test -- ip route default via 10.244.1.1 dev eth0 Tài liệu tham khảo https://kubernetes.io/docs/concepts/cluster-administration/addons/ https://www.weave.works/docs/net/latest/kubernetes/kube-addon/ "
},
{
	"uri": "/vi/1-introduce/1.11-ipam-weave/",
	"title": "IPAM weave",
	"tags": [],
	"description": "",
	"content": "IPAM weave Take me to Lecture\nQuản lý Địa chỉ IP trong Cụm Kubernetes\nCách weaveworks Quản lý Địa chỉ IP trong Cụm Kubernetes Tài liệu Tham khảo https://www.weave.works/docs/net/latest/kubernetes/kube-addon/ https://kubernetes.io/docs/concepts/cluster-administration/networking/ "
},
{
	"uri": "/vi/1-introduce/1.12-service-networking/",
	"title": "Service Networking",
	"tags": [],
	"description": "",
	"content": "Service Networking Trong phần này, chúng ta sẽ xem xét Dịch Vụ Mạng\nCác Loại Dịch Vụ ClusterIP clusterIP.yaml\rapiVersion: v1\rkind: Service\rmetadata:\rname: local-cluster\rspec:\rports:\r- port: 80\rtargetPort: 80\rselector:\rapp: nginx NodePort nodeportIP.yaml\rapiVersion: v1\rkind: Service\rmetadata:\rname: nodeport-wide\rspec:\rtype: NodePort\rports:\r- port: 80\rtargetPort: 80\rselector:\rapp: nginx Để tạo ra dịch vụ $ kubectl create -f clusterIP.yaml\rservice/local-cluster created\r$ kubectl create -f nodeportIP.yaml\rservice/nodeport-wide created Để lấy Thông Tin Bổ Sung $ kubectl get pods -o wide\rNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES\rnginx 1/1 Running 0 1m 10.244.1.3 node01 \u0026lt;none\u0026gt; \u0026lt;no Để lấy Dịch Vụ $ kubectl get service\rNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rkubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 5m22s\rlocal-cluster ClusterIP 10.101.67.139 \u0026lt;none\u0026gt; 80/TCP 3m\rnodeport-wide NodePort 10.102.29.204 \u0026lt;none\u0026gt; 80:30016/TCP 2m Để kiểm tra Phạm Vi IP Cluster của Dịch Vụ $ ps -aux | grep kube-apiserver\r--secure-port=6443 --service-account-key-file=/etc/kubernetes/pki/sa.pub --\rservice-cluster-ip-range=10.96.0.0/12 Để kiểm tra các quy tắc được tạo ra bởi kube-proxy trong iptables $ iptables -L -t nat | grep local-cluster\rKUBE-MARK-MASQ all -- 10.244.1.3 anywhere /* default/local-cluster: */\rDNAT tcp -- anywhere anywhere /* default/local-cluster: */ tcp to:10.244.1.3:80\rKUBE-MARK-MASQ tcp -- !10.244.0.0/16 10.101.67.139 /* default/local-cluster: cluster IP */ tcp dpt:http\rKUBE-SVC-SDGXHD6P3SINP7QJ tcp -- anywhere 10.101.67.139 /* default/local-cluster: cluster IP */ tcp dpt:http\rKUBE-SEP-GEKJR4UBUI5ONAYW all -- anywhere anywhere /* default/local-cluster: */ Để kiểm tra nhật ký của kube-proxy Vị trí tệp có thể thay đổi tùy thuộc vào quá trình cài đặt của bạn. $ cat /var/log/kube-proxy.log Tài Liệu Tham Khảo https://kubernetes.io/docs/concepts/services-networking/service/ "
},
{
	"uri": "/vi/1-introduce/1.13-dns-in-kubernetes/",
	"title": "DNS in Kubernetes",
	"tags": [],
	"description": "",
	"content": "DNS trong Kubernetes Dẫn tôi đến Bài giảng Trong phần này, chúng ta sẽ xem xét DNS trong Cụm Kubernetes\nBản ghi DNS của Pod Quá trình giải quyết DNS sau: \u0026lt;ĐỊA-CHỈ-IP-POD\u0026gt;.\u0026lt;tên-namespace\u0026gt;.pod.cluster.local Ví dụ\n# Pod nằm trong một namespace mặc định 10-244-1-10.default.pod.cluster.local # Để tạo một namespace $ kubectl create ns apps # Để tạo một Pod $ kubectl run nginx --image=nginx --namespace apps # Để lấy thông tin bổ sung của Pod trong namespace \u0026#34;apps\u0026#34; $ kubectl get po -n apps -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx 1/1 Running 0 99s 10.244.1.3 node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # Để lấy bản ghi dns của Pod nginx từ namespace mặc định $ kubectl run -it test --image=busybox:1.28 --rm --restart=Never -- nslookup 10-244-1-3.apps.pod.cluster.local Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: 10-244-1-3.apps.pod.cluster.local Address 1: 10.244.1.3 pod \u0026#34;test\u0026#34; deleted # Truy cập với lệnh curl $ kubectl run -it nginx-test --image=nginx --rm --restart=Never -- curl -Is http://10-244-1-3.apps.pod.cluster.local HTTP/1.1 200 OK Server: nginx/1.19.2 Bản ghi DNS của Service Quá trình giải quyết DNS sau: \u0026lt;tên-dịch-vụ\u0026gt;.\u0026lt;tên-namespace\u0026gt;.svc.cluster.local Ví dụ\n# Dịch vụ nằm trong một namespace mặc định web-service.default.svc.cluster.local Pod, Service nằm trong namespace apps # Tiếp cận Pod nginx $ kubectl expose pod nginx --name=nginx-service --port 80 --namespace apps service/nginx-service exposed # Lấy nginx-service trong namespace \u0026#34;apps\u0026#34; $ kubectl get svc -n apps NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-service ClusterIP 10.96.120.174 \u0026lt;none\u0026gt; 80/TCP 6s # Để lấy bản ghi dns của nginx-service từ namespace mặc định $ kubectl run -it test --image=busybox:1.28 --rm --restart=Never -- nslookup nginx-service.apps.svc.cluster.local Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: nginx-service.apps.svc.cluster.local Address 1: 10.96.120.174 nginx-service.apps.svc.cluster.local pod \u0026#34;test\u0026#34; deleted # Truy cập với lệnh curl $ kubectl run -it nginx-test --image=nginx --rm --restart=Never -- curl -Is http://nginx-service.apps.svc.cluster.local HTTP/1.1 200 OK Server: nginx/1.19.2 Tài liệu Tham khảo https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/ https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/ "
},
{
	"uri": "/vi/1-introduce/1.14-coredns-in-kubernetes/",
	"title": "CoreDNS in Kubernetes",
	"tags": [],
	"description": "",
	"content": "CoreDNS trong Kubernetes Trong phần này, chúng ta sẽ xem xét CoreDNS trong Kubernetes\nĐể xem Pod $ kubectl get pods -n kube-system\rNAME READY STATUS RESTARTS AGE\rcoredns-66bff467f8-2vghh 1/1 Running 0 53m\rcoredns-66bff467f8-t5nzm 1/1 Running 0 53m Để xem Deployment $ kubectl get deployment -n kube-system\rNAME READY UP-TO-DATE AVAILABLE AGE\rcoredns 2/2 2 2 53m Để xem configmap của CoreDNS $ kubectl get configmap -n kube-system\rNAME DATA AGE\rcoredns 1 52m Tập tin Cấu hình CoreDNS $ kubectl describe cm coredns -n kube-system\rCorefile:\r---\r.:53 {\rerrors\rhealth { lameduck 5s\r}\rready\rkubernetes cluster.local in-addr.arpa ip6.arpa {\rpods insecure\rfallthrough in-addr.arpa ip6.arpa\rttl 30\r}\rprometheus :9153\rforward . /etc/resolv.conf\rcache 30\rloop\rreload\r} Để xem Dịch vụ $ kubectl get service -n kube-system\rNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rkube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 62m Để xem Cấu hình vào kubelet $ cat /var/lib/kubelet/config.yaml | grep -A2 clusterDNS\rclusterDNS:\r- 10.96.0.10\rclusterDomain: cluster.local Để xem tên miền đầy đủ Với lệnh host, chúng ta sẽ nhận được tên miền đầy đủ (FQDN). $ host web-service\rweb-service.default.svc.cluster.local has address 10.106.112.101\r$ host web-service.default\rweb-service.default.svc.cluster.local has address 10.106.112.101\r$ host web-service.default.svc\rweb-service.default.svc.cluster.local has address 10.106.112.101\r$ host web-service.default.svc.cluster.local\rweb-service.default.svc.cluster.local has address 10.106.112.101 Để xem tập tin /etc/resolv.conf $ kubectl run -it --rm --restart=Never test-pod --image=busybox -- cat /etc/resolv.conf\rnameserver 10.96.0.10\rsearch default.svc.cluster.local svc.cluster.local cluster.local\roptions ndots:5\rpod \u0026#34;test-pod\u0026#34; deleted Giải quyết Pod $ kubectl get pods -o wide\rNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES\rtest-pod 1/1 Running 0 11m 10.244.1.3 node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rnginx 1/1 Running 0 10m 10.244.1.4 node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\r$ kubectl exec -it test-pod -- nslookup 10-244-1-4.default.pod.cluster.local\rServer: 10.96.0.10\rAddress 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local\rName: 10-244-1-4.default.pod.cluster.local\rAddress 1: 10.244.1.4 Giải quyết Dịch vụ $ kubectl get service\rNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rkubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 85m\rweb-service ClusterIP 10.106.112.101 \u0026lt;none\u0026gt; 80/TCP 9m\r$ kubectl exec -it test-pod -- nslookup web-service.default.svc.cluster.local\rServer: 10.96.0.10\rAddress 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local\rName: web-service.default.svc.cluster.local\rAddress 1: 10.106.112.101 web-service.default.svc.cluster.local Tài liệu Tham khảo https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#services https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pods "
},
{
	"uri": "/vi/1-introduce/1.15-ingress/",
	"title": "Ingress",
	"tags": [],
	"description": "",
	"content": "Ingress Trong phần này, chúng ta sẽ xem xét Ingress\nBộ điều khiển Ingress Tài nguyên Ingress Bộ điều khiển Ingress Triển khai Bộ điều khiển Ingress Bản đồ Cấu hình kind: ConfigMap\rapiVersion: v1\rmetadata:\rname: nginx-configuration Triển khai apiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: ingress-controller\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rname: nginx-ingress\rtemplate:\rmetadata:\rlabels:\rname: nginx-ingress\rspec:\rserviceAccountName: ingress-serviceaccount\rcontainers:\r- name: nginx-ingress-controller\rimage: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0\rargs:\r- /nginx-ingress-controller\r- --configmap=$(POD_NAMESPACE)/nginx-configuration\renv:\r- name: POD_NAME\rvalueFrom:\rfieldRef:\rfieldPath: metadata.name\r- name: POD_NAMESPACE\rvalueFrom:\rfieldRef:\rfieldPath: metadata.namespace\rports:\r- name: http\rcontainerPort: 80\r- name: https\rcontainerPort: 443 Tài khoản Dịch vụ Tài khoản Dịch vụ yêu cầu cho mục đích xác thực cùng với các Quyền, Quyền Cụm và Liên kết Quyền chính xác.\nTạo một tài khoản dịch vụ ingress\n$ kubectl create -f ingress-sa.yaml\rserviceaccount/ingress-serviceaccount created Loại Dịch vụ - NodePort # service-Nodeport.yaml\rapiVersion: v1\rkind: Service\rmetadata:\rname: ingress\rspec:\rtype: NodePort\rports:\r- port: 80\rtargetPort: 80\rprotocol: TCP\rname: http\r- port: 443\rtargetPort: 443\rprotocol: TCP\rname: https\rselector:\rname: nginx-ingress Tạo một dịch vụ $ kubectl create -f service-Nodeport.yaml Để lấy dịch vụ $ kubectl get service Tài nguyên Ingress Ingress-wear.yaml\rapiVersion: extensions/v1beta1\rkind: Ingress\rmetadata:\rname: ingress-wear\rspec:\rbackend:\rserviceName: wear-service\rservicePort: 80 Để tạo tài nguyên ingress $ kubectl create -f Ingress-wear.yaml\ringress.extensions/ingress-wear created Để lấy ingress $ kubectl get ingress\rNAME CLASS HOSTS ADDRESS PORTS AGE\ringress-wear \u0026lt;none\u0026gt; * 80 18s Tài nguyên Ingress - Luật 1 Luật và 2 Đường dẫn. apiVersion: extensions/v1beta1\rkind: Ingress\rmetadata:\rname: ingress-wear-watch\rspec:\rrules:\r- http:\rpaths:\r- path: /wear\rbackend:\rserviceName: wear-service\rservicePort: 80\r- path: /watch\rbackend:\rserviceName: watch-service\rservicePort: 80 Mô tả tài nguyên ingress đã tạo trước đó $ kubectl describe ingress ingress-wear-watch\rName: ingress-wear-watch\rNamespace: default\rAddress:\rDefault backend: default-http-backend:80 (\u0026lt;none\u0026gt;)\rRules:\rHost Path Backends\r---- ---- --------\r*\r/wear wear-service:80 (\u0026lt;none\u0026gt;)\r/watch watch-service:80 (\u0026lt;none\u0026gt;)\rAnnotations: \u0026lt;none\u0026gt;\rEvents:\rType Reason Age From Message\r---- ------ ---- ---- -------\rNormal CREATE 23s nginx-ingress-controller Ingress default/ingress-wear-watch 2 Luật và 1 Đường dẫn mỗi luật. # Ingress-wear-watch.yaml\rapiVersion: extensions/v1beta1\rkind: Ingress\rmetadata:\rname: ingress-wear-watch\rspec:\rrules:\r- host: wear.my-online-store.com\rhttp:\rpaths:\r- backend:\rserviceName: wear-service\rservicePort: 80\r- host: watch.my-online-store.com\rhttp:\rpaths:\r- backend:\rserviceName: watch-service\rservicePort: 80 Tài liệu Tham khảo https://kubernetes.io/docs/concepts/services-networking/ingress/ https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/ https://thenewstack.io/kubernetes-ingress-for-beginners/ "
},
{
	"uri": "/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]